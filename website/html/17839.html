<html> <head> <title>Idempotent matrix</title></head><body>In [[algebra]], an '''idempotent matrix''' is a [[matrix (mathematics)|matrix]] which, when multiplied by itself, yields itself.<ref>Chiang, Alpha C., ''Fundamental Methods of Mathematical Economics'', McGraw–Hill, 3rd edition, 1984: p. 80.</ref><ref name=Greene>Greene, William H., ''Econometric Analysis'', Prentice–Hall, 5th edition, 2003: pp. 808–809.</ref> That is, the matrix ''M'' is idempotent if and only if ''MM'' = ''M''. For this product ''MM'' to be [[Matrix multiplication|conformable for multiplication]], ''M'' must necessarily be a [[square matrix]].

==Properties==

With the exception of the [[identity matrix]], an idempotent matrix is [[singular matrix|singular]]; that is, its number of independent rows (and columns) is less than its number of rows (and columns).  This can be seen from writing ''MM = M'', assuming that ''M'' has full rank (is non-singular), and pre-multiplying by ''M''<sup>−1</sup> to obtain ''M'' = ''M''<sup>−1</sup>''M'' = ''I''.

When an idempotent matrix is subtracted from the identity matrix, the result is also idempotent. This holds since [''I'' − ''M''][''I'' − ''M''] = ''I'' − ''M'' − ''M'' + ''M''<sup>2</sup> = ''I'' − ''M'' − ''M'' + ''M'' = ''I'' − ''M''.

The [[trace (linear algebra)|trace]] of an idempotent matrix — the sum of the elements on its main diagonal — equals the [[rank (linear algebra)|rank]] of the matrix and thus is always an integer. This provides an easy way of computing the rank, or alternatively an easy way of determining the trace of a matrix whose elements are not specifically known (which is helpful in [[econometrics]], for example, in establishing the degree of [[bias (statistics)|bias]] in using a [[variance|sample variance]] as an estimate of a [[variance|population variance]]).

==Applications==

Idempotent matrices arise frequently in [[regression analysis]] and [[econometrics]]. For example, in [[ordinary least squares]], the regression problem is to choose a vector <math>\beta</math> of coefficient estimates so as to minimize the sum of squared residuals (mispredictions) ''e''<sub>''i''</sub>: in matrix form,

:<math>\text{Minimize } (y - X \beta)^T(y - X \beta) \, </math>

where ''y'' is a vector of [[Dependent and independent variables#Use in statistics|dependent variable]] observations, and ''X'' is a matrix each of whose columns is a column of observations on one of the [[Dependent and independent variables#Use in statistics|independent variables]]. The resulting estimator is 

:<math>\beta = (X^TX)^{-1}X^Ty \, </math>

where superscript ''T'' indicates a [[transpose]], and the vector of residuals is<ref name=Greene/> 

:<math>e = y - X \beta = y - X(X^TX)^{-1}X^Ty = [I - X(X^TX)^{-1}X^T]y = My. \, </math>

Here both ''M'' and <math>X(X^TX)^{-1}X^T</math> are idempotent matrices, a fact which allows simplification when the sum of squared residuals is computed:

:<math> e^Te = (My)^T(My) = y^TM^TMy = y^TMMy = y^TMy. \, </math>

The idempotency of ''M'' plays a role in other calculations as well, such as in determining the variance of the estimator <math>\beta</math>.

==See also==

* [[Projection (linear algebra)]]

==References==

{{reflist}}

[[Category:Mathematics]]
[[Category:Algebra]]
[[Category:Econometrics]]</body> </html>