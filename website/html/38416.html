<html> <head> <title>Turing test</title></head><body>{{other uses}}

{{multiple issues|citation style=September 2010|cleanup=September 2010}}
[[Image:Turing Test version 3.png|thumb|The "standard interpretation" of the Turing Test, in which player C, the interrogator, is tasked with trying to determine which player - A or B - is a computer and which is a human. The interrogator is limited to using the responses to written questions in order to make the determination. Image adapted from Saygin, 2000.<ref name="Saygin 2000">{{Harvnb|Saygin|2000}}</ref>]]
The '''Turing test''' is a test of a [[machine]]'s ability to demonstrate [[intelligence]]. A human judge engages in a natural language [[conversation]] with one [[human]] and one machine, each of which tries to appear human. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test. In order to test the machine's intelligence rather than its ability to render words into audio, the conversation is limited to a text-only channel such as a [[keyboard (computing)|computer keyboard]] and [[visual display unit|screen]].<ref>Turing originally suggested a [[teleprinter]], one of the few text-only communication systems available in 1950. {{Harv|Turing|1950|p=433}}</ref>

The test was introduced by [[Alan Turing]] in his 1950 paper ''[[Computing Machinery and Intelligence]]'', which opens with the words: "I propose to consider the question, 'Can machines think?'" Since "thinking" is difficult to define, Turing chooses to "replace the question by another, which is closely related to it and is expressed in relatively unambiguous words."<ref name=T433/> Turing's new question is: "Are there imaginable digital computers which would do well in the <nowiki>[</nowiki>Turing test<nowiki>]</nowiki>"?<ref>{{Harv|Turing|1950|p=442}} This particular version of the test was called "The Imitation Game". Turing continues with a more technical version: "these questions [are] equivalent to this, 'Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?'" {{Harv|Turing|1950|p=442}}</ref> This question, Turing believed, is one that can actually be answered. In the remainder of the paper, he argued against all the major objections to the proposition that "machines can think".<ref name=RN948/> 

In the years since 1950, the test has proven to be both highly influential and widely criticized, and it is an essential concept in the [[philosophy of artificial intelligence]].<ref>{{Harvnb|Saygin|Cicekli|Akman|2000}}</ref><ref>{{Harvnb|Russell|Norvig|2003|pp= 2–3 and 948}}</ref> <!-- To date, there are no machines that can convincingly pass the test.<ref>{{Citation|last=Gibb|first=Barry|title=The Rough Guide to the Brain|publisher=Rough Guides Ltd.|location=London|year=2007|pages=236}}</ref> --><!-- This statement seems at least doubtful: Loebner prize judges are frequently being fooled by contestants, see below.  Turing would probably reject '''convincingly''' as the kind of terminology that his test set out to avoid. -->

==History==
===Philosophical background===
The question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction between [[dualism (philosophy of mind)|dualist]] and [[materialism|materialist]] views of the mind. From the perspective of dualism, the [[mind]] is [[non-physical entity|non-physical]] (or, at the very least, has [[property dualism|non-physical properties]])<ref>For an example of property dualism, see [[Qualia]].</ref> and, therefore, cannot be explained in purely physical terms. The materialist perspective argues that the mind can be explained physically, and thus leaves open the possibility of minds that are artificially produced.<ref>Noting that materialism does not ''necessitate'' the possibility of artificial minds (for example, [[Roger Penrose#Physics and consciousness|Roger Penrose]]), any more than dualism necessarily ''precludes'' the possibility.  (See, for example, [[Property dualism]].)</ref>

In 1936, philosopher [[Alfred Ayer]] considered the standard philosophical question of [[other minds problem|other minds]]: how do we know that other people have the same conscious experiences that we do?  In his book ''[[Language, Truth and Logic]]'' Ayer suggested a protocol to distinguish between a conscious man and an unconscious machine: "The only ground I can have for asserting that an object which appears to be conscious is not really a conscious being, but only a dummy or a machine, is that it fails to satisfy one of the empirical tests by which the presence or absence of consciousness is determined."<ref>{{cite book | last=Ayer | first=A. J. | authorlink=A. J. Ayer | title=Language, Truth and Logic| p=140 | publisher=[[Penguin Books|Penguin]] | year=2001}}</ref> (This suggestion is very similar to the Turing test, but it is not certain that Ayer's popular philosophical classic was familiar to Turing.)

===Alan Turing===
Researchers in the [[United Kingdom]] had been exploring "machine intelligence" for up to ten years prior to the founding of the field of AI research in 1956.<ref>The [[Dartmouth conference]]s of 1956 are widely considered the "birth of AI". {{Harv|Crevier|1993|p=49}}</ref>  It was a common topic among the members of the [[Ratio Club]] who were an informal group of British [[cybernetics]] and [[electronics]] researchers that included [[Alan Turing]], after whom the test is named.<ref>{{Harvnb|McCorduck|2004|p=95}}</ref>

Turing, in particular, had been tackling the notion of machine intelligence since at least 1941<ref>{{Harvnb|Copeland|2003|p=1}}</ref> and one of the earliest-known mentions of "computer intelligence" was made by him in 1947.<ref>{{Harvnb|Copeland|2003|p=2}}</ref>  In Turing's report, "Intelligent Machinery", he investigated "the question of whether or not it is possible for machinery to show intelligent behaviour"<ref>{{Harvnb|Turing|1948|p=412}}</ref> and, as part of that investigation, proposed what may be considered the forerunner to his later tests:

<blockquote>It is not difficult to devise a paper machine which will play a not very bad game of chess.<ref>In 1948, working with his former undergraduate colleague, [[DG Champernowne]], Turing began writing a chess program for a computer that did not yet exist and, in 1952, lacking a computer powerful enough to execute the program, played a game in which he simulated it, taking about half an hour over each move. The game was recorded, and the program lost to Turing's colleague [[Alick Glennie]], although it is said that it won a game against Champernowne's wife.</ref>  Now get three men as subjects for the experiment. A, B and C. A and C are to be rather poor chess players, B is the operator who works the paper machine. ...  Two rooms are used with some arrangement for communicating moves, and a game is played between C and either A or the paper machine. C may find it quite difficult to tell which he is playing.</blockquote>

When Turing published "[[Computing Machinery and Intelligence]]" he had been considering the possibility of artificial intelligence for many years, though this was the first published paper by Turing to focus exclusively on the notion.<ref>"Intelligent Machinery" was not published by Turing, and did not see publication until 1968 in Evans, C. R. & Robertson, A. D. J. (1968) ''Cybernetics: Key Papers,'' University Park Press.</ref>

Turing begins his 1950 paper with the claim "I propose to consider the question 'Can machines think?'"<ref name="T433">{{Harvnb|Turing|1950|p=433}}</ref>  As he highlights, the traditional approach to such a question is to start with [[definition]]s, defining both the terms "machine" and "intelligence". Turing chooses not to do so; instead he replaces the question with a new one, "which is closely related to it and is expressed in relatively unambiguous words."<ref name=T433/>  In essence he proposes to change the question from "Do machines think?" to "Can machines do what we (as thinking entities) can do?"<ref>{{Harvnb|Harnad|2004|p=1}}</ref>  The advantage of the new question, Turing argues, is that it draws "a fairly sharp line between the physical and intellectual capacities of a man."<ref name=T434>{{Harvnb|Turing|1950|p=434}}</ref>

To demonstrate this approach Turing proposes a test inspired by a [[party game]], known as the "Imitation Game", in which a man and a woman go into separate rooms and guests try to tell them apart by writing a series of questions and reading the typewritten answers sent back.  In this game both the man and the woman aim to convince the guests that they are the other. Turing proposes recreating the game as follows:

<blockquote>We now ask the question, "What will happen when a machine takes the part of A in this game?"  Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?  These questions replace our original, "Can machines think?"<ref name=T434/></blockquote>

Later in the paper Turing suggests an "equivalent" alternative formulation involving a judge conversing only with a computer and a man.<ref>{{Harvnb|Turing|1950|p=446}}</ref>  While neither of these formulations precisely matches the version of the Turing Test that is more generally known today, he proposed a third in 1952. In this version, which Turing discussed in a [[BBC]] radio broadcast, a jury asks questions of a computer and the role of the computer is to make a significant proportion of the jury believe that it is really a man.<ref>{{Harvnb|Turing|1952|pp=524–525}}.  Turing does not seem to distinguish between "man" as a gender and "man" as a human.  In the former case, this formulation would be closer to the Imitation Game, whereas in the latter it would be closer to current depictions of the test.</ref>

Turing's paper considered nine putative objections, which include all the major arguments against artificial intelligence that have been raised in the years since the paper was published.  (See ''[[Computing Machinery and Intelligence]]''.)<ref name=RN948>{{Harvnb|Turing|1950|pp=442–454}} and see {{Harvtxt|Russell|Norvig|2003|p=948}}, where they comment, "Turing examined a wide variety of possible objections to the possibility of intelligent machines, including virtually all of those that have been raised in the half century since his paper appeared."</ref>

===ELIZA and PARRY===
[[Blay Whitby]] lists four major turning points in the history of the Turing Test — the publication of "Computing Machinery and Intelligence" in 1950, the announcement of [[Joseph Weizenbaum]]'s [[ELIZA]] in 1966, [[Kenneth Colby]]'s creation of [[PARRY]], which was first described in 1972, and the Turing Colloquium in 1990.<ref>{{Harvnb|Whitby|1996|p=53}}</ref> Sixty years following its introduction, and continued argument over Turing's 'can machines think?' experiment, led to its reconsideration for the 21st century through the [http://www.cse.dmu.ac.uk/~aayesh/TuringTestRevisited/Welcome.html AISB's 'Towards a comprehensive intelligence test' symposium], 29 March - 1 April 2010, at De Montford University, UK.

ELIZA works by examining a user's typed comments for keywords.  If a keyword is found, a rule that transforms the user's comments is applied, and the resulting sentence is returned.  If a keyword is not found, ELIZA responds either with a generic riposte or by repeating one of the earlier comments.<ref>{{Harvnb|Weizenbaum|1966|p=37}}</ref>  In addition, Weizenbaum developed ELIZA to replicate the behaviour of a [[person-centered psychotherapy|Rogerian psychotherapist]], allowing ELIZA to be "free to assume the pose of knowing almost nothing of the real world."<ref name=Weizenbaum42>{{Harvnb|Weizenbaum|1966|p=42}}</ref>  With these techniques, Weizenbaum's program was able to fool some people into believing that they were talking to a real person, with some subjects being "very hard to convince that ELIZA [...] is ''not'' human."<ref name=Weizenbaum42 />  Thus, ELIZA is claimed by some to be one of the programs (perhaps the first) able to pass the Turing Test,<ref name=Weizenbaum42 /><ref>{{Harvnb|Thomas|1995|p=112}}</ref> although this view is highly contentious (see [[#Naivete of interrogators and the anthropomorphic fallacy|below]]).

Colby's PARRY has been described as "ELIZA with attitude":<ref>{{Harvnb|Bowden|2006|p=370}}</ref> it attempts to model the behaviour of a [[paranoia|paranoid]] [[schizophrenic]], using a similar (if more advanced) approach to that employed by Weizenbaum.  In order to validate the work, PARRY was tested in the early 1970s using a variation of the Turing Test.  A group of experienced psychiatrists analysed a combination of real patients and computers running PARRY through [[teletype]] machines.  Another group of 33 psychiatrists were shown transcripts of the conversations.  The two groups were then asked to identify which of the "patients" were human and which were computer programs.<ref>{{Harvnb|Colby|Hilf|Weber|Kraemer|1972|p=42}}</ref>  The psychiatrists were able to make the correct identification only 48 per cent of the time — a figure consistent with random guessing.<ref>{{Harvnb|Saygin|Cicekli|Akman|2000|p=501}}</ref>

In the 21st century, ELIZA and PARRY have been developed into [[malware]] systems, such as [http://www.itwire.com/your-it-news/home-it/15748-flirty-bot-passes-for-human CyberLover], which preys on Internet users convincing them to "reveal information about their identities or to lead them to visit a web site that will deliver malicious content to their computers" ([http://www.itwire.com/your-it-news/home-it/15748-flirty-bot-passes-for-human (iTWire, 2007)]. A one-trick pony, ''CyberLover'', a software program developed in Russia,  has emerged as a "Valentine-risk" flirting with people "seeking relationships online in order to collect their personal data" [http://www.v3.co.uk/vnunet/news/2205441/online-love-seekers-warned-flirt-bots# (V3, 2010)].

===The Chinese room===
[[John Searle]]'s 1980 paper ''[[Minds, Brains, and Programs]]'' proposed an argument against the Turing Test known as the "[[Chinese room]]" thought experiment. Searle argued that software (such as ELIZA) could pass the Turing Test simply by manipulating symbols of which they had no understanding.  Without understanding, they could not be described as "thinking" in the same sense people do.  Therefore—Searle concludes—the Turing Test cannot prove that a machine can think.<ref name="Searle 1980">{{Harvnb|Searle|1980}}</ref>

Arguments such as that proposed by Searle and others working on the [[philosophy of mind]] sparked off a more intense debate about the nature of intelligence, the possibility of intelligent machines and the value of the Turing test that continued through the 1980s and 1990s.<ref>{{Harvnb|Saygin|Cicekli|Akman|2000|p=479}}</ref>

===Turing Colloquium===
1990 was the fortieth anniversary of the first publication of Turing's "Computing Machinery and Intelligence" paper, and, thus, saw renewed interest in the test.  Two significant events occurred in that year: The first was the Turing Colloquium, which was held at the [[University of Sussex]] in April, and brought together academics and researchers from a wide variety of disciplines to discuss the Turing Test in terms of its past, present, and future; the second was the formation of the annual [[Loebner Prize]] competition. However, after nineteen Loebner Prize competitions, the contest is not viewed as contributing toward the science of machine intelligence, nor palliating the controversy surrounding the usefulness of Turing's test.

===Loebner Prize===
{{main|Loebner Prize}}

The Loebner Prize provides an annual platform for practical Turing Tests with the first competition held in November, 1991.<ref>{{Harvnb|Sundman|2003}}</ref> It is underwritten by [[Hugh Loebner]]; the Cambridge Center for Behavioral Studies in [[Massachusetts]], [[United States]] organized the Prizes up to and including the 2003 contest. As Loebner described it, one reason the competition was created is to advance the state of AI research, at least in part, because no one had taken steps to implement the Turing Test despite 40 years of discussing it [http://loebner.net/Prizef/In-response.html]. 

The first Loebner Prize competition in 1991 led to a renewed discussion of the viability of the Turing Test and the value of pursuing it, in both the popular press{{sfn|"Artificial Stupidity"|1992}} and in academia.<ref name=SHAPIRO_SHIEBER>{{harvnb|Shapiro|1992|page=10–11}} and {{harvnb|Shieber|1994}}, amongst others.</ref> The first contest was won by a mindless program with no identifiable intelligence that managed to fool naive interrogators into making the wrong identification. This highlighted several of the shortcomings of Turing test (discussed [[#Weaknesses of the test|below]]): The winner won, at least in part, because it was able to "imitate human typing errors";{{sfn|"Artificial Stupidity"|1992}} the unsophisticated interrogators were easily fooled;<ref name=SHAPIRO_SHIEBER/> and some researchers in AI have been led to feel that the test is merely a distraction from more fruitful research.<ref name=SHIEBER>{{harvnb|Shieber|1994|page=77}}</ref>

The silver (text only) and gold (audio and visual) prizes have never been won. However, the competition has awarded the bronze medal every year for the computer system that, in the judges' opinions, demonstrates the "most human" conversational behavior among that year's entries. [[Artificial Linguistic Internet Computer Entity]] (A.L.I.C.E.) has won the bronze award on three occasions in recent times (2000, 2001, 2004). Learning AI [[Jabberwacky]] won in 2005 and 2006.<ref>Jabberwacky is discussed in {{harvnb|Shah|Warwick|2009a}}.</ref>

The Loebner Prize tests conversational intelligence; winners are typically [[chatterbot]] programs, or [[Artificial Conversational Entity (ACE)|Artificial Conversational Entities (ACE)]]s. Early Loebner Prize rules restricted conversations: Each entry and hidden-human conversed on a single topic, thus the interrogators were restricted to one line of questioning per entity interaction. The restricted conversation rule was lifted for the 1995 Loebner Prize. Interaction duration between judge and entity has varied in Loebner Prizes. In Loebner 2003, at the University of Surrey, each interrogator was allowed five minutes to interact with an entity, machine or hidden-human. Between 2004 and 2007, the interaction time allowed in Loebner Prizes was more than twenty minutes. In 2008, the interrogation duration allowed was five minutes per pair, because the organiser, [[Kevin Warwick]], and coordinator, Huma Shah, consider this to be the duration for any test, as Turing stated in his 1950 paper: " ... making the right identification after five minutes of questioning".<ref name=T442>{{Harvnb|Turing|1950|p=442}}</ref> They felt Loebner's longer test, implemented in Loebner Prizes 2006 and 2007, was inappropriate for the state of artificial conversation technology.{{sfn|Shah|2009}} It is ironic that the 2008 winning entry, [[Elbot]], does not mimic a human; its personality is that of a robot, yet Elbot deceived three human judges that it was the human during human-parallel comparisons.<ref>See {{Harvtxt|Shah|Warwick|2010}}. Results and report can be found here [http://www.rdg.ac.uk/research/Highlights-News/featuresnews/res-featureloebner.asp].  
Transcripts can be found at {{cite web
| url= http://www.loebner.net/Prizef/loebner-prize.html
| title = Loebner Prize 
| accessdate = 29 March 2009
}}</ref>

During the 2009 competition, held in Brighton, UK, the communication program restricted judges to 10 minutes for each round, 5 minutes to converse with the human, 5 minutes to converse with the program. This was to test the alternative reading of Turing's prediction that the 5-minute interaction was to be with the computer. For the 2010 competition, the Sponsor has again increased the interaction time, between interrogator and system, to 25 minutes ([http://loebner.net/Prizef/2010_Contest/Loebner_Prize_Rules_2010.html Rules for the 20th Loebner Prize contest]).

===2005 Colloquium on Conversational Systems===
In November 2005, the University of Surrey hosted an inaugural one-day meeting of artificial conversational entity developers,<ref>
{{cite web 
| url=http://www.alicebot.org/bbbbbbb.html
| title=ALICE Anniversary and Colloquium on Conversation
| publisher=A.L.I.C.E. Artificial Intelligence Foundation
| accessdate=29 March 2009
}}</ref>
attended by winners of practical Turing Tests in the Loebner Prize: Robby Garner, Richard Wallace and Rollo Carpenter. Invited speakers included [[David Hamill]], Hugh Loebner (sponsor of the [[Loebner Prize]]) and Huma Shah.

===AISB 2008 Symposium on the Turing Test===
In parallel to the 2008 [[Loebner Prize]] held at the [[University of Reading]],<ref>
{{cite web
| url=http://www.reading.ac.uk/cirg/loebner/cirg-loebner-main.asp 
| title=Loebner Prize 2008 
| publisher=University of Reading 
| accessdate=29 March 2009 }}
</ref>
the [[Society for the Study of Artificial Intelligence and the Simulation of Behaviour]] (AISB), hosted a one-day symposium to discuss the Turing Test, organised by [http://www.cs.bham.ac.uk/~jab/ John Barnden], [[Mark Bishop]], Huma Shah and [[Kevin Warwick]].<ref>
{{cite web
| url= http://www.aisb.org.uk/events/turingevent.shtml 
| title=AISB 2008 Symposium on the Turing Test 
| publisher=Society for the Study of Artificial Intelligence and the Simulation of Behaviour 
| accessdate=29 March 2009 }}
</ref> 
The Speakers included Royal Institution's Director [[Susan Greenfield, Baroness Greenfield|Baroness Susan Greenfield]], [[Selmer Bringsjord]], Turing's biographer [[Andrew Hodges]], and consciousness scientist [[Owen Holland]]. No agreement emerged for a canonical Turing Test, though Bringsjord expressed that a sizeable prize would result in the Turing Test being passed sooner.

===The Alan Turing Year, and Turing100 in 2012===
2012 will <!-- or "The year 2010 will": First letter of a sentence is capitalized-->see a celebration of Turing’s life and scientific impact, with a number of major events taking place throughout the year. Most of these will be linked to places with special significance in Turing’s life, such as Cambridge, Manchester, and [[Bletchley Park]].
The [http://www.cs.swan.ac.uk/turing2012/ Alan Turing Year] is coordinated by the Turing Centenary Advisory Committee (TCAC), representing a range of expertise and organisational involvement in the 2012 celebrations. Supporting organisations for the Alan Turing Year include the [http://www.acm.org/ ACM], the [http://www.aslonline.org/ ASL], the 
[http://www.aisb.org.uk/ SSAISB], the [http://www.bcs.org/ BCS], the [http://www.bctcs.ac.uk/ BCTCS], [http://www.bletchleypark.org.uk/ Bletchley Park], 
the [http://www.gap-system.org/~history/Societies/BMC.html BMC], the [http://www.cs.bham.ac.uk/~exr/blc/ BLC], the [http://www.computerconservationsociety.org/ CCS], the 
[http://www.maths.leeds.ac.uk/cie/ Association CiE], the [http://www.eacsl.org/ EACSL], the [http://www.eatcs.org/ EATCS], [http://folli.loria.fr/ FoLLI], [http://www.ia-cap.org/ IACAP], the [http://www.iacr.org/ IACR], the [http://kgs.logic.at/ KGS], and [http://www2.informatik.hu-berlin.de/lics/ LICS].

Supporting TCAC is [http://www.kevinwarwick.com/turing100.htm Turing100]. With the aim of taking Turing's idea for a thinking machine, picturised in Hollywood movies such as Blade Runner, to a wider audience including children, Turing100 is set up to organise a special Turing test event, celebrating the 100th anniversary of Turing's birth in June 2012, at the place where the mathematician broke codes during the Second World War: Bletchley Park.  The Turing100 team comprises [[Kevin Warwick]] (Chair), Huma Shah (coordinator), Ian Bland, Chris Chapman, Marc Allen; supporters include Rory Dunlop, Loebner winners [[Robby Garner]], and Fred Roberts.

== Versions of the Turing test ==<!-- This title is linked to by the article Computing Machinery and Intelligence -->
[[Image:The Imitation Game.png|thumb|The Imitation Game, as described by Alan Turing in "Computing Machinery and Intelligence." Player C, through a series of written questions, attempts to determine which of the other two players is a man, and which of the two is the woman. Player A, the man, tries to trick player C into making the wrong decision, while player B tries to help player C. Figure adapted from Saygin, 2000.<ref name="Saygin 2000"/>]]
There are at least three primary versions of the Turing test, two of which are offered in "Computing Machinery and Intelligence" and one that Saul Traiger describes as the "Standard Interpretation."<ref name=Traiger2000>{{harvnb|Traiger|2000}}</ref>  While there is some debate regarding whether the "Standard Interpretation" is that described by Turing or, instead, based on a misreading of his paper, these three versions are not regarded as equivalent,<ref name=Traiger2000 /> and their strengths and weaknesses are distinct.<ref>Saygin, A.P. (2008). Comments on “Computing Machinery and Intelligence” by Alan Turing. In  R.Epstein, G. Roberts, G. Poland, (eds.) Parsing the Turing Test. Springer: Dordrecht, Netherlands</ref>

===The Imitation Game===
Turing's original game, as we have seen, described a simple party game involving three players.  Player A is a man, player B is a woman and player C (who plays the role of the interrogator) is of either sex.  In the Imitation Game, player C is unable to see either player A or player B, and can only communicate with them through written notes.  By asking questions of player A and player B, player C tries to determine which of the two is the man and which is the woman.  Player A's role is to trick the interrogator into making the wrong decision, while player B attempts to assist the interrogator in making the right one.<ref name="Saygin 2000"/> 

Sterret refers to this as the "Original Imitation Game Test,"<ref name=Moor2003 /> Turing proposes that the role of player A be filled by a computer.  Thus, the computer's task is to pretend to be a woman and attempt to trick the interrogator into making an incorrect evaluation.  The success of the computer is determined by comparing the outcome of the game when player A is a computer against the outcome when player A is a man.  If, as Turing puts it, "the interrogator decide[s] wrongly as often when the game is played [with the computer] as he does when the game is played between a man and a woman",<ref name=T434/> it may be argued that the computer is intelligent.{{sfn|Shah|Warwick|2010}} and in contrast to Sterrett's opinion, posit that Turing did not expect the design of the machine to imitate a woman, when compared against a human.

[[Image:Turing Test Version 1.png|thumb|The Original Imitation Game Test, in which the player A is replaced with a computer. The computer is now charged with the role of the woman, while player B continues to attempt to assist the interrogator. Figure adapted from Saygin, 2000.<ref name="Saygin 2000"/>]]

The second version appears later in Turing's 1950 paper.  As with the Original Imitation Game Test, the role of player A is performed by a computer, the difference being that the role of player B is now to be performed by a man rather than a woman.

<blockquote>"Let us fix our attention on one particular digital computer ''C.'' Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, ''C'' can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?"<ref name=T434/></blockquote>

In this version, both player A (the computer) and player B are trying to trick the interrogator into making an incorrect decision.

===The standard interpretation===
Common understanding has it that the purpose of the Turing Test is not specifically to determine whether a computer is able to fool an interrogator into believing that it is a human, but rather whether a computer could ''imitate'' a human.<ref name="Saygin 2000"/>  While there is some dispute whether this interpretation was intended by Turing — Sterrett believes that it was<ref name="Moor2003">{{Harvnb|Moor|2003}}</ref> and thus conflates the second version with this one, while others, such as Traiger, do not<ref name="Traiger2000" /> — this has nevertheless led to what can be viewed as the "standard interpretation." In this version, player A is a computer and player B a person of either gender.  The role of the interrogator is not to determine which is male and which is female, but which is a computer and which is a human.<ref>{{harvnb|Traiger|2000|p=99}}</ref>

===Imitation Game vs. Standard Turing Test===
There has arisen some controversy over which of the alternative formulations of the test Turing intended.<ref name=Moor2003 />  Sterrett argues that two distinct tests can be extracted from his 1950 paper and that, ''pace'' Turing's remark, they are not equivalent.  The test that employs the party game and compares frequencies of success is referred to as the "Original Imitation Game Test," whereas the test consisting of a human judge conversing with a human and a machine is referred to as the "Standard Turing Test," noting that Sterrett equates this with the "standard interpretation" rather than the second version of the imitation game.  Sterrett agrees that the Standard Turing Test (STT) has the problems that its critics cite but feels that, in contrast, the Original Imitation Game Test (OIG Test) so defined is immune to many of them, due to a crucial difference: Unlike the STT, it does not make similarity to human performance the criterion, even though it employs human performance in setting a criterion for machine intelligence.  A man can fail the OIG Test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: The OIG Test requires the resourcefulness associated with intelligence and not merely "simulation of human conversational behaviour."  The general structure of the OIG Test could even be used with non-verbal versions of imitation games.<ref>{{Harvnb|Sterrett|2000}}</ref>

Still other writers<ref>{{Harvnb|Genova|1994}}, {{Harvnb|Hayes|Ford|1995}}, {{Harvnb|Heil|1998}}, {{Harvnb|Dreyfus|1979}}</ref> have interpreted Turing as proposing that the imitation game itself is the test, without specifying how to take into account Turing's statement that the test that he proposed using the party version of the imitation game is based upon a criterion of comparative frequency of success in that imitation game, rather than a capacity to succeed at one round of the game.

Saygin has suggested that maybe the original game is a way of proposing a less biased experimental design as it hides the participation of the computer.<ref>R.Epstein, G. Roberts, G. Poland, (eds.) Parsing the Turing Test: Philosophical and Methodological Issues in the Quest for the Thinking Computer. Springer: Dordrecht, Netherlands</ref>

===Should the interrogator know about the computer?===
Turing never makes clear whether the interrogator in his tests is aware that one of the participants is a computer. To return to the Original Imitation Game, he states only that player A is to be replaced with a machine, not that player C is to be made aware of this replacement.<ref name=T434 /> When Colby, FD Hilf, S Weber and AD Kramer tested PARRY, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation.<ref>{{harvnb|Colby|Hilf|Weber|Kraemer|1972}}</ref> As Ayse Saygin and others have highlighted, this makes a big difference to the implementation and outcome of the test.<ref name="Saygin 2000"/> Huma Shah & [[Kevin Warwick]], who have organised practical Turing tests, argue knowing/not knowing ''may'' make a difference in some judges' verdict. Judges in the  finals of the parallel-paired Turing tests, staged in the [http://www.loebner.net/Prizef/2008_Contest/loebner-prize-2008.html 18th Loebner Prize] were not explicitly told, some did assume each hidden pair contained one human and one machine. Spelling errors gave away the hidden-humans; machines were identified by 'speed of response' and lengthier utterances.<ref>
{{Citation|url=http://www.rdg.ac.uk/research/Highlights-News/featuresnews/res-featureloebner.asp  | title=Can a machine think? -Results from the 18th Loebner Prize | place=University of Reading|year=2008}}</ref>  In an experimental study looking at [[Paul_Grice#Conversational_Maxims|Gricean maxim violations]] that also used the Loebner transcripts, Ayse Saygin found significant differences between the responses of participants who knew and did not know about computers being involved.<ref>Saygin, A.P. & Cicekli, I. (2002) Pragmatics in human-computer conversation. Journal of Pragmatics, 34(3): 227-258</ref>

==Strengths of the test==
===Tractability===
The [[philosophy of mind]], [[psychology]], and modern [[neuroscience]] have been unable to provide definitions of "intelligence" and "thinking" that are sufficiently precise and general to be applied to machines. Without such definitions, the central questions of the [[philosophy of artificial intelligence]] cannot be answered. The Turing test, even if imperfect, at least provides something that can actually be measured. As such, it is a pragmatic solution to a difficult philosophical question.

===Breadth of subject matter===
The power of the Turing test derives from the fact that it is possible to talk about anything.  Turing wrote that "the question and answer method seems to be suitable for introducing almost any one of the fields of human endeavor that we wish to include."<ref>{{Harvnb|Turing|1950}} under "Critique of the New Problem"</ref>  [[John Haugeland]] adds that "understanding the words is not enough; you have to understand the ''topic'' as well."<ref>{{Harvnb|Haugeland|1985|p=8}}</ref>

In order to pass a well-designed Turing test, the machine must use [[natural language processing|natural language]], [[commonsense reasoning|reason]], have [[knowledge representation|knowledge]] and [[machine learning|learn]].  The test can be extended to include video input, as well as a "hatch" through which objects can be passed: this would force the machine to demonstrate the skill of [[computer vision|vision]] and [[robotics]] as well.  Together, these represent almost all of the major problems of artificial intelligence.<ref>"These six disciplines," write [[Stuart J. Russell]] and [[Peter Norvig]], "represent most of AI." {{Harvnb|Russell|Norvig|2003|p=3}}</ref>

The [[Feigenbaum test]] is designed to take advantage of the broad range of topics available to a Turing test. It compares the machine against the abilities of experts in specific fields such as [[literature]] or [[chemistry]].

==Weaknesses of the test==
The Turing test is based on the assumption that human beings can judge a machine's intelligence by comparing its behaviour with human behaviour. Every element of this assumption has been questioned: the human's judgement, the value of comparing only behaviour and the value of comparing against a human. Because of these and other considerations, some AI researchers have questioned the usefulness of the test. In practice, the test's results can easily be dominated not by the computer's (pseudo-?) intelligence, but by the attitudes, skill or naiveté of the questioner.

===Human intelligence vs intelligence in general===
[[Image:Weakness of Turing test 1.svg|right|250px]]
The Turing test does not directly test whether the computer behaves intelligently - it tests only whether the computer behaves like a human being. Since human behavior and intelligent behavior are not exactly the same thing, the test can fail to accurately measure intelligence in two ways:

;Some human behavior is unintelligent: The Turing test requires that the machine be able to execute ''all'' human behaviors, regardless of whether they are intelligent.  It even tests for behaviors that we may not consider intelligent at all, such as the susceptibility to insults,<ref>Saygin, A.P. & Cicekli, I. (2002). Journal of Pragmatics, 34, 227-258.</ref> the temptation to [[lie]] or, simply, a high frequency of [[typographical error|typing mistakes]].  If a machine cannot imitate human behavior in detail, it fails the test.

:This objection was raised by  ''[[The Economist]],'' in an article entitled "[[artificial stupidity|Artificial Stupidity]]" published shortly after the first Loebner prize competition in 1992. The article noted that the first Loebner winner's victory was due, at least in part, to its ability to "imitate human typing errors."{{sfn|"Artificial Stupidity"|1992}} Turing himself had suggested that programs add errors into their output, so as to be better "players" of the game.<ref>{{harvnb|Turing|1950|p=448}}</ref>

;Some intelligent behavior is inhuman: The Turing test does not test for highly intelligent behaviors, such as the ability to solve difficult problems or come up with original insights.  In fact, it specifically requires deception on the part of the machine: if the machine is ''more'' intelligent than a human being it must deliberately avoid appearing too intelligent. If it were to solve a computational problem that is impossible for any human to solve, then the interrogator would know the program is not human, and the machine would fail the test.

:Because it can't measure intelligence that is beyond the ability of humans, the test can't be used in order to build or evaluate systems that are more intelligent than humans. Because of this, several test alternatives that would be able to evaluate superintelligent systems have been proposed.<ref>{{Citation | title = Beyond the Turing Test | journal = Journal of Logic, Language and Information | author = Jose Hernandez-Orallo | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.8943 |  accessdate = 2009-07-21 | year = 2000 | pages = 447–466 | volume = 9  | issue = 4 | doi = 10.1023/A:1008367325700 | postscript = . }}</ref><ref>{{Citation | title = A computational extension to the Turing Test | journal = Proceedings of the 4th Conference of the Australasian Cognitive Science Society | author =  D L Dowe and A R Hajek | url = http://www.csse.monash.edu.au/publications/1997/tr-cs97-322-abs.html |  accessdate = 2009-07-21 | year = 1997 | postscript = . }}</ref><ref>{{Citation | title = Universal Intelligence: A Definition of Machine Intelligence | journal = Minds and Machines | author = Shane Legg and Marcus Hutter | url = http://www.vetta.org/documents/UniversalIntelligence.pdf | format = PDF | accessdate = 2009-07-21 | year = 2007 | pages = 391–444 | volume = 17 | doi = 10.1007/s11023-007-9079-x | postscript = . }}</ref><ref name="J Hernandez-Orallo and D L Dowe 2010">{{cite journal | title = Measuring Universal Intelligence: Towards an Anytime Intelligence Test | journal = Artificial Intelligence Journal | author =  J Hernandez-Orallo and D L Dowe | url = http://dx.doi.org/10.1016/j.artint.2010.09.006 |  accessdate = 2010-10-01 | year = 2010| ref = harv }}</ref>

===Real intelligence vs simulated intelligence===
{{See also|synthetic intelligence}}
The Turing test is concerned strictly with how the subject ''acts'' — the external behaviour of the machine. In this regard, it assumes a [[behaviourist]] or [[Functionalism (philosophy of mind)|functionalist]] definition of intelligence. The example of [[ELIZA]] suggested that a machine passing the test may be able to simulate human conversational behavior by following a simple (but large) list of mechanical rules, without thinking or having a mind at all. 

[[John Searle]] argued that external behavior cannot be used to determine if a machine is "actually" thinking or merely "simulating thinking."<ref name="Searle 1980"/> His [[chinese room]] argument is intended to show that, even if the Turing test is a good operational definition of intelligence, it may not indicate that the machine has a [[mind]], [[consciousness]], or [[intentionality]]. (Intentionality is a philosophical term for the power of thoughts to be "about" something.)

Turing anticipated to this line of criticism in his original paper,<ref>{{Harvtxt|Russell|Norvig|2003|pp=958–960}} identify Searle's argument with the one Turing answers.</ref> writing that: {{quote|I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.| Alan Turing, {{Harv|Turing|1950}} }}

===Naivete of interrogators and the anthropomorphic fallacy===
The Turing test assumes that the interrogator is sophisticated enough to determine the difference between the behaviour of a machine and the behaviour of a human being, though critics argue that this is not a skill most people have.

Turing does not specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term "average interrogator": "[the] average interrogator would not have more than 70 per cent chance of making the right identification after five minutes of questioning".{{sfn|Turing|1950|p=442}} {{harvtxt|Shah|Warwick|2009c}}{{citation not found}} show that experts are fooled, and that interrogator strategy, 'power' vs 'solidarity' affects correct identification, the latter being more successful.

Chatterbot programs such as ELIZA have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. In these cases, the "interrogator" is not even aware of the possibility that they are interacting with a computer. To successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required. Most would agree that a "true" Turing test has not been passed in "uninformed" situations like these.{{Citation needed|date=November 2010}}

Early Loebner prize competitions used "unsophisticated" interrogators who were easily fooled by the machines.<ref name=SHAPIRO_SHIEBER/>  Since 2004, the Loebner Prize organizers have deployed philosophers, computer scientists, and journalists among the interrogators. However, even some of these experts have been deceived by the machines.{{sfn|Shah|Warwick|2010}}

[[Michael Shermer]] points out that human beings consistently choose to consider non-human objects as human whenever they are allowed the chance, a mistake called the [[anthropomorphic fallacy]]: They talk to their cars, ascribe desire and intentions to natural forces (e.g., "nature abhors a vacuum"), and worship the sun as a human-like being with intelligence. If the Turing test is applied to religious objects, Shermer argues, then, that inanimate statues, rocks, and places have consistently passed the test throughout history.<ref>{{Harvnb|Shermer|YEAR?}} [[CITATION IN PROGRESS]]</ref> This human tendency towards anthropomorphism effectively lowers the bar for the Turing test, unless interrogators are specifically trained to avoid it.

===Impracticality and irrelevance: the Turing test and AI research===
Mainstream AI researchers argue  that trying to pass the Turing Test is merely a distraction from more fruitful research.<ref name=SHIEBER/> Indeed, the Turing test is not an active focus of much academic or commercial effort—as [[Stuart Russell]] and [[Peter Norvig]] write: "AI researchers have devoted little attention to passing the Turing test."<ref name=RussellNorvig2003p3>{{Harvnb|Russell|Norvig|2003|p=3}}</ref> There are several reasons.

First, there are easier ways to test their programs. Most current research in AI-related fields is aimed at modest and specific goals, such as [[automated planning and scheduling|automated scheduling]], [[object recognition]], or [[logistics]]. In order to test the intelligence of the programs that solve these problems, AI researchers simply give them the task directly, rather than going through the roundabout method of posing the question in a [[chat room]] populated with computers and people.

Second, creating life-like simulations of human beings is a difficult problem on its own that does not need to be solved to achieve the basic goals of AI research. Believable human characters may be interesting in a work of art, a [[video game|game]], or a sophisticated [[user interface]], but they are not part of the science of creating intelligent machines, that is, machines that solve problems using intelligence. Russell and Norvig suggest an analogy with the [[history of flight]]: Planes are tested by how well they fly, not by comparing them to birds.  "[[Aeronautics|Aeronautical engineering]] texts," they write, "do not define the goal of their field as 'making machines that fly so exactly like [[pigeon]]s that they can fool other pigeons.'"<ref name=RussellNorvig2003p3/> 

Turing, for his part, never intended his test to be used as a practical, day-to-day measure of the intelligence of AI programs; he wanted to provide a clear and understandable example to aid in  the discussion of the [[philosophy of artificial intelligence]].<ref>{{Harvnb|Turing|1950}}, under the heading "The Imitation Game," where he writes, "Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words."</ref> As such, it is not surprising that the Turing test has had so little influence on AI research — the philosophy of AI, writes [[John McCarthy (computer scientist)|John McCarthy]], "is unlikely to have any more effect on the practice of AI research than philosophy of science generally has on the practice of science."<ref>John McCarthy [http://www-formal.stanford.edu/jmc/aiphil/node2.html#SECTION00020000000000000000 The philosophy of artificial intelligence]</ref>

==Predictions==
Turing predicted that machines would eventually be able to pass the test; in fact, he estimated that by the year 2000, machines with 10<sup>9</sup> [[bit]]s (about 119.2 [[mebibyte|MiB]] or approximately 120 [[megabyte]]s) of memory would be able to fool thirty percent of human judges in a five-minute test.  He also predicted that people would then no longer consider the phrase "thinking machine" contradictory.  He further predicted that [[machine learning]] would be an important part of building powerful machines, a claim considered plausible by contemporary researchers in artificial intelligence.<ref>{{Harv|Turing|1950|p=442}}</ref>

In a paper submitted to 19th Midwest Artificial Intelligence and Cognitive Science Conference, Dr.Shane T. Mueller predicted a modified Turing Test called a "Cognitive Decathlon" could be accomplished within 5 years.<ref>{{Citation | title = Is the Turing Test Still Relevant? A Plan for Developing the Cognitive Decathlon to Test Intelligent Embodied Behavior| journal = Paper submitted to the 19th Midwest Artificial Intelligence and Cognitive Science Conference | author = Shane T. Mueller, Ph.D.| url = http://www.dod.mil/pubs/foi/darpa/08_F_0799Is_the_Turing_test_Still_Relevant.pdf | accessdate = 2010-09-08 | year = 2008 | pages = 8pp }}</ref>

By extrapolating an [[technological singularity#Accelerating change|exponential growth]] of technology over several decades, [[futurology|futurist]] [[Raymond Kurzweil]] predicted that Turing test-capable computers would be manufactured in the near future. In 1990, he set the year around 2020.<ref>{{Harvnb|Kurzweil|1990}}</ref> By 2005, he had revised his estimate to 2029.<ref>{{Harvnb|Kurzweil|2005}}</ref>

The [[Long Bet Project]] is a wager of [[United States dollar|$]]20,000 between [[Mitch Kapor]] (pessimist) and Kurzweil (optimist) about whether a computer will pass a Turing Test by the year 2029.  The bet specifies the conditions in some detail.<ref>[http://www.longbets.org/1#terms Long Bets - By 2029 no computer - or "machine intelligence" - will have passed the Turing Test]</ref>

==Variations of the Turing test==
Numerous other versions of the Turing test, including those expounded above, have been mooted through the years.

===Reverse Turing test and CAPTCHA===
{{main|reverse Turing test|CAPTCHA}}
A modification of the Turing test wherein the objective of one or more of the roles have been reversed between machines and humans is termed a reverse Turing test.  An example is implied in the work of psychoanalyst [[Wilfred Bion]],<ref>{{Harvnb|Bion|1979}}</ref> who was particularly fascinated by the "storm" that resulted from the encounter of one mind by another.  Carrying this idea forward, [[R. D. Hinshelwood]]<ref>{{Harvnb|Hinshelwood|2001}}</ref> described the mind as a "mind recognizing apparatus," noting that this might be some sort of "supplement" to the Turing test.  The challenge would be for the computer to be able to determine if it were interacting with a human or another computer.  This is an extension of the original question that Turing attempted answer but would, perhaps, offer a high enough standard to define a machine that could "think" in a way that we typically define as characteristically human.

[[CAPTCHA]] is a form of reverse Turing test.  Before being allowed to perform some action on a [[website]], the user is presented with alphanumerical characters in a distorted graphic image and asked to type them out.  This is intended to prevent automated systems from being used to abuse the site.  The rationale is that software sufficiently sophisticated to read and reproduce the distorted image accurately does not exist (or is not available to the average user), so any system able to do so is likely to be a human.

Software that can reverse CAPTCHA with some accuracy by analyzing patterns in the generating engine is being actively developed.<ref>{{Citation 
| url=http://www.cs.sfu.ca/~mori/research/gimpy/
| title=Breaking a Visual CAPTCHA
| authors=Greg Mori and [[Jitendra Malik]]
}}</ref>

==="Fly on the wall" Turing test===
{{Unreferenced section|date=May 2009}}

The "fly on the wall" variation of the Turing test changes the original Turing-test parameters in three ways.  First, parties A and B communicate with each other rather than with party C, who plays the role of a detached observer ("[[fly on the wall]]") rather than of an interrogator or other participant in the conversation.  Second, party A and party B may each be either a human or a computer of the type being tested.  Third, it is specified that party C must not be informed as to the identity (human versus computer) of either participant in the conversation.  Party C's task is to determine which of four possible participant combinations (human A/human B, human A/computer B, computer A/human B, computer A/computer B) generated the conversation.  At its most rigorous, the test is conducted in numerous iterations, in each of which the identity of each participant is determined at random (e.g., using a fair-coin toss) and independently of the determination of the other participant's identity, and in each of which a new human observer is used (to prevent the discernment abilities of party C from improving through conscious or unconscious [[pattern recognition]] over time).  The computer passes the test for human-level intelligence if, over the course of a [[statistical significance|statistically significant]] number of iterations, the respective parties C are unable to determine with better-than-chance frequency which participant combination generated the conversation.

The "fly on the wall" variation increases the [[Multiple intelligences|scope]] of intelligence being tested in that the observer is able to evaluate not only the participants' ability to answer questions but their capacity for other aspects of intelligent communication, such as the generation of questions or comments regarding an existing aspect of a conversation subject ("deepening"), the generation of questions or comments regarding new subjects or new aspects of the current subject ("broadening"), and the ability to abandon certain subject matter in favor of other subject matter currently under discussion ("narrowing") or new subject matter or aspects thereof ("shifting").

The [[Wilfred Bion|Bion]]-[[R. D. Hinshelwood|Hinshelwood]] extension of the traditional test is applicable to the "fly on the wall" variation as well, enabling the testing of intellectual functions involving the ability to recognize intelligence:  If a computer placed in the role of party C (reset after each iteration to prevent pattern recognition over time) can identify conversation participants with a success rate equal to or higher than the success rate of a set of humans in the party-C role, the computer is functioning at a human level with respect to the skill of intelligence recognition.

===Subject matter expert Turing test===
{{main|Subject matter expert Turing test}}
Another variation is described as the [[subject matter expert]] Turing test, where a machine's response cannot be distinguished from an expert in a given field. This is also known as a "Feigenbaum test" and was proposed by [[Edward Feigenbaum]] in a 2003 paper.<ref>{{Harvnb|McCorduck|2003|pp=503–505}}, {{Harvnb|Feigenbaum|2003}}. The subject matter expert test is also mentioned in {{Harvtxt|Kurzweil|2005}}</ref>

===Immortality test===
{{main|Immortality test}}
The Immortality-test variation of the Turing test would determine if a person's essential character is reproduced with enough fidelity to make it impossible to distinguish a reproduction of a person from the original person.

===Minimum Intelligent Signal Test===
{{main|Minimum Intelligent Signal Test}}
The Minimum Intelligent Signal Test, proposed by [[Chris McKinstry]], is another variation of Turing's test, where only binary responses are permitted.  It is typically used to gather statistical data against which the performance of artificial intelligence programs may be measured.

===Meta Turing test===
Yet another variation is the Meta Turing test, in which the subject being tested (say, a computer) is classified as intelligent if it has created something that the subject itself wants to test for intelligence.

===Hutter Prize===
The organizers of the [[Hutter Prize]] believe that compressing natural language text is a hard AI problem, equivalent to passing the Turing test.

The data compression test has some advantages over most versions and variations of a Turing test, including:

*It gives a single number that can be directly used to compare which of two machines is "more intelligent."
*It does not require the computer to lie to the judge

The main disadvantages of using data compression as a test are:
*It is not possible to test humans this way.
*It is unknown what particular "score" on this test—if any—is equivalent to passing a human-level Turing test.

===Other tests based on compression or Kolmogorov Complexity===

A related approach to Hutter's prize which appeared in the late 1990s is the inclusion of compression problems in an extended Turing Test.<ref>{{cite journal | title = A computational extension to the Turing Test | journal = Proceedings of the 4th Conference of the Australasian Cognitive Science Society | author =  D L Dowe and A R Hajek | url = http://www.csse.monash.edu.au/publications/1997/tr-cs97-322-abs.html |  accessdate = 2009-07-21 | year = 1997 | ref = harv }}</ref> Other related tests in this line are,.<ref>{{cite journal | title = Beyond the Turing Test | journal = Journal of Logic, Language and Information | author = Jose Hernandez-Orallo | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.8943 |  accessdate = 2009-07-21 | year = 2000 | pages = 447–466 | volume = 9  | issue = 4 | doi = 10.1023/A:1008367325700 | ref = harv }}</ref><ref name="J Hernandez-Orallo and D L Dowe 2010"/>
Two major advantages of some of these tests are their applicability to nonhuman intelligences and their absence of a requirement for human testers.

==See also==
* [[Artificial intelligence in fiction]]
* [[Graphics Turing Test]]
* [[HAL 9000]] (from 2001: A Space Odyssey)
* [[GLaDOS]]
* [[Mark V Shaney]] (USENET bot)
* [[Simulated reality]]
* [[Technological singularity]]
* [[Uncanny valley]]
* [[Voight-Kampff machine]]
* [[CAPTCHA]]
* [[SHRDLU]]

==Notes==
{{reflist|2}}

==References==
{{Refbegin}}

* {{Citation | title = Artificial Stupidity | date = 1992-09-01 | journal = [[The Economist]] | volume = 324 | issue = 7770 | pages = 14 | ref={{harvid|"Artificial Stupidity"|1992}}}}
* {{Citation | last=Bion | first=W.S. | year=1979 | chapter=Making the best of a bad job | title=Clinical Seminars and Four Papers | publisher=Abingdon: Fleetwood Press. }}
* {{Citation | last = Bowden | first = Margaret A. | year = 2006| title = Mind As Machine: A History of Cognitive Science | publisher = [[Oxford University Press]] | isbn = 9780199241446}}
* {{Citation | last = Colby | first = K. M. | last2 = Hilf | first2 = F. D. | last3 = Weber | first3 = S. | last4 = Kraemer | year = 1972 | title = Turing-like indistinguishability tests for the validation of a computer simulation of paranoid processes | journal = [[Artificial Intelligence (journal)|Artificial Intelligence]] | volume = 3 | pages = 199–221 | doi = 10.1016/0004-3702(72)90049-5 | first4 = H.}}
* {{Citation | last = Copeland | first = Jack | authorlink = Jack Copeland | year = 2003 | title = The Turing Test | work = The Turing Test: The Elusive Standard of Artificial Intelligence | editor-last = Moor | editor-first = James | publisher = Springer | isbn = 1-40-201205-5}}
* {{Citation  | last = Crevier | first = Daniel | authorlink = Daniel Crevier | year = 1993 | title = AI: The Tumultuous Search for Artificial Intelligence | location = New York, NY | publisher = BasicBooks | isbn = 0-465-02997-3}}
* {{Citation | last = Dreyfus | first = Hubert  | title = What Computers ''Still'' Can't Do | year =1979 | publisher = MIT Press | location = New York | authorlink = Hubert Dreyfus|isbn= ISBN 0-06-090613-8}}
* {{Citation |last=Feigenbaum|first=Edward A.| author-link=Edward Feigenbaum| year=2003|title=Some challenges and grand challenges for computational intelligence|journal=Journal of the ACM|volume=50|issue=1|pages=32–40|doi=10.1145/602382.602400}}
* {{Citation | last=Genova | first=J. | year=1994 | title=Turing's Sexual Guessing Game | journal=Social Epistemology | volume=8 | pages=314–326 | doi=10.1080/02691729408578758 | issue=4}}
* {{Citation| last=Harnad | first=Stevan | author-link=Stevan Harnad | year=2004 | contribution=The Annotation Game: On Turing (1950) on Computing, Machinery, and Intelligence | url=http://cogprints.org/3322/ | editor-last=Epstein | editor-first=Robert | editor2-last=Peters | editor2-first=Grace | title=The Turing Test Sourcebook: Philosophical and Methodological Issues in the Quest for the Thinking Computer | publisher=Klewer }}
* {{Citation |  last = Haugeland | first = John |year = 1985 | title = Artificial Intelligence: The Very Idea | publisher=MIT Press | author-link = John Haugeland| publication-place= Cambridge, Mass.}}.
* {{Citation | last=Hayes | first=Patrick | last2=Ford | first2=Kenneth | year=1995 | title=Turing Test Considered Harmful | journal=Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI95-1),  Montreal, Quebec, Canada. |  pages=972–997}}
* {{Citation| last=Heil | first=John | year=1998 | title=Philosophy of Mind:  A Contemporary Introduction | location=London and New York | publisher=Routledge | isbn=0-415-13060-3 }}
* {{Citation | last=Hinshelwood | first=R.D. | year=2001 | title=Group Mentality and Having a Mind: Reflections on Bion's work on groups and on psychosis }}
* {{Citation | last=Kurzweil | first=Ray | author-link=Ray Kurzweil | year=1990 | title=The Age of Intelligent Machines | isbn=0-262-61079-5 | publisher=MIT Press | location=Cambridge, Mass. }}
* {{Citation | last=Kurzweil | first=Ray | author-link=Ray Kurzweil | year=2005 | title=The Singularity is Near | publisher=Penguin Books | isbn=0-670-03384-7 }}
* {{Citation | last = Loebner | first = Hugh Gene | authorlink = Hugh Loebner | title = In response | year = 1994 | journal = [[Communications of the ACM]] | volume = 37 | issue = 6 | pages = 79–82 | url = http://loebner.net/Prizef/In-response.html | accessdate = 2008-03-22 | doi = 10.1145/175208.175218}}
* {{McCorduck 2004}}
* {{Citation| editor-last=Moor | editor-first=James | year=2003 | title=The Turing Test: The Elusive Standard of Artificial Intelligence | isbn=1-4020-1205-5| publisher=Kluwer Academic Publishers| location=Dordrecht }}
* {{Citation | last=Penrose | first=Roger | author-link=Roger Penrose | year=1989 | title= [[The Emperor's New Mind|The Emperor's New Mind: Concerning Computers, Minds, and The Laws of Physics]]|publisher= Oxford University Press| isbn=0-14-014534-6 }}
* {{Citation | last = Russell | first = Stuart J. | authorlink = Stuart J. Russell | last2 = Norvig | first2 = Peter | authorlink2 = Peter Norvig | year = 2003 | title = [[Artificial Intelligence: A Modern Approach]] | edition = 2nd | location = Upper Saddle River, NJ | publisher = Prentice Hall | isbn = 0-13-790395-2}}
*{{citation|doi=10.1023/A:1011288000451|first=A. P.|last=Saygin| authorlink = Ayse P. Saygin|last2=Cicekli|year=2000|first2=Ilyas|last3=Akman|first3=Varol|title=Turing Test: 50 Years Later|journal=Minds and Machines|volume=10|issue=4|pages=463–518|url=http://crl.ucsd.edu/~saygin/papers/MMTT.pdf}}.
*{{citation|doi=10.1023/A:1011288000451|first1=A. P.|last1=Saygin|first2=I.|last2=Cicekli|first3=V.|last3=Akman| year=2000|title=Turing Test: 50 Years Later|journal=Minds and Machines|volume=10|issue=4|pages=463–518|url=http://crl.ucsd.edu/~saygin/papers/MMTT.pdf}}. Reprinted in {{citation|title=The Turing Test: The Elusive Standard of Artificial Intelligence|editor-first=James H.|editor-last=Moor|publisher=Kluwer Academic|year=2003|isbn=1-4020-1205-5|pages=23–78}}.
* {{citation | doi=10.1016/S0378-2166(02)80001-7 | first1=A. P.|last1=Saygin|first2=I.|last2=Cicekli| authorlink = Ayse P. Saygin |year=2002|title=Pragmatics in human-computer conversation|journal=Journal of Pragmatics|volume=34|issue=3|pages=227–258}}.
* {{Citation| doi=10.1017/S0140525X00005756| last=Searle| first=John | author-link=John Searle | year=1980 | url = http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html  | title = Minds, Brains and Programs | journal = Behavioral and Brain Sciences | volume = 3| issue = 3| pages= 417–457}}. ''Page numbers above refer to a standard [[pdf]] print of the article. See also Searle's [http://www.bbsonline.org/Preprints/OldArchive/bbs.searle2.html original draft].''
* {{Citation | url = http://humashah.blogspot.com/ 
| last = Shah | first = Huma 
| date = 15 January 2009
| title = Winner of 2008 Loebner Prize for Artificial Intelligence
| publisher = Conversation, Deception and Intelligence
| accessdate = 29 March 2009
}}.
* {{Citation | last = Shah | first = Huma | last2 = Warwick | first2 = Kevin |  year = 2009a | title = Emotion in the Turing Test: A Downward Trend for Machines in Recent Loebner Prizes}} in {{citation | title = Handbook of Research on Synthetic Emotions and Sociable Robotics: New Applications in Affective Computing and Artificial Intelligence | editor1-last = Vallverdú | editor-first = Jordi |  editor2-last = Casacuberta | editor2-first = David |year = 2009 | publisher = Information Science, IGI | isbn = 978-1-60566-354-8}}
* {{Citation | last = Shah | first = Huma | last2 = Warwick | first2 = Kevin | year = 2009b | title=Hidden Interlocutor Misidentification in Practical Turing Tests}} submitted to journal, November 2009).
* {{Citation | last = Shah | first = Huma | last2 = Warwick | first2 = Kevin |  year = 2010 | title = Testing Turing's Five Minutes Parallel-paired Imitation Game}} in {{citation | journal = [[Kybernetes (journal)|Kybernetes]] Turing Test Special Issue | volume = 4 | date= April 2010| year=2010| pages =  | doi = }}
* {{Citation | last = Shapiro | first = Stuart C. | title = The Turing Test and the economist | year = 1992 | journal = ACM SIGART Bulletin | volume = 3 | issue = 4 | pages = 10–11 | doi = 10.1145/141420.141423}}
* {{Citation | last = Shieber | first = Stuart M. | title = Lessons from a Restricted Turing Test | year = 1994 | journal = [[Communications of the ACM]] | volume = 37 | issue = 6 | pages = 70–78 | url = http://www.eecs.harvard.edu/shieber/Biblio/Papers/loebner-rev-html/loebner-rev-html.html | accessdate = 2008-03-25 | doi = 10.1145/175208.175217}}
* {{Citation | last=Sterrett | first=S. G. | year=2000 | title=Turing's Two Test of Intelligence | journal=Minds and Machines | volume=10 | doi=10.1023/A:1011242120015 | pages=541 | issue=4}} (reprinted in The Turing Test: The Elusive Standard of Artificial Intelligence edited by James H. Moor, Kluwer Academic 2003) ISBN 1-4020-1205-5
* {{Citation | last = Sundman | first = John | title = Artificial stupidity | journal = [[Salon.com]] | date = February 26, 2003 | url = http://dir.salon.com/story/tech/feature/2003/02/26/loebner_part_one/index.html | accessdate = 2008-03-22}}
* {{Citation | last = Thomas | first = Peter J. | year = 1995 | title = The Social and Interactional Dimensions of Human-Computer Interfaces | publisher = [[Cambridge University Press]] | isbn = 052145302X }}
* {{Citation | last = Traiger | first = Saul | year = 2000 | title= Making the Right Identification in the Turing Test| journal=Minds and Machines | volume=10 | doi = 10.1023/A:1011254505902 | pages = 561 | issue=4}} (reprinted in The Turing Test: The Elusive Standard of Artificial Intelligence edited by James H. Moor, Kluwer Academic 2003) ISBN 1-4020-1205-5
* {{Citation | last = Turing | first = Alan | authorlink=Alan Turing | year=1948 | chapter=Machine Intelligence | title = The Essential Turing: The ideas that gave birth to the computer age | editor=Copeland, B. Jack | isbn = 0-19-825080-0 | publisher = Oxford University Press | location = Oxford }}
* {{Turing 1950}}
* {{Citation | last = Turing | first = Alan | authorlink=Alan Turing | year=1952 | chapter = Can Automatic Calculating Machines be Said to Think? |  title = The Essential Turing: The ideas that gave birth to the computer age | editor=Copeland, B. Jack | isbn = 0-19-825080-0 | publisher = Oxford University Press | location = Oxford }}
* {{Citation | last=Zylberberg | first=A. | last2=Calot | first2=E. | year=2007 | title=Optimizing Lies in State Oriented Domains based on Genetic Algorithms | journal= Proceedings VI Ibero-American Symposium on Software Engineering |  pages=11–18 | isbn = 978-9972-2885-1-7 }}
* {{Citation | last = Weizenbaum | first = Joseph | authorlink = Joseph Weizenbaum | title = ELIZA - A Computer Program For the Study of Natural Language Communication Between Man And Machine | journal = [[Communications of the ACM]] | volume = 9| issue = 1 | date = January 1966 | pages = 36–45 | doi = 10.1145/365153.365168 }}
* {{Citation | last = Whitby | first = Blay | year = 1996 | contribution = The Turing Test: AI's Biggest Blind Alley? | title = Machines and Thought: The Legacy of Alan Turing | volume = 1 | editor = Millican, Peter & Clark, Andy | publisher = [[Oxford University Press]] | pages = 53–62 | isbn = 0-19-823876-2 }}
* {{Citation | last = Adams | first = Scott | year = 2008 | title = Dilbert | distributed by = [[UFS, inc.]] | url =http://www.dilbert.com/comics/dilbert/archive/images/dilbert2008033349280.jpg}}
{{Refend}}

==Further reading==
{{Refbegin}}
*{{citation|first=Paul R.|last=Cohen|year=2006|url=http://www.cs.arizona.edu/~cohen/Publications/papers/IfNotWhat.pdf|title='If Not Turing's Test, Then What?|journal=[[AI Magazine]]|volume=26|issue=4}}.
{{Refend}}

==External links==
* [http://www.fil.ion.ucl.ac.uk/~asaygin/tt/ttest.html Turing Test Page]
* [http://www.turingtestopera.com The Turing Test - an Opera by Julian Wagstaff]
* {{dmoz|Computers/Artificial_Intelligence/Natural_Language/Turing_Test|Turing Test}}
* [http://www.rmcybernetics.com/science/cybernetics/ai_turing_test.htm The Turing Test]- How accurate could the turing test really be?
* [http://plato.stanford.edu Stanford Encyclopedia of Philosophy] entry on [http://plato.stanford.edu/entries/turing-test the Turing test], by G. Oppy and [http://www.csse.monash.edu.au/~dld D. Dowe].
* [http://crl.ucsd.edu/~saygin/papers/MMTT.pdf Turing Test: 50 Years Later] reviews a half-century of work on the Turing Test, from the vantage point of 2000.
* [http://www.longbets.org/1 Bet between Kapor and Kurzweil], including detailed justifications of their respective positions.
* [http://www.cogs.susx.ac.uk/users/blayw/tt.html Why The Turing Test is AI's Biggest Blind Alley] by Blay Witby
* [http://www.turinghub.com TuringHub.com] Take the Turing Test, live, online
* [http://www.jabberwacky.com Jabberwacky.com] An AI [[chatterbot]] that learns from and imitates humans
* New York Times essays on machine intelligence [http://www.rci.rutgers.edu/~cfs/472_html/Intro/NYT_Intro/History/MachineIntelligence2.html part 1] and [http://www.rci.rutgers.edu/~cfs/472_html/Intro/NYT_Intro/History/MachineIntelligence2.html part 2]
* [http://vvi.onstreammedia.com/cgi-bin/visearch?user=pbs-saf&template=template.html&query=turing&category=0&viKeyword=turing&submit=Search Machines Who Think"]: [[Scientific American Frontiers]] video on "the first ever [restricted] Turing test."
* [[n:Computer professionals celebrate 10th birthday of A.L.I.C.E.|Wiki News:]] "Talk:Computer professionals celebrate 10th birthday of A.L.I.C.E."

{{DEFAULTSORT:Turing Test}}
[[Category:Turing tests| ]]
[[Category:Philosophy of artificial intelligence]]
[[Category:Alan Turing]]
[[Category:Human–computer interaction]]
[[Category:History of artificial intelligence]]

[[bg:Тест на Тюринг]]
[[ca:Test de Turing]]
[[cs:Turingův test]]
[[da:Turing-test]]
[[de:Turing-Test]]
[[et:Turingi test]]
[[es:Test de Turing]]
[[eo:Testo de Turing]]
[[fa:آزمون تورینگ]]
[[fr:Test de Turing]]
[[gl:Proba de Turing]]
[[ko:튜링 테스트]]
[[hr:Turingov test]]
[[is:Próf Turings]]
[[it:Test di Turing]]
[[he:מבחן טיורינג]]
[[la:Examen Turing]]
[[lv:Tjūringa tests]]
[[lt:Tiuringo testas]]
[[hu:Turing-teszt]]
[[nl:Turingtest]]
[[ja:チューリング・テスト]]
[[no:Turingtest]]
[[pl:Test Turinga]]
[[pt:Teste de Turing]]
[[ro:Testul Turing]]
[[ru:Тест Тьюринга]]
[[simple:Turing test]]
[[sk:Turingov test]]
[[sr:Тјурингов тест]]
[[fi:Turingin testi]]
[[sv:Turingtest]]
[[th:การทดสอบของทัวริง]]
[[tr:Turing Testi]]
[[uk:Тест Тюрінга]]
[[vi:Phép thử Turing]]
[[zh:圖靈試驗]]</body> </html>