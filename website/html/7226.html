<html> <head> <title>Comparison of programming paradigms</title></head><body>{{refimprove|date=December 2010}}
{{Programming paradigms}}
This article attempts to set out the various similarities and differences between the various [[programming paradigm]]s as a summary in both graphical and tabular format with links to the separate discussions concerning these similarities and differences in existing Wikipedia articles
== Main paradigm approaches ==
{{Original research|section|date=January 2010}}
The following are considered {{By whom|date=January 2010}} the main programming paradigms. There is inevitably some overlap in these non mutually-exclusive  paradigms but the main features or identifiable differences are summarized in the following table:
 
* [[Imperative programming]] - describes computation in terms of [[statement (programming)|statement]]s that change a program [[state (computer science)|state]] 
* [[Functional programming]] - treats [[computation]] as the evaluation of [[function (mathematics)|mathematical function]]s and avoids [[program state|state]] and [[immutable object|mutable]] data.
* [[Procedural programming]] / [[structured programming]] - specifying the steps the program must take to reach the desired state
* [[Event-driven programming]] - the [[Program flow|flow of the program]] is determined by [[event (computing)|event]]s&mdash;i.e., [[sensor]] outputs or user actions ([[computer mouse|mouse]] clicks, key presses) or [[Message passing|messages]] from other programs or [[Thread (computer science)|threads]].
* [[Object oriented programming]] (OOP) - uses "[[Object (computer science)|objects]]" &ndash; [[data structures]] consisting of [[Field (computer science)|datafields]] and [[Method (computer science)|methods]] together with their interactions &ndash; to design applications and computer programs.
* [[Declarative programming]] - expresses the logic of a [[computation]] without describing its [[control flow]]
* [[Automata-based programming]] - in which the program or its part is thought of as a model of a finite state machine or any other  formal automata.

None of the main programming paradigms have a precise, globally unanimous definition, let alone an official international standard. Nor is there any agreement on which paradigm constitutes the best approach to developing software. The subroutines that actually implement OOP [[Method (computer science)|methods]] might be ultimately coded in an imperative, functional or procedural style that might, or might not, directly alter [[state (computer science)|state]] on behalf of the invoking program.

{| class="wikitable sortable"
|- 
! [[Programming paradigm|Paradigm]]
! Description 
! Main characteristics (examples)
! Related paradigm(s)	
! Critics?
|-
! [[Imperative programming|Imperative]]
| Computation in terms of [[statement (programming)|statement]]s that ''directly'' change a program [[state (computer science)|state]] ([[Field (computer science)|datafields]]) 
| Direct [[Assignment (computer science)|assignment]]s, Common [[data structure]]s, [[Global variable]]s
|
| [[Edsger W. Dijkstra]], [[Michael A. Jackson]] 
|-
! [[structured programming|Structured]]
| A style of [[Imperative programming]] with more logical program structure
| [[Structogram]]s, [[Indent style|Indentation]], absence of [[GOTO]] statements
| [[Imperative programming|Imperative]]
| 
|- 
! [[Functional programming|Functional]]
| Treats [[computation]] as the evaluation of [[function (mathematics)|mathematical function]]s avoiding [[program state|state]] and [[immutable object|mutable]] data.
| [[Lambda calculus]], [[Compositionality]], [[Formula]], [[Referential transparency (computer science)|Referential transparency]], 
no [[Side effect (computer science)|side effects]]
|
| 
|-
! [[Procedural programming|Procedural]]
| Derived from [[structured programming]], based upon the concept of [[modular programming]] or the ''procedure call''
| [[Local variable]]s, sequence, selection, and [[iteration]] and [[Modular programming|modularization]]
| [[structured programming|Structured]], [[imperative programming|Imperative]]
| 
|-
! [[Event-driven programming|Event-driven]] including [[Time-driven programming|time driven]]
| [[Program flow]] is mainly determined by [[event (computing)|event]]s (such as [[mouse click]]s or interrupts including timer)
| [[Main loop]], [[Event handler]]s, [[Asynchronous programming|Asynchronous processes]]
| [[Procedural programming|Procedural]]
| 
|- 
! [[Object oriented programming|Object-oriented]]
| Treats [[Field (computer science)|datafields]] as "objects" manipulated only through pre-defined [[Method (computer science)|methods]]
| [[Object (computer science)|Objects]], [[method (computer science)|Methods]], [[Message passing]], [[Information hiding]], [[Data abstraction]], [[Encapsulation (computer science)|Encapsulation]], [[Polymorphism in object-oriented programming|Polymorphism]], [[Inheritance (computer science)|Inheritance]] and [[Serialization|Serialization/Marshalling]]
|
| [[Object-oriented_programming#Criticism|See here]] and <ref>{{cite web|url=http://www.geocities.com/tablizer/oopbad.htm |title=Object Oriented Programming Oversold |last=Jacobs |first=B.|date=2006-08-27|archiveurl=http://web.archive.org/web/20061015181417/http://www.geocities.com/tablizer/oopbad.htm|archivedate=2006-10-15}}</ref><ref name="flaws"/><ref name="executioniKoN"/>
|- 
! [[Declarative programming|Declarative]]
| Expresses the logic of a [[computation]] without describing its detailed [[control flow]]
| ([[4GL]]s, [[Spreadsheet]]s, [[Report program generator]]s)
|
|  
|- 
! [[Automata-based programming]] 
| The program is thought of as a model of a [[finite state machine]] or any other formal automata.
| [[Enumeration|State enumeration]], [[Control variable]], Changes in [[state (computer science)|state]], [[Isomorphism]], [[State transition table]]
| [[Imperative programming|Imperative]], [[Event-driven programming|Event-driven]]
| 
|- 
|-class="sortbottom"
! [[Programming paradigm|Paradigm]]
! Description 
! Main characteristics (examples)
! Related paradigm(s)
! Critics?
|}

== Differences in terminology ==
Despite multiple (types of) programming [[paradigm]]s existing in parallel (with sometimes apparently conflicting definitions), many of the underlying ''[[Essence|fundamental components]]'' remain more or less the same ([[Constant (programming)|constant]]s, [[Variable (programming)|variable]]s, [[Field (computer science)|datafield]]s, [[subroutine]]s, Calls etc.) and must somehow therefore inevitably be incorporated into each separate paradigm with equally similar attributes or functions. The table below is not intended as a guide to precise similarities, but more an index of where to look for more information - based on the different naming of these entities - within each paradigm. Non-standardized implementations of each paradigm in numerous [[programming language]]s further complicate the overall picture, especially those languages that support [[Multi-paradigm programming language|multiple paradigms]], each with its own [[jargon]].

== Support for new paradigms, supporting languages & syntactic sugar ==
{{Main|Syntactic sugar}}
[[Syntactic sugar]] is a term used to describe the sweetening of program functionality by the introduction of "new features" together with its new terminology. One example of syntactic sugar may arguably be classes in [[C++]] (as well as in [[Java (programming language)|Java]], [[C Sharp (programming language)|C#]], etc.). The [[C programming language]] is fully capable of [[object-oriented programming]] using its powerful facilities of [[function pointer]]s, type casting, and structures. However, it is claimed {{Who|date=June 2010}} that languages such as [[C++]] make object-oriented programming more convenient by introducing syntax specific to this coding style. Moreover, the specialized syntax works to emphasize the object-oriented approach to new programmers. Another example of syntactic sugar may arguably be functions and looping syntax in [[C (programming language)|C]] (as well as other procedural and structured programming languages). [[Assembly language]] is fully capable of procedural or structured programming using its powerful facilities for modifying register values and branching execution depending on program state. However, it is claimed that languages such as C make procedural and structured programming more convenient by introducing syntax specific to these coding styles. Moreover, the specialized syntax works to emphasize the structured approach to new programmers. Features of the C# (C Sharp) programing language, such as properties and interfaces, similarly do not enable new functionality, but instead (it is claimed), make good programming practices more prominent and more natural.

Some programmers feel that these features are either unimportant or outright frivolous. For example, [[Alan Perlis]] once quipped, in a reference to [[Curly bracket programming language|bracket-delimited languages]], that "syntactic sugar causes cancer of the [[semicolon]]" (see [[Epigrams on Programming]]). 

An extension of this is the term "[[Syntactic_sugar#Syntactic_saccharin|syntactic saccharin]]", meaning gratuitous syntax which does not actually make programming easier<ref>{{cite web|url=http://www.retrologic.com/jargon/S/syntactic-sugar.html|title= The Jargon File v4.4.7: "syntactic sugar"}}</ref>.

== Performance comparison ==
Purely in terms of total [[instruction path length]], a program coded in an imperative style, without using any subroutines at all would have the lowest count. However, the [[binary file|binary]] size of such a program might be larger than the same program coded using subroutines (as in functional and procedural programming) and would reference more "[[locality of reference|"non-local"]] ''physical'' instructions that may increase [[cache misses]] and increase instruction fetch [[Computational overhead|overhead]] in modern processors. 

The paradigms that use subroutines extensively (including functional, procedural and object oriented) and do not also use significant [[inlining]] (via [[compiler optimization]]s) will, consequently, use a greater percentage of total resources on the subroutine linkages themselves. Object oriented programs - that deliberately do not alter [[program state]] directly - instead using [[mutator method]]s (or "setters") to encapsulate these state changes - will, as a direct consequence, have a greater overhead (since [[message passing]] is essentially a subroutine call - but with three more additional overheads; [[dynamic memory allocation]], parameter copying and [[dynamic dispatch]]). Obtaining memory from the [[dynamic memory allocation|heap]] and copying parameters for message passing may involve significant resources that far exceed those required for the state change itself. Assessors (or "getters") - that merely return the values of private member variables - also depend upon similar message passing subroutines, instead of using a more direct assignment (or comparison) adding to total path length.
=== Pseudocode examples comparing various paradigms ===
A pseudocode comparison of imperative, procedural, and object oriented approaches used to calculate the area of a circle (<math>
   \pi r^2.\, </math>), assuming no subroutine [[inlining]], no [[macro (computer science)|macro]] preprocessors, register arithmetic and weighting each instruction 'step' as just 1 instruction - as a crude measure of [[instruction path length]] -  is presented below. The instruction step that is conceptually performing the actual state change is highlighted in bold typeface in each case.  Note that the actual arithmetic operations used to compute the area of the circle are the same in all three paradigms, with the difference being that the procedural and object-oriented paradigms wrap those operations in a subroutine call that makes the computation general and reusable. The same effect could be achieved in a [[purely imperative]] program using a macro preprocessor at just the cost of increased program size (only at each macro invocation site) without a corresponding [[pro rata]] runtime cost (proportional to ''n'' invocations - that maybe situated within an [[inner loop]] for instance). Conversely, subroutine inlining by a compiler could reduce procedural programs to something similar in size to the purely imperative code. However, for object-oriented programs, even with inlining, messages still have to be built (from copies of the arguments) for processing by the object oriented methods. The overhead of calls, virtual or otherwise, is not dominated by the [[control flow]] alteration itself - but by the surrounding [[calling convention]] costs, like [[function prologue|prologue and epilogue]] code, stack setup and [[Parameter_(computer_science)#Parameters_and_arguments|argument]] passing<ref>{{cite web|url=http://hbfs.wordpress.com/2008/12/30/the-true-cost-of-calls/ |title=The True Cost of Calls|date=2008-12-30|publisher=wordpress.com}}</ref> (see here<ref>http://en.wikibooks.org/wiki/X86_Disassembly/Functions_and_Stack_Frames</ref> for more realistic instruction path length, stack and other costs associated with calls on an [[x86]] platform). See also here<ref>{{cite web|url=http://www-cs-faculty.stanford.edu/~eroberts/books/ArtAndScienceOfJava/slides/07-ObjectsAndMemory.ppt |title=Art and Science of Java; Chapter 7: Objects and Memory|last=Roberts|first=Eric S.|publisher=Stanford University|date=2008}}</ref> for a slide presentation by [[Eric S. Roberts]] ("The Allocation of Memory to Variables", chapter 7)<ref>{{cite book|url=http://www-cs-faculty.stanford.edu/~eroberts/books/ArtAndScienceOfJava/slides/07-ObjectsAndMemory.ppt |title=Art and Science of Java|last=Roberts|first=Eric S.|publisher=Addison-Wesley|date=2008|ISBN=978-0321486127}}</ref> - illustrating the use of stack and heap memory usage when summing three [[rational number]]s in the [[Java (programming language)|Java]] object oriented language.
{| class="wikitable" border="1" cellpadding="1" 
!Imperative
!Procedural
!Object-oriented
|- 
| 
<code>
  load r;                      1
  r2 = r * r;                  2
  '''result = r2 * "3.142";'''       3
 . 
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .
 .... storage .............
 result variable
 constant "3.142"
</code>
|
<code>
 area proc(r2,res):
    push stack                                 5
    load r2;                                   6
    r3 = r2 * r2;                              7
    '''res = r3 * "3.142";'''                        8
    pop stack                                  9
    return;                                   10
 ...............................................
 main proc:
    load r;                                    1
    call area(r,result);
     +load p = address of parameter list;      2
     +load v = address of subroutine 'area';   3
     +goto v with return;                      4
 .
 .
 .
 .
 .... storage .............
 result variable
 constant "3.142" 
 parameter list variable
 function pointer (==>area)
 stack storage
</code>
|
<code>
 circle.area method(r2):
    push stack                                 7
    load r2;                                   8
    r3 = r2 * r2;                              9
    res = r3 * "3.142";                       10
    pop stack                                 11
    '''return(res);'''                           12,13
 ...............................................
 main proc:
    load r;                                    1
    result = circle.area(r); 
       +allocate heap storage;                 2<ref group="See">See section-"[[Comparison of programming paradigms#Allocation of dynamic memory for Message storage and object storage|Allocation of dynamic memory for Message storage and object storage]]"</ref>                  
       +copy r to message;                     3
       +load p = address of message;           4
       +load v = addr. of method 'circle.area' 5
       +goto v with return;                    6
 .
 .
 .... storage .............
 result variable (assumed pre-allocated)
 immutable variable "3.142" (final)
 (heap) message variable for circle method call
 vtable(==>area)
 stack storage
</code>
|}
<references group="See" />
The advantages of procedural abstraction and object-oriented-style polymorphism are not well illustrated by a small example like the one above. This example is designed principally to illustrate some intrinsic performance differences, not abstraction or code re-use. 

==== Subroutine/Method call overhead ====
The presence of a (called) subroutine in a program contributes nothing extra to the functionality of the program regardless of paradigm, but may contribute greatly to the structuring and generality of the program, making it much easier to write, modify, and extend<ref name="steele1997">Guy Lewis Steele, Jr. "Debunking the 'Expensive Procedure Call' Myth, or, Procedure Call Implementations Considered Harmful, or, Lambda: The Ultimate GOTO". MIT AI Lab. AI Lab Memo AIM-443. October 1977. [http://repository.readscheme.org/ftp/papers/ai-lab-pubs/AIM-443.pdf][http://dspace.mit.edu/handle/1721.1/5753][http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.4404&rep=rep1&type=pdf]</ref>. The extent to which different paradigms utilize subroutines (and their consequent memory requirements) influences the overall performance of the complete algorithm, although as [[Guy Steele]] pointed out in a 1977 paper, a well-designed programming language implementation ''can'' have very low overheads for procedural abstraction (but laments, in most implementations, that they seldom achieve this in practice - being "rather thoughtless or careless in this regard"). In the same paper, Steele also makes a considered case for [[automata-based programming]] (utilizing procedure calls with [[tail recursion]]) and concludes that "we should have a healthy respect for procedure calls" (because they are powerful) but suggested "use them sparingly"<ref name="steele1997"/> 
 
In terms of the frequency of subroutine calls:- 
* for procedural programming, the [[Granularity#Data_granularity|granularity]] of the code is largely determined by the number of discrete procedures or [[modular programming|module]]s.
* for functional programming, frequent calls to [[library (computing)|library]] subroutines are commonplace{{Citation needed|date=March 2010}} (but may be frequently inlined by the optimizing compiler)
* for object oriented programming, the number of method calls invoked is also partly determined by the granularity of the data structures and may therefore include many "read-only" accesses to low level objects that are encapsulated (and therefore accessible in no other, more direct, way). Since increased granularity is a prerequisite for greater [[code reuse]], the tendency is towards fine-grained data structures, and a corresponding increase in the number of discrete objects (and their methods) and, consequently, subroutine calls. The creation of "[[god object]]s" is actively discouraged. [[Constructor (object-oriented programming)|Constructor]]s also add to the count as they are also subroutine calls (unless they are inlined). Performance problems caused by excessive granularity may not become apparent until [[scalability]] becomes an issue.
* for other paradigms, where a mixture of the above paradigms may be employed, subroutine usage is less predictable.

====Allocation of dynamic memory for Message storage and object storage====
Uniquely, the object oriented paradigm involves dynamic allocation of memory from [[Dynamic memory allocation|heap storage]] for both object creation and message passing. A 1994 benchmark - "Memory Allocation Costs in Large C and C++ Programs" conducted by [[Digital Equipment Corporation]] on a variety of software, using an instruction-level profiling tool, measured how many instructions were required per dynamic storage allocation. The results showed that the lowest absolute number of instructions executed averaged around 50 but others reached as high as 611<ref>{{cite journal |journal=SOFTWARE—PRACTICE AND EXPERIENCE |title=Memory Allocation Costs in Large C and
C++ Programs; Page 532 |author=David Detlefs and Al Dosser and Benjamin Zorn |volume=24|issue=6 |pages= 527–542 |date=1994-06 |format=PDF|url=http://www.cs.ubc.ca/local/reading/proceedings/spe91-95/spe/vol24/issue6/spe895.pdf }})</ref>. See also "Heap:Pleasures and pains" by Murali R. Krishnan<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/ms810466%28v=MSDN.10%29.aspx|publisher=microsoft.com|title=Heap: Pleasures and pains|last=Krishnan|first=Murali R.|date=1999-02 }}</ref> that states "Heap implementations tend to stay general for all platforms, and hence have heavy overhead". The above pseudocode example does not include a realistic estimate of this memory allocation pathlength or the memory prefix overheads involved and the subsequent associated garbage collection overheads (To gain some appreciation that heap allocation is not a "trivial" task, this <ref>http://microallocator.googlecode.com/svn/trunk/MicroAllocator.cpp</ref> is an  example of one [[open source]] microallocator by games developer, [[John W. Ratcliff]], consisting of nearly 1,000 lines of code).

==== Dynamically dispatched Message calls v. Direct procedure Call overheads ====
In their Abstract "''Optimization of Object-Oriented Programs Using Static Class Hierarchy Analysis''"<ref>{{cite paper|title=Optimization of Object-Oriented Programs Using Static Class Hierarchy Analysis|author=Jeffrey Dean, David Grove, and Craig Chambers |publisher=University of Washington |DOI=10.1.1.117.2420 |url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.2420&rep=rep1&type=pdf}}</ref>, Jeffrey Dean, David Grove, and Craig Chambers of the Department of Computer Science and Engineering, at the [[University of Washington]], claim that "Heavy use of inheritance and dynamically-bound messages is likely to make code more extensible and reusable, but it also imposes a significant performance overhead, compared to an equivalent but non-extensible program written in a non-object-oriented manner. In some domains, such as structured graphics packages, the performance cost of the extra flexibility provided by using a heavily object-oriented style is acceptable. However, in other domains, such as basic data structure libraries, numerical computing packages, rendering libraries, and trace-driven simulation frameworks, the cost of message passing can be too great, forcing the programmer to avoid object-oriented programming in the “hot spots” of their application."

=== Serialization of objects ===
{{Main|Serialization}}
[[Serialization]] imposes quite considerable overheads when passing [[Object (computer science)|object]]s from one system to another, especially when the transfer is in human-readable formats such as [[XML]] and [[JSON]]. This contrasts with compact binary formats for non object oriented data. Both encoding and decoding of the objects data value and its attributes are involved in the serialization process (that also includes awareness of complex issues such as inheritance, encapsulation and data hiding).

==See also==
* [[Comparison of programming languages]]
* [[Comparison of programming languages (basic instructions)]]
* [[Granularity#In_computing|Granularity]]
* [[Message passing]]
* [[Subroutine]]

==References==
{{Reflist|refs=<ref name="flaws">{{cite web| first = Asaf| last= Shelly |title = Flaws of Object Oriented Modeling| date=2008-08-22|accessdate=2010-07-04| publisher = Intel® Software Network| url=http://software.intel.com/en-us/blogs/2008/08/22/flaws-of-object-oriented-modeling/}}</ref><ref name="executioniKoN">{{cite web| first = Steve| last=Yegge | title = Execution in the Kingdom of Nouns| date=2006-03-30|accessdate=2010-07-03| publisher = steve-yegge.blogspot.com| url=http://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html}}</ref>}}
==Further reading==
* [http://g.oswego.edu/dl/html/malloc.html "A Memory Allocator"] by Doug Lea
* [http://www.sqa.org.uk/e-learning/LinkedDS01CD/page_01.htm "Dynamic Memory Allocation and Linked Data Structures"] by ([[Scottish Qualifications Authority]])
* [http://www.flounder.com/inside_storage_allocation.htm "Inside A Storage Allocator"] by Dr. Newcomer Ph.D

==External links==
* [http://users.ecs.soton.ac.uk/mrd/research/prog.html Comparing Programming Paradigms] by Dr Rachel Harrison and Mr Lins Samaraweera
* [http://eprints.ecs.soton.ac.uk/597/ Comparing Programming Paradigms: an Evaluation of Functional and Object-Oriented Programs] by Harrison, R., Samaraweera, L. G., Dobie, M. R. and Lewis, P. H. (1996) pp. 247–254. ISSN 0268-6961
* [http://www.info.ucl.ac.be/~pvr/paradigmsDIAGRAMeng101.pdf "The principal programming paradigms"] By Peter Van Roy
* [http://www.info.ucl.ac.be/~pvr/book.html "Concepts, Techniques, and Models of Computer Programming" ](2004) by Peter Van Roy & Seif Haridi, ISBN 0-262-22069-5
* [http://hbfs.wordpress.com/2008/12/30/the-true-cost-of-calls/ The True Cost of Calls ]- from "Harder, Better, Faster, Stronger" blog by computer scientist [http://www.stevenpigeon.org/Publications/ Steven Pigeon]

{{DEFAULTSORT:Comparison Of Programming Paradigms}}
[[Category:Programming paradigms|!]]</body> </html>