<html> <head> <title>Observed information</title></head><body>{{Mergeto|Fisher information|date=January 2010}}
In [[statistics]], the '''observed information''', or '''observed Fisher information''', is the negative of the second derivative (the [[Hessian matrix]]) of the "log-likelihood" (the logarithm of the [[likelihood function]]). It is a sample-based version of the [[Fisher information]].

==Definition==
Suppose we observe [[Random_variable|random variables]] <math>X_1,\ldots,X_n</math>, independent and identically distributed with density ''f''(''X''; θ), where θ is a (possibly unknown) vector.  Then the log-likelihood of the parameters <math>\theta</math> given the data <math>X_1,\ldots,X_n</math> is

:<math>\ell(\theta | X_1,\ldots,X_n) = \sum_{i=1}^n \log f(X_i| \theta) </math>.

We define the '''observed information matrix''' at <math>\theta^{*}</math> as

:<math>\mathcal{J}(\theta^*) 
  = - \left. 
    \nabla \nabla^{\top} 
    \ell(\theta)
  \right|_{\theta=\theta^*} 
</math>

::<math>= -
\left.
\left( \begin{array}{cccc}
  \tfrac{\partial^2}{\partial \theta_1^2}
  &  \tfrac{\partial^2}{\partial \theta_1 \partial \theta_2}
  &  \cdots
  &  \tfrac{\partial^2}{\partial \theta_1 \partial \theta_n} \\
  \tfrac{\partial^2}{\partial \theta_2 \partial \theta_1}
  &  \tfrac{\partial^2}{\partial \theta_2^2}
  &  \cdots
  &  \tfrac{\partial^2}{\partial \theta_2 \partial \theta_n} \\
  \vdots &
  \vdots &
  \ddots &
  \vdots \\
  \tfrac{\partial^2}{\partial \theta_n \partial \theta_1}
  &  \tfrac{\partial^2}{\partial \theta_n \partial \theta_2}
  &  \cdots
  &  \tfrac{\partial^2}{\partial \theta_n^2} \\
\end{array} \right) 
\ell(\theta)
\right|_{\theta = \theta^*}
</math>

In many instances, the observed information is evaluated at the [[Maximum likelihood|maximum-likelihood estimate]].<ref>Dodge, Y. (2003) ''The Oxford Dictionary of Statistical Terms'', OUP. ISBN 0-19-920613-9</ref>

==Fisher information==
The [[Fisher information]] <math>\mathcal{I}(\theta)</math> is the [[expected value]] of the observed information:

:<math>\mathcal{I}(\theta) = \mathrm{E}(\mathcal{J}(\theta))</math>.

==Applications==
In a notable article, [[Bradley Efron]] and [[David V. Hinkley]] <ref>{{cite journal
 |last1=Efron   |first1=B.   |authorlink1=Bradley Efron 
 |last2=Hinkley |first2=D.V. |authorlink2=David V. Hinkley
 |year=1978
 |title=Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher Information 
 |journal=[[Biometrika]]
 |volume=65 |issue=3 |pages=457&ndash;487
 |doi=10.1093/biomet/65.3.457 |id={{MR|0521817}}. {{JSTOR|2335893}}
}} 
</ref> argued that the observed information should be used in preference to the expected information when employing [[asymptotic normality|normal approximations]] for the distribution of [[Maximum likelihood|maximum-likelihood estimate]]s.

==References==
{{reflist}}

[[Category:Information theory]]
[[Category:Statistical terminology]]
[[Category:Estimation theory]]</body> </html>