<html> <head> <title>Feature (computer vision)</title></head><body>{{Unreferenced|date=December 2009}}
In [[computer vision]] and [[image processing]] the concept of '''feature''' is used to denote a piece of information which is relevant for solving the computational task related to a certain application.  More specifically, features can refer to

* the result of a general [[neighborhood operation]] (feature extractor or [[feature detector]]) applied to the image,
* specific structures in the image itself, ranging from simple structures such as points or edges to more complex structures such as objects.

Other examples of features are related to motion in image sequences, to shapes defined in terms of curves or boundaries between different image regions, or to properties of such a region.

The feature concept is very general and the choice of features in a particular computer vision system may be highly dependent on the specific problem at hand.

==Introduction==
When features are defined in terms of local neighborhood operations applied to an image, a procedure commonly referred to as ''feature extraction'', one can distinguish between [[feature detection]] approaches that produce local decisions whether there is a feature of a given type at a given image point or not, and those who produce non-binary data as result.  The distinction becomes relevant when the resulting detected features are relatively sparse. Although local decisions are made, the output from a feature detection step does not need to be a binary image. The result is often represented in terms sets of (connected or unconnected) coordinates of the image points where features have been detected, sometimes with subpixel accuracy.

When feature extraction is done without local decision making, the result is often referred to as a ''feature image''.  Consequently, a feature image can be seen as an image in the sense that it is a function of the same spatial (or temporal) variables as the original image, but where the pixel values hold information about image features instead of intensity or color.  This means that a feature image can be processed in a similar way as an ordinary image generated by an image sensor. Feature images are also often computed as integrated step in algorithms for [[feature detection]].

==Feature representation==
A specific image feature, defined in terms of a specific structure in the image data, can often be represented in different ways.  For example, an [[Edge (geometry)|edge]] can be represented as a [[boolean variable]] in each image point that describes whether an edge is present at that point.  Alternatively, we can instead use a representation which provides a [[certainty measure]] instead of a boolean statement of the edge's existence and combine this with information about the orientation of the edge.  Similarly, the color of a specific region can either be represented in terms of the average color (three scalars) or a [[color histogram]] (three functions).

When a computer vision system or computer vision algorithm is designed the choice of feature representation can be a critical issue.  In some cases, a higher level of detail in the description of a feature may be necessary for solving the problem, but this comes at the cost of having to deal with more data and more demanding processing.  Below, some of the factors which are relevant for choosing a suitable representation are discussed.  In this discussion, an instance of a feature representation is referred to as a (feature) descriptor.

===Certainty or confidence===
Two examples of image features are local edge orientation and local velocity in an image sequence.  In the case of orientation, the value of this feature may be more or less undefined if more than one edge are present in the corresponding neighborhood.  Local velocity is undefined if the corresponding image region does not contain any spatial variation.  As a consequence of this observation it may be relevant use a feature representation which includes a measure of certainty or confidence related to the statement about the feature value.  Otherwise, it is a typical situation that the same descriptor is used to represent feature values of low certainty and feature values close to zero, with a resulting ambiguity in the interpretation of this descriptor.  Depending on the application, such an ambiguity may or may not be acceptable.

In particular if a feature image will be used in subsequent processing, it may be a good idea to employ a feature representation which includes information about [[certainty]] or [[confidence]].  This enables a new feature descriptor to be computed from several descriptors, for example computed at the same image point but at different scales, or from different but neighboring points, in terms of a weighted average where the weights are derived from the corresponding certainties.  In the simples case, the corresponding computation can be implemented as a low-pass filtering of the feature image.  The resulting feature image will, in general, be more stable to noise.

===Averageability===
In addition to having certainty measures included in the representation, the representation of the corresponding feature values may itself be suitable for an [[averaging]] operation or not.  Most feature representations can be averaged in practice, but only in certain cases can the resulting descriptor be given a correct interpretation in terms of a feature value.  Such representations are referred to as ''averageable''.

For example, if the orientation of an edge is represented in terms of an angle, this representation must have a discontinuity where the angle wraps from its maximal value to its minimal value.  Consequently, it can happen that two similar orientations are represented by angles which have a mean that does not lie close to either of the original angles and, hence, this representation is not averageable.  There are other representations of edge orientation, such as the structure tensor, which are averageable.

Another example relates to motion, where in some cases only the normal velocity relative to some edge can be extracted.  If two such features have been extracted and they can be assumed to refer to same true velocity, this velocity is not given as the average of the normal velocity vectors.  Hence, normal velocity vectors are not averageable.  Instead, there are other representations of motions, using matrices or tensors, that give the true velocity in terms of an average operation of the normal velocity descriptors.

==Feature vectors and feature spaces==
In some applications it is not sufficient to extract only one type of feature to obtain the relevant information from the image data.  Instead two or more different features are extracted, resulting in two or more feature descriptors at each image point.  A common practice is to organize the information provided by all these descriptors as the elements of one single vector, commonly referred to as a ''feature vector''.  The set of all possible feature vectors constitutes a ''feature space''.

A common example of feature vectors appears when each image point is to be classified as belonging to a specific class.  Assuming that each image point has a corresponding feature vector based on a suitable set of features, meaning that each class is well separated in the corresponding feature space, the classification of each image point can be done using standard [[statistical classification|classification]] method.

Another, and related example, occurs when neural network based processing is applied to images.  The input data fed to the neural network is often given in terms of a feature vector from each image point, where the vector is constructed from several different feature extracted from the image data.  During a learning phase, the networks can itself find which combinations of different features that are useful for solving the problem at hand.

==Multi-level feature processing==
The extraction of feature are sometimes made over several scalings. One of these methods are [[Scale-invariant feature transform]]; in this algorithm, various scales of an image is analyzed to extract features.

==See also==
* [[Feature detection (computer vision)]]
* [[Edge detection]] 
* [[Corner detection]]
* [[Blob detection]]
* [[Ridge detection]] 
* [[Interest point detection]]
* [[Feature extraction]]
* [[Computer vision]]

{{DEFAULTSORT:Feature (Computer Vision)}}
[[Category:Computer vision]]
[[Category:Image processing]]</body> </html>