<html> <head> <title>Rprop</title></head><body>'''Rprop''', short for resilient [[backpropagation]], is a learning [[heuristics|heuristic]] for [[supervised learning]] in [[Feedforward neural network|feedforward]] [[artificial neural network]]s. This is a [[First-order approximation|first-order]] [[optimization (mathematics)|optimization]] [[algorithm]]. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992. 

Similarly to the [[Manhattan update rule]], Rprop takes into account only the [[Sign (mathematics)|sign]] of the [[partial derivative]] over all patterns (not the magnitude), and acts independently on each "weight".  For each weight, if there was a sign change of the partial derivative of the total error function compared to the last iteration, the update value for that weight is multiplied by a factor ''η''<sup>&minus;</sup>, where ''η''<sup>&minus;</sup> < 1. If the last iteration produced the same sign, the update value is multiplied by a factor of ''η''<sup>+</sup>, where ''η''<sup>+</sup> > 1. The update values are calculated for each weight in the above manner, and finally each weight is changed by its own update value, in the opposite direction of that weight's partial derivative , so as to minimise the total error function.  ''η''<sup>+</sup> is empirically set to 1.2 and ''η''<sup>&minus;</sup> to 0.5.

Next to the [[cascade correlation algorithm]] and the [[Levenberg&ndash;Marquardt algorithm]], Rprop is one of the fastest weight update mechanisms.

RPROP is a batch update algorithm.

==Variations==
Martin Riedmiller developed three algorithms, all named RPROP. Igel and Hüsken assigned a new name to them: [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.1332 Improving the Rprop Learning Algorithm].
# RPROP+ is defined at [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.1417 A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm].
# RPROP&minus; is defined at [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.7876 Advanced Supervised Learning in Multi-layer Perceptrons &ndash; From Backpropagation to Adaptive Learning Algorithms]. Backtracking is removed from RPROP+.
# iRPROP&minus; is defined at [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3428  Rprop &ndash; Description and Implementation Details]. This is reinvented by Igel and Hüsken. This is most popular, most simple, and in many cases most efficient.
# iRPROP+ is defined at [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.1332 Improving the Rprop Learning Algorithm].

== References ==
*[http://citeseer.ist.psu.edu/rd/2171473%2C711503%2C1%2C0.25%2CDownload/http://citeseer.ist.psu.edu/cache/papers/cs2/20/http:zSzzSzamy.informatik.uos.dezSzriedmillerzSzpublicationszSzrprop.details.pdf/riedmiller94rprop.pdf Rprop &ndash; Description and Implementation Details] Martin Riedmiller, 1994. Technical report.

[[Category:Neural networks]]
[[Category:Machine learning]]
[[de:Resilient Propagation]]</body> </html>