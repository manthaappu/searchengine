<html> <head> <title>Moment (mathematics)</title></head><body>{{redirect|Second moment|the technique in probability theory|Second moment method}}
{{seealso|Moment (physics)}}

[[Image:Moments.svg|right|thumb|300px|Increasing each of the first four moments in turn whilst keeping the others constant, for a [[discrete uniform distribution]] with four values.]] 

In [[mathematics]], '''moment''' is, loosely speaking, a quantitative measure of the shape of a set of points. The "second moment", for example, is widely used and measures the "width" (in a particular sense) of a set of points in one dimension or in higher dimensions measures the shape of a cloud of points as it could be fit by an [[ellipsoid]]. Other moments describe other aspects of a [[Distribution (mathematics)|distribution]] such as how the distribution is skewed from its mean, or peaked. The mathematical concept is closely related to the concept of [[moment (physics)|moment]] in [[physics]], although moment in physics is [[Moment_of_inertia#Comparison_with_covariance_matrix|often represented somewhat differently]].  Any distribution can be characterized by a number of features (such as the mean, the variance, the skewness, etc.), and the moments of a function<ref>A function such as a [[probability density function]] or [[cumulative distribution function]]; see [[Moment-generating function]].</ref> describe the nature of its distribution.

The 1st moment is denoted by ''μ<sub>1</sub>''.  The first moment of the distribution of the random variable ''X'' is the expectation operator, i.e., the [[population mean]] (if the first moment exists).

In higher orders, the central moments (moments about the mean) are more interesting than the moments about zero.  The ''k''th [[central moment]], of a real-valued random variable probability distribution ''X'', with the [[expected value]] ''μ'' is:

:<math>\mu_k=E((X-\mu)^k).\,</math>

The first central moment is thus 0.  The zero-th central moment, ''μ''<sub>0</sub> is one.  See also [[central moment]].

==Significance of the  moments==
The ''n''<sup>th</sup> moment of a real-valued continuous function ''f''(''x'') of a real variable about a value ''c'' is

:<math>\mu'_n=\int_{-\infty}^\infty (x - c)^n\,f(x)\,dx.\,\!</math>

It is possible to define moments for [[random variable]]s in a more general fashion than moments for real values—see [[#Moments in metric spaces|moments in metric spaces]]. The moment of a function, without further explanation, usually refers to the above expression with ''c'' = 0. 

Usually, except in the special context of the [[Moment problem|problem of moments]], the function ''f''(''x'') will be  a [[probability density function]]. The ''n''<sup>th</sup> moment about zero of a probability density function ''f''(''x'') is the [[expected value]] of ''X''<sup>''n''</sup> and is called a ''raw moment'' or ''crude moment''<ref>http://mathworld.wolfram.com/RawMoment.html Raw Moments at Math-world</ref>.  The moments about its mean μ are called [[central moment|''central'' moments]]; these describe the shape of the function, independently of [[translation (geometry)|translation]]. 

If ''f'' is a [[probability density function]], then the value of the integral above is called the ''n''th moment of the [[probability distribution]].  More generally, if ''F'' is a [[cumulative distribution function|cumulative probability distribution function]] of any probability distribution, which may not have a density function, then the ''n''th moment of the probability distribution is given by the [[Riemann–Stieltjes integral]]

:<math>\mu'_n = \operatorname{E}(X^n)=\int_{-\infty}^\infty x^n\,dF(x)\,</math>

where ''X'' is a [[random variable]] that has this distribution and '''E''' the [[expectation operator]] or mean.

When

:<math>\operatorname{E}(|X^n|) = \int_{-\infty}^\infty |x^n|\,dF(x) = \infty,\,</math>

then the moment is said not to exist.  If the ''n''th moment about any point exists, so does (''n'' &minus; 1)th moment, and all lower-order moments, about every point.

===Variance===
The second [[central moment]] about the mean is the [[variance]], the positive square root of which is the [[standard deviation]] ''σ''.

==== Normalized moments ====
The ''normalized'' ''n''th central moment or [[standardized moment]] is the  ''n''th central moment divided by ''σ''<sup>''n''</sup>; the normalized ''n''th central moment of ''x'' = E((''x'' &minus; μ)<sup>''n''</sup>)/σ<sup>''n''</sup>. These normalized central moments are [[dimensionless number|dimensionless quantities]], which represent the distribution independently of any linear change of scale.

===Skewness===
The third central moment is a measure of the lopsidedness of the distribution; any symmetric distribution will have a third central moment, if defined, of zero. The normalized third central moment is called the [[skewness]], often γ.  A distribution that is skewed to the left (the tail of the distribution is heavier on the left) will have a negative skewness.  A distribution that is skewed to the right (the tail of the distribution is heavier on the right), will have a positive skewness.

For distributions that are not too different from the [[normal distribution]], the [[median]] will be somewhere near μ &minus; γσ/6; the [[Mode (statistics)|mode]] about μ &minus; γσ/2.

===Kurtosis===
The fourth central moment is a measure of whether the distribution is tall and skinny or short and squat, compared to the normal distribution of the same variance.  Since it is the expectation of a fourth power, the fourth central moment, where defined, is always non-negative; and except for a [[degenerate probability distribution|point distribution]], it is always strictly positive. The fourth central moment of a normal distribution is 3σ<sup>4</sup>.

The [[kurtosis]] κ is defined to be the normalized fourth central moment minus 3.  (Equivalently, as in the next section, it is the fourth [[cumulant]] divided by the square of the variance.)  Some authorities<ref name="CasellaBerger">{{cite book
  | last1 = Casella
  | first1 = George
  | last2 = Berger
  | first2 = Roger L.
  | authorlink1 = George Casella
  | authorlink2 = Roger L. Berger
  | title = Statistical Inference
  | publisher = [[Duxbury]]
  | location = Pacific Grove
  | year = 2002
  | edition = 2
  | isbn = 0534243126 }}</ref><ref name="BalandaMacGillivray88">{{cite journal
  | last1 = Ballanda
  | first1 = Kevin P.
  | last2 = MacGillivray
  | first2 = H. L.
  | title = Kurtosis: A Critical Review
  | journal = The American Statistician
  | volume = 42
  | issue = 2
  | pages = 111–119
  | year = 1988
  | doi = 10.2307/2684482
  | url = http://jstor.org/stable/2684482
  | publisher = American Statistical Association}}</ref> do not subtract three, but it is usually more convenient to have the normal distribution at the origin of coordinates. If a distribution has a peak at the mean and long tails, the fourth moment will be high and the kurtosis positive (leptokurtic); and conversely; thus, bounded distributions tend to have low kurtosis (platykurtic).

The kurtosis can be positive without limit, but κ must be greater than or equal to γ<sup>2</sup> &minus; 2; equality only holds for [[Bernoulli distribution|binary distributions]]. For unbounded skew distributions not too far from normal, κ tends to be somewhere in the area of γ<sup>2</sup> and 2γ<sup>2</sup>.

The inequality can be proven by considering 

:<math>\operatorname{E} ((T^2 -  aT)^2)\,</math>

where ''T'' = (''X'' &minus; μ)/σ. This is the expectation of a square, so it is non-negative whatever ''a'' is; on the other hand, it's also a [[quadratic equation]] in ''a''. Its [[discriminant]] must be non-positive, which gives the required relationship.

=== Mixed moments ===

'''Mixed moments''' are moments involving multiple variables.

Some examples are [[covariance]], co-skewness and co-kurtosis.  While there is a unique covariance, there are multiple co-skewnesses and co-kurtoses.

=== Higher moments ===

'''High-order moments''' are moments beyond 4th-order moments. The higher the moment, the harder it is to estimate, in the sense that larger samples are required in order to obtain estimates of similar quality.{{Citation needed|date=September 2010}}

==Cumulants==
:{{main|cumulant}}

The first moment and the second and third ''unnormalized central'' moments are additive in the sense that if ''X'' and ''Y'' are [[statistical independence|independent]] random variables then

:<math>\mu_1(X+Y)=\mu_1(X)+\mu_1(Y)\,</math>

and

:<math>\operatorname{Var}(X+Y)=\operatorname{Var}(X) + \operatorname{Var}(Y)</math>

and

:<math>\mu_3(X+Y)=\mu_3(X)+\mu_3(Y).\,</math>

(These can also hold for variables that satisfy weaker conditions than independence.  The first always holds; if the second holds, the variables are called [[correlation|uncorrelated]]).

In fact, these are the first three cumulants and all cumulants share this additivity property.

== Sample moments ==
The moments of a population can be estimated using the sample ''k''-th moment

:<math>\frac{1}{n}\sum_{i = 1}^{n} X^k_i\,\!</math>

applied to a sample ''X''<sub>1</sub>,''X''<sub>2</sub>,..., ''X''<sub>''n''</sub> drawn from the population.

It can be shown that the expected value of the sample moment is equal to the ''k''-th moment of the population, if that moment exists, for any sample size ''n''.  It is thus an unbiased estimator.

==Problem of moments==
The '''[[problem of moments]]''' seeks characterizations of sequences { ''μ''&prime;<sub>''n''</sub> : ''n'' = 1, 2, 3, ... } that are sequences of moments of some function ''f''.

==Partial moments==
Partial moments are sometimes referred to as "one-sided moments." The ''n''th order lower and upper partial moments with respect to a reference point ''r'' may be expressed as

:<math>\mu_n^-(r)=\int_{-\infty}^r (r - x)^n\,f(x)\,dx,</math>
:<math>\mu_n^+(r)=\int_r^\infty (x - r)^n\,f(x)\,dx.</math>

Partial moments are normalized by being raised to the power 1/''n''.  The [[upside potential ratio]] may be expressed as a ratio of a first-order upper partial moment to a normalized second-order lower partial moment.

==Moments in metric spaces==

Let (''M'', ''d'') be a [[metric space]], and let B(''M'') be the [[Borel sigma algebra|Borel &sigma;-algebra]] on ''M'', the [[sigma algebra|&sigma;-algebra]] generated by the ''d''-[[open set|open subsets]] of ''M''. (For technical reasons, it is also convenient to assume that ''M'' is a [[separable space]] with respect to the [[metric (mathematics)|metric]] ''d''.) Let 1 ≤ ''p'' ≤ +∞.

The '''''p''<sup>th</sup> moment''' of a measure ''μ'' on the [[measurable space]] (''M'', B(''M'')) about a given point ''x''<sub>0</sub> in ''M'' is defined to be

:<math>\int_{M} d(x, x_{0})^{p} \, \mathrm{d} \mu (x).</math>

''μ'' is said to have '''finite ''p''<sup>th</sup> moment''' if the ''p''<sup>th</sup> moment of ''μ'' about ''x''<sub>0</sub> is finite for some ''x''<sub>0</sub> ∈ ''M''.

This terminology for measures carries over to random variables in the usual way: if (Ω, Σ, '''P''') is a [[probability space]] and ''X'' : Ω → ''M'' is a random variable, then the '''''p''<sup>th</sup> moment''' of ''X'' about ''x''<sub>0</sub> ∈ ''M'' is defined to be

:<math>\int_{M} d (x, x_{0})^{p} \, \mathrm{d} \left( X_{*} (\mathbf{P}) \right) (x) \equiv \int_{\Omega} d (X(\omega), x_{0})^{p} \, \mathrm{d} \mathbf{P} (\omega),</math>

and ''X'' has '''finite ''p''<sup>th</sup> moment''' if the ''p''<sup>th</sup> moment of ''X'' about ''x''<sub>0</sub> is finite for some ''x''<sub>0</sub> ∈ ''M''.

==See also==

* [[Hamburger moment problem]]
* [[Hausdorff moment problem]]
* [[Image moments]]
* [[L-moment]]
* [[Method of moments]]
* [[Second moment method]]
* [[Standardized moment]]
* [[Stieltjes moment problem]]
* [[Taylor expansions for the moments of functions of random variables]]

==External links==
<references/>
*[http://mathworld.wolfram.com/topics/Moments.html Moments at Mathworld]
*[http://www.geo.upm.es/postgrado/CarlosLopez/geost_03/node37.html Higher Moments]

{{Theory of probability distributions}}
{{Statistics|descriptive}}

[[Category:Probability theory]]
[[Category:Mathematical analysis]]
[[Category:Theory of probability distributions]]
[[ar:عزم (رياضيات)]]
[[de:Moment (Stochastik)]]
[[fr:Moment (mathématiques)]]
[[it:Momento (statistica)]]
[[hu:Momentum]]
[[ja:モーメント (数学)]]
[[pl:Moment (matematyka)]]
[[pt:Momento não-centrado]]
[[ru:Моменты случайной величины]]
[[sl:Moment (matematika)]]
[[sv:Moment (matematik)]]
[[tr:Momentler]]
[[uk:Моменти випадкової величини]]
[[zh:矩 (數學)]]</body> </html>