<html> <head> <title>Character encoding</title></head><body>{{Selfref|"Special characters" redirects here. For the Wikipedia editor's handbook page, see [[Help:Special characters]].}}
A '''character encoding''' system consists of a [[code]] that pairs each [[character (computing)|character]] from a given repertoire with something else, such as a sequence of natural [[number]]s, [[octet (computing)|octets]] or electrical pulses, in order to facilitate the transmission of data (generally numbers and/or text) through telecommunication networks or [[Computer data storage|storage]] of [[Character (computing)|text]] in [[computer]]s.

Other terms like '''character encoding''', '''character set''' ''(charset)'', and sometimes '''character map''' or '''code page''' are used almost interchangeably, but these terms now have related but distinct meanings. See [[#General terminology|terminology section]].

==History==

Common examples of character encoding systems include [[Morse code]], the [[Baudot code]], the [[American Standard Code for Information Interchange]] ([[ASCII]]) and [[Unicode]].

[[Morse code]] was introduced in the 1840s and is used to encode each letter of the [[Latin alphabet]] and each [[Hindu-Arabic numeral system|Hindu-Arabic numeral]] as a series of long and short presses of a [[telegraph key]]. Representations of characters encoded using Morse code varied in length.

The [[Baudot code]] was created by [[Émile Baudot]] in 1870, patented in 1874, modified by Donald Murray in 1901, and standardized by CCITT as International Telegraph Alphabet No. 2 (ITA2) in 1930.

[[ASCII]] was introduced in 1963 and is a 7-bit encoding scheme used to encode letters, numerals, symbols, and device [[control code]]s as fixed-length codes using [[integer]]s. 

[[IBM]]'s [[Extended Binary Coded Decimal Interchange Code]] (usually abbreviated [[EBCDIC]]) is an 8-bit encoding scheme developed in 1963.

The limitations of such sets soon became apparent, and a number of [[ad hoc|ad-hoc]] methods were developed to extend them. The need to support more [[writing system]]s for different languages, including the [[CJK characters|CJK]] family of East Asian scripts, required support for a far larger number of characters and demanded a systematic approach to character encoding rather than the previous [[ad hoc]] approaches.

Early binary repertoires include:
* [[Braille]] 
* [[International maritime signal flags]]
* [[Chinese telegraph code]] ([[Hans Schjellerup]], 1869, modified 1872 and following)
*:Encoding of Chinese characters as 4-digit decimals.

==Common character encodings==
* [[ISO/IEC 646|ISO 646]]
** [[ASCII]]
* [[Extended Binary Coded Decimal Interchange Code|EBCDIC]]
** [[EBCDIC 37|CP37]] 
** [[EBCDIC 930|CP930]] 
** [[EBCDIC 1047|CP1047]] 
* [[ISO/IEC 8859|ISO 8859]]:
** [[ISO/IEC 8859-1|ISO 8859-1]] Western Europe
** [[ISO/IEC 8859-2|ISO 8859-2]] Western and Central Europe
** [[ISO/IEC 8859-3|ISO 8859-3]] Western Europe and South European (Turkish, Maltese plus Esperanto)
** [[ISO/IEC 8859-4|ISO 8859-4]] Western Europe and Baltic countries (Lithuania, Estonia and Lapp)
** [[ISO/IEC 8859-5|ISO 8859-5]] Cyrillic alphabet
** [[ISO/IEC 8859-6|ISO 8859-6]] Arabic
** [[ISO/IEC 8859-7|ISO 8859-7]] Greek
** [[ISO/IEC 8859-8|ISO 8859-8]] Hebrew
** [[ISO/IEC 8859-9|ISO 8859-9]] Western Europe with amended Turkish character set
** [[ISO/IEC 8859-10|ISO 8859-10]] Western Europe with rationalised character set for Nordic languages, including complete Icelandic set
** [[ISO/IEC 8859-11|ISO 8859-11]] Thai
** [[ISO/IEC 8859-13|ISO 8859-13]] Baltic languages plus Polish
** [[ISO/IEC 8859-14|ISO 8859-14]] Celtic languages (Irish Gaelic, Scottish, Welsh)
** [[ISO/IEC 8859-15|ISO 8859-15]] Added the Euro sign and other rationalisations to ISO 8859-1
** [[ISO/IEC 8859-16|ISO 8859-16]] Central, Eastern and Southern European languages (Polish, Czech, Slovak, Serbian, Croatian, Slovene, Hungarian, Albanian, Romanian, German, Italian)
* [[Code page 437|CP437]], [[Code page 737|CP737]], [[Code page 850|CP850]], [[Code page 852|CP852]], [[Code page 855|CP855]], [[Code page 857|CP857]], [[Code page 858|CP858]], [[Code page 860|CP860]], [[Code page 861|CP861]], [[Code page 863|CP863]], [[Code page 865|CP865]], [[Code page 866|CP866]], [[Code page 869|CP869]]
*[[Code page#Windows .28ANSI.29 code pages|MS-Windows character sets]]:
** [[Windows-1250]] for Central European languages that use Latin script, (Polish, Czech, Slovak, Hungarian, Slovene, Serbian, Croatian, Romanian and Albanian)
** [[Windows-1251]] for Cyrillic alphabets
** [[Windows-1252]] for Western languages
** [[Windows-1253]] for Greek
** [[Windows-1254]] for Turkish
** [[Windows-1255]] for Hebrew
** [[Windows-1256]] for Arabic
** [[Windows-1257]] for Baltic languages
** [[Windows-1258]] for Vietnamese
* [[Mac OS Roman]]
* [[KOI8-R]], [[KOI8-U]], [[KOI7]]
* [[MIK Code page|MIK]]
* [[Indian Script Code for Information Interchange|ISCII]]
* [[Tamil Script Code for Information Interchange|TSCII]]
* [[Vietnamese Standard Code for Information Interchange|VISCII]]
* [[JIS X 0208]] is a widely deployed standard for Japanese character encoding that has several encoding forms.
** [[Shift JIS]] (Microsoft [[Code page 932]] is a dialect of Shift_JIS)
** [[Extended Unix Code|EUC-JP]]
** [[ISO/IEC 2022|ISO-2022-JP]]
* [[JIS X 0213]] is an extended version of JIS X 0208.
** [[Shift JIS|Shift_JIS-2004]]
** [[Extended Unix Code|EUC-JIS-2004]]
** [[ISO/IEC 2022|ISO-2022-JP-2004]]
* Chinese [[List of GB standards|Guobiao]]
** [[GB 2312]]
** [[GBK]] (Microsoft Code page 936)
** [[GB 18030]]
* Taiwan [[Big5]] (a more famous variant is Microsoft [[Code page 950]])
* Hong Kong [[HKSCS]]
* Korean
** [[KS X 1001]] is a Korean double-byte character encoding standard
** [[Extended Unix Code#EUC-KR|EUC-KR]]
** [[ISO/IEC 2022|ISO-2022-KR]]
* [[Unicode]] (and subsets thereof, such as the 16-bit 'Basic Multilingual Plane'). See [[UTF-8]]
* [[ANSEL]] or [[ISO/IEC 6937]]

== Character encoding translation ==

As a result of having many character encoding methods in use (and the need for backward compatibility with archived data), many computer programs have been developed to translate data between encoding schemes. Some of these are cited below.

[[Cross-platform]]:
* [[Web browser]]s - most modern web browsers feature automatic character encoding detection. On Firefox 3, for example, see the View/Character Encoding submenu. 
* [[iconv]] – program and standardized API to convert encodings
* convert_encoding.py – Python based utility to convert text files between arbitrary encodings and line endings.<ref>[http://users.physik.fu-berlin.de/~mgoerz/blog/programs/convert_encoding/ Homepage of Michael Goerz - convert_encoding.py<!-- Bot generated title -->]</ref>
* decodeh.py - algorithm and module to heuristically guess the encoding of a string.<ref>[http://gizmojo.org/code/decodeh/ Decodeh - heuristically decode a string or text file<!-- Bot generated title -->]</ref> 
* [[International Components for Unicode]] - A set of C and Java libraries to perform charset conversion. uconv can be used from ICU4C.
* [http://chardet.feedparser.org/ chardet] - This is a translation of the [[Mozilla]] automatic-encoding-detection code into the Python computer language. 
* The newer versions of the unix [[File (command)|File]] command attempt to do a basic detection of character encoding. (also available on cygwin and mac)
[[Linux]]: 
* recode – convert file contents from one encoding to another<ref>[http://www.gnu.org/software/recode/recode.html Recode - GNU Project - Free Software Foundation (FSF)<!-- Bot generated title -->]</ref>
* utrac – convert file contents from one encoding to another.<ref>[http://utrac.sourceforge.net/ Utrac Homepage<!-- Bot generated title -->]</ref>
* cstocs – convert file contents from one encoding to another 
* convmv – convert a filename from one encoding to another.<ref>[http://www.j3e.de/linux/convmv/man/ Convmv - converts filenames from one encoding to another<!-- Bot generated title -->]</ref>
* enca – analyzes encodings for given text files.<ref>[http://directory.fsf.org/project/enca/ Extremely Naive Charset Analyser <!-- Bot generated title -->]</ref>

[[Microsoft Windows|Windows]]:
* Encoding.Convert - .NET API<ref>[http://msdn.microsoft.com/en-us/library/system.text.encoding.convert(VS.71).aspx Microsoft .NET Framework Class Library - Encoding.Convert Method]</ref>
* MultiByteToWideChar/WideCharToMultiByte - Convert from ANSI to Unicode & Unicode to ANSI<ref>[http://support.microsoft.com/kb/138813 MultiByteToWideChar/WideCharToMultiByte - Convert from ANSI to Unicode & Unicode to ANSI]</ref>
* cscvt – character set conversion tool<ref>[http://www.kalytta.com/tools.php Character Set Converter]</ref>

== Terminology ==
===Unicode encoding model===

[[Unicode]] and its parallel standard, the ISO/IEC 10646 [[Universal Character Set]], together constitute a modern, unified character encoding. Rather than mapping characters directly to octets ([[Byte|bytes]]), they separately define what characters are available, their numbering, how those numbers are encoded as a series of "code units" (limited-size numbers), and finally how those units are encoded as a stream of octets. The idea behind this decomposition is to establish a universal set of characters that can be encoded in a variety of ways.<ref name=utr17>{{cite web |url=http://www.unicode.org/reports/tr17/ |title=Unicode Technical Report #17: Unicode Character Encoding Model |date=2008-11-11 |accessdate=2009-08-08}}</ref> To correctly describe this model one needs more precise terms than "character set" and "character encoding". The terms used in the modern model follow:<ref name=utr17/>

A '''character repertoire''' is the full set of abstract characters that a system supports. The repertoire may be closed, i.e. no additions are allowed without creating a new standard (as is the case with ASCII and most of the ISO-8859 series), or it may be open, allowing additions (as is the case with Unicode and to a limited extent the [[Windows code page]]s). The characters in a given repertoire reflect decisions that have been made about how to divide writing systems into linear information units. The basic variants of the [[Latin alphabet|Latin]], [[Greek alphabet|Greek]], and [[Cyrillic alphabet]]s, can be broken down into letters, digits, punctuation, and a few '''special characters''' like the space,{{citation needed|date=March 2010}} which can all be arranged in simple linear sequences that are displayed in the same order they are read. Even with these alphabets however [[diacritic]]s pose a complication: they can be regarded either as part of a single character containing a letter and diacritic (known in modern terminology as a precomposed character), or as separate characters. The former allows a far simpler text handling system but the latter allows any letter/diacritic combination to be used in text. Other writing systems, such as Arabic and Hebrew, are represented with more complex character repertoires due to the need to accommodate things like bidirectional text and [[glyph]]s that are joined together in different ways for different situations.

{{anchor|CCS}}
A '''coded character set''' (CCS) specifies how to represent a repertoire of characters using a number of non-negative integer codes called ''[[code point]]s''. For example, in a given repertoire, a character representing the capital letter "A" in the Latin alphabet might be assigned to the integer 65, the character for "B" to 66, and so on. A complete set of characters and corresponding integers is a coded character set. Multiple coded character sets may share the same repertoire; for example [[ISO/IEC 8859-1]] and IBM code pages 037 and 500 all cover the same repertoire but map them to different codes. In a coded character set, each code point only represents one character, i.e., a coded character set is a [[function (mathematics)|function]].

A '''character encoding form''' (CEF) specifies the conversion of a coded character set's integer codes into a set of limited-size integer ''code values'' that facilitate storage in a system that represents numbers in binary form using a fixed number of bits (i.e. practically any computer system). For example, a system that stores numeric information in 16-bit units would only be able to directly represent integers from 0 to 65,535 in each unit, but larger integers could be represented if more than one 16-bit unit could be used. This is what a CEF accommodates: it defines a way of mapping a ''single'' code ''point'' from a range of, say, 0 to 1.4 million, to a series of ''one or more'' code ''values'' from a range of, say, 0 to 65,535.

The simplest CEF system is simply to choose large enough units that the values from the coded character set can be encoded directly (one code point to one code value). This works well for coded character sets that fit in 8 bits (as most legacy non-CJK encodings do) and reasonably well for coded character sets that fit in 16 bits (such as early versions of Unicode). However, as the size of the coded character set increases (e.g. modern Unicode requires at least 21 bits/character), this becomes less and less efficient, and it is difficult to adapt existing systems to use larger code values. Therefore, most systems working with later versions of Unicode use either [[UTF-8]], which maps Unicode code points to variable-length sequences of octets, or [[UTF-16/UCS-2]], which maps Unicode code points to variable-length sequences of 16-bit words.

Next, a '''character encoding scheme''' (CES) specifies how the fixed-size integer code values should be mapped into an octet sequence suitable for saving on an octet-based file system or transmitting over an octet-based network. With Unicode, a simple character encoding scheme is used in most cases, simply specifying whether the bytes for each integer should be in big-[[endianness|endian]] or little-endian order (even this isn't needed with UTF-8). However, there are also compound character encoding schemes, which use escape sequences to switch between several simple schemes (such as [[ISO/IEC 2022]]), and compressing schemes, which try to minimise the number of bytes used per code unit (such as [[Standard Compression Scheme for Unicode|SCSU]], [[Binary Ordered Compression for Unicode|BOCU]], and [[Punycode]]). See [[comparison of Unicode encodings]] for a detailed discussion.

Finally, there may be a '''higher level protocol''' which supplies additional information that can be used to select the particular variant of a [[Unicode]] character, particularly where there are regional variants that have been 'unified' in Unicode as the same character. An example is the [[XML]] attribute xml:lang.

The Unicode model reserves the term '''character map''' for historical systems which directly assign a sequence of characters to a sequence of bytes, covering all of CCS, CEF and CES layers.<ref name=utr17/>

=== General terminology ===
In [[computer science]], the terms '''character encoding''', '''character map''', '''character set''' or '''code page''' were historically synonymous{{citation needed|date=March 2010}}, as the same standard would specify a repertoire of characters and how they were to be encoded into a stream of code units — usually with a single character per code unit. The terms now have related but distinct meanings, reflecting the efforts of standards bodies to use precise terminology when writing about and unifying many different encoding systems.<ref name=utr17/> Regardless, the terms are still used interchangeably, with ''character set'' being nearly ubiquitous.

A '''[[code page]]''' usually means a [[byte oriented]] encoding, but with emphasis to some suite of encodings (covering different scripts), where many characters share same [[code point|codes]] in all these code pages (or most). Well known code page suites are "Windows" (based on Windows-1252) and "IBM"/"DOS" (based on code page 437), see [[Windows code page]] for details. Most encodings referred to as code pages, but not all of them, are single byte encodings.

IBM's Character Data Representation Architecture (CDRA) designates with coded character set identifiers ([[CCSID]]s) and each of which is variously called a '''charset''', '''character set''', '''code page''', or '''CHARMAP'''.<ref name=utr17/>

Contrasted to [[#CCS|CCS above]], a '''character encoding''' is a map from abstract characters to [[code word]]s. A '''character set''' in [[HTTP]] (and [[MIME]]) parlance is the same as a character encoding (but not the same as CCS).

'''[[legacy system|Legacy]] encoding''' is a cliché used sometimes to characterize old character encoding, but with an ambiguity of sense. Most of its use is in the context of [[Unicode|Unicodification]], where it refers to encodings that do not cover all Unicode code points, or, more generally, using somewhat different character repertoire: several code points representing one Unicode character<ref>[http://www.basistech.co.jp/knowledge-center/database/uc-trillium-2.ppt "Processing database information using Unicode, a case study"]</ref>, or versa (see e.g. [[code page 437]]). Some sources declare an encoding to be ''legacy'' only because it preceded Unicode.<ref>{{cite web|url=http://scripts.sil.org/cms/scripts/page.php?site_id=nrsi&item_id=IWS-Chapter03#79e846db|publisher=[[SIL International]]|title=Character set encoding basics|work=Implementing Writing Systems: An introduction|first=Peter|last=Constable|date=2001-06-13|accessdate=2010-03-19}}</ref> All Windows code pages are usually referred to as legacy.

==See also==
* [[Alt code]]
* [[:Category:Character encoding|Character encoding]] — articles related to character encoding in general
* [[:Category:Character sets|Character sets]] — articles detailing specific character encodings
* [[Code page]] — various character set encodings used by IBM, Microsoft, SAP…
* ''[[Mojibake]]'' — character set mismap.
* [[Mojikyo]] - a system ('glyph set') that includes over 100,000 Chinese character drawings, modern and ancient, popular and obscure.<ref name="mojikyoorg">
{{ cite web | url = http://www.mojikyo.org/html/abroad/abroad_top.html
 | title = Mojikyo English page
 | accessdate = 2009 11 07
 | publisher = mojikyo.org }} {{Dead link|date=September 2010|bot=H3llBot}}
</ref><ref name="jbrowsecom">
{{ cite web | url = http://www.jbrowse.com/text/unij.html
 | title = Character Set List
 | accessdate = 2009 11 07
 | publisher = jbrowse.com }}
</ref>
* [[TRON (encoding)|TRON]], part of the [[TRON Project]], is an encoding system that does not use [[Han Unification]], instead it uses 'control codes' to switch between 16bit 'planes' of characters.<ref name="jbrowsecom"/><ref name="tronorg">
{{ cite web | url = http://www2.tron.org/troncode.html
 | title = TRON code website
 | accessdate = 2009 11 07
 | publisher = tron.org }}

</ref>
* [[Universal Character Set characters]]
* [[Windows code page]] — various character set encodings used by Microsoft Windows

== References ==
{{Refbegin}}
*{{cite book |last=Mackenzie |first=Charles E. |title=Coded Character Sets, History and Development |publisher=Addison-Wesley |year =1980 |isbn =0-201-14460-3 }}
{{Refend}}
{{Reflist|2}}

==External links==
* [http://www.iana.org/assignments/character-sets Character sets registered by Internet Assigned Numbers Authority]
* [http://www.unicode.org/unicode/reports/tr17/ Unicode Technical Report #17: Character Encoding Model]

{{character encoding}}

[[Category:Character encoding| ]]
[[Category:Natural language and computing|Encoding]]

[[ast:Códigu de caracteres]]
[[bn:ক্যারেক্টার এনকোডিং]]
[[zh-min-nan:Pian-bé]]
[[ca:Codificació de caràcters]]
[[cs:Znaková sada]]
[[da:Tegnsæt]]
[[de:Zeichenkodierung]]
[[el:Κωδικοποίηση χαρακτήρων]]
[[es:Codificación de caracteres]]
[[fr:Codage des caractères]]
[[gl:Codificación de caracteres]]
[[ko:문자 인코딩]]
[[id:Pengkodean karakter]]
[[ia:Codification de characteres]]
[[it:Codifica di caratteri]]
[[he:קידוד תווים]]
[[lv:Rakstzīmju kodējums]]
[[hu:Karakterkódolás]]
[[mr:कॅरॅक्टर एनकोडिंग]]
[[mn:Тэмдэгтийн кодлолын схем]]
[[nl:Tekencodering]]
[[ja:文字コード]]
[[no:Tegnsett]]
[[nn:Teiknsett]]
[[pl:Kod znaku]]
[[pt:Codificação de caracteres]]
[[ru:Набор символов]]
[[sv:Teckenkodning]]
[[th:รหัสอักขระ]]
[[uk:Кодування символів]]
[[zh:字符编码]]</body> </html>