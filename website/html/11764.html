<html> <head> <title>Egomotion</title></head><body>{{merge|Visual odometry|discuss=Talk:Visual odometry#Merger proposal|date=June 2010}}

'''Egomotion''' is defined as the 3D motion of a camera within an environment<ref name="irani">{{cite journal
| author = Irani, M.
| coauthors = Rousso, B.; Peleg S.
| title = Recovery of Ego-Motion Using Image Stabilization
| url = http://www.vision.huji.ac.il/papers/ego-mtn-cvpr94.pdf
| journal = IEEE Computer Society Conference on Computer Vision and Pattern Recognition
| pages = 21–23
| year = 1994
| month = June
| accessdate = 7 June 2010
}}</ref>. In the field of [[computer vision]], egomotion refers to estimating a camera's motion relative to a rigid scene<ref>{{cite journal
| author = Burger, W.
| coauthors = Bhanu, B.
| title = Estimating 3D egomotion from perspective image sequence
| url = http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=61704
| journal = IEEE Transactions on Pattern Analysis and Machine Intelligence
| volume = 12
| issue = 11
| pages = 1040–1058
| year = 1990
| month = Nov
| accessdate = 7 June 2010
}}</ref>. An example of egomotion estimation would be estimating a car's moving position relative to lines on the road or street signs as observed from the car itself. The estimation of egomotion is important in autonomous robot navigation applications<ref>{{cite journal
| author = Shakernia, O.
| coauthors = Vidal, R.; Shankar, S.
| title = Omnidirectional Egomotion Estimation From Back-projection Flow
| url = http://cis.jhu.edu/~rvidal/publications/OMNIVIS03-backflow.pdf
| journal = Conference on Computer Vision and Pattern Recognition Workshop
| volume = 7
| pages = 82
| year = 2003
| accessdate = 7 June 2010
}}</ref>.

==Overview==
The goal of estimating the egomotion of a camera is the determine the 3D motion of that camera within the environment using a sequence of images taken by the camera<ref>{{cite journal
| author = Tian, T.
| coauthors = Tomasi, C.; Heeger, D.
| title = Comparison of Approaches to Egomotion Computation
| url = http://www.cs.duke.edu/~tomasi/papers/tian/tianCvpr96.pdf
| journal = IEEE Computer Society Conference on Computer Vision and Pattern Recognition
| pages = 315
| year = 1996
| accessdate = 7 June 2010
}} {{Dead link|date=November 2010|bot=H3llBot}}</ref>. The process of estimating a camera's motion within an environment involves the use of [[visual odometry]] techniques on a sequence of images captured by the moving camera<ref name="milella" />. This is typically done using [[feature detection]] to construct an [[optical flow]] from two image frames in a sequence<ref name="irani" /> generated from either single cameras or stereo cameras<ref name="milella">{{cite journal
| author = Milella, A.
| coauthors = Siegwart, R.
| title = Stereo-Based Ego-Motion Estimation Using Pixel Tracking and Iterative Closest Point
| url = http://asl.epfl.ch/aslInternalWeb/ASL/publications/uploadedFiles/21_amilella_EgoMotion_rev_publication.pdf
| journal = IEEE International Conference on Computer Vision Systems
| pages = 21
| year = 2006
| month = January
| accessdate = 7 June 2010
}}</ref>. Using stereo image pairs for each frame helps reduce error and provides additional depth and scale information<ref name="olson">{{cite journal
| author = Olson, C. F.
| coauthors = Matthies, L.; Schoppers, M.; Maimoneb M. W.
| year = 2003
| month = June
| title = Rover navigation using stereo ego-motion
| journal = Robotics and Autonomous Systems
| volume = 43
| issue = 4
| pages = 215–229
| url = http://faculty.washington.edu/cfolson/papers/pdf/ras03.pdf
| accessdate = 07 June 2010
}}</ref>.

Features are detected in the first frame, and then matched in the second frame. This information is then used to make the optical flow field for the detected features in those two images. The optical flow field illustrates how features diverge from a single point, the ''focus of expansion''. The focus of expansion can be detected from the optical flow field, indicating the direction of the motion of the camera, and thus providing an estimate of the camera motion.

There are other methods of extracting egomotion information from images as well, including a method that avoids feature detection and optical flow fields and directly uses the image intensities<ref name="irani" />.

==See also==
* [[Visual odometry]]

==References==
{{Reflist}}

[[Category:Computer vision]]


{{Comp-sci-stub}}</body> </html>