<html> <head> <title>Parallel algorithm</title></head><body>{{unreferenced|date=October 2010}}
In [[computer science]], a '''parallel algorithm''' or '''concurrent algorithm''', as opposed to a traditional [[Sequential algorithm|sequential (or serial) algorithm]], is an [[algorithm]] which can be executed a piece at a time on many different processing devices, and then put back together again at the end to get the correct result.

Some algorithms are easy to divide up into pieces like this.  For example, splitting up the job of checking all of the numbers from one to a hundred thousand to see which are [[prime number|primes]] could be done by assigning a subset of the numbers to each available processor, and then putting the list of positive results back together.

Most of the available algorithms to compute [[pi]] (π), on the other hand, cannot be easily split up into parallel portions.  They require the results from a preceding step to effectively carry on with the next step. Such problems are called inherently serial problems.  Iterative [[Numerical analysis|numerical methods]], such as [[Newton's method]] or the [[three-body problem]], are also algorithms which are inherently serial. Some problems are very difficult to parallelize, although they are recursive. One such example is the [[depth-first search]] of [[Graph (data structure)|graphs]]. 

Parallel algorithms are valuable because of substantial improvements in [[multiprocessing]] systems and the rise of [[multi-core]] processors. In general, it is easier to construct a computer with a single fast processor than one with many slow processors with the same [[throughput]]. But processor speed is increased primarily by shrinking the circuitry, and modern processors are pushing physical size and heat limits. These twin barriers have flipped the equation, making multiprocessing practical even for small systems.

The cost or complexity of serial algorithms is estimated in terms of the space (memory) and time (processor cycles) that they take. Parallel algorithms need to optimize one more resource, the communication between different processors. There are two ways parallel processors communicate, shared memory or message passing.

[[Shared memory]] processing needs additional [[Lock (computer science)|locking]] for the data, imposes the overhead of additional processor and bus cycles, and also serializes some portion of the algorithm.

[[Message passing]] processing uses channels and message boxes but this communication adds transfer overhead on the bus, additional memory need for queues and message boxes and latency in the messages. Designs of parallel processors use special buses like crossbar so that the communication overhead will be small but it is the parallel algorithm that decides the volume of the traffic.

Another problem with parallel algorithms is ensuring that they are suitably [[Load balancing (computing)|load balanced]]. For example, checking all numbers from one to a hundred thousand for primality is easy to split amongst processors; however, some processors will get more work to do than the others, which will sit idle until the loaded processors complete.

A subtype of parallel algorithms, ''[[distributed algorithms]]'' are algorithms designed to work in [[cluster computing]] and [[distributed computing]] environments, where additional concerns beyond the scope of "classical" parallel algorithms need to be addressed.

==See also==
* [[Multiple-agent system]] (MAS)
* [[Neural network]]
* [[Parallel computing]]

==External links==
*[http://www-unix.mcs.anl.gov/dbpp/ Designing and Building Parallel Programs page at the US Argonne National Laboratories]

[[Category:Parallel computing]]
[[Category:Concurrent algorithms]]
[[Category:Distributed algorithms]]

[[ar:خوارزمية متوازية]]
[[de:Paralleler Algorithmus]]
[[es:Algoritmo paralelo]]
[[fa:الگوریتم موازی]]
[[hu:Párhuzamos algoritmus]]
[[ja:並列アルゴリズム]]
[[pl:Algorytm równoległy]]
[[ru:Параллельный алгоритм]]
[[uk:Паралельний алгоритм]]</body> </html>