<html> <head> <title>Harmonic pitch class profiles</title></head><body>'''Harmonic pitch class profiles (HPCP''') is a vector of features extracted from an [[audio signal]], based on the ''Pitch Class Profile'' descriptor proposed by Fujishima in the context of a chord recognition system<ref>Fujishima, T. ''Realtime chord recognition of musical sound: a system using Common Lisp Music'', ICMC, Beijing, China, 1999, pp. 464–467.</ref>. HPCP is an enhanced pitch distribution feature which are sequences of feature vectors describing [[Tonality|tonality]] measuring the intensity of each of the 12 pitch classes of the temperate scale within an analysis frame <ref>Gomez, E. Herrera, P. (2004). ''Estimating The Tonality Of Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies''. ISMIR 2004 – 5th International Conference on Music Information Retrieval.</ref>. It is also called '''Chroma'''. By doing some process on musical signals, HPCP feature can be found and be used to measure pitch similarity by computed in a frame-by-frame basis, and only uses the local maxima of the [[Spectrum|spectrum]] within a certain frequency band. The process is related to [[Time-frequency analysis|time-frequency analysis]]. In general, chroma features is robust to noise (e.g., ambient noise or percussive sounds), independent of timbre and played instruments and independent of loudness and dynamics.
HPCPs are tuning independent and consider the presence of harmonic frequencies, so that the reference frequency can be different from the standard A 440 Hz. The result of HPCP computation is a 12, 24, or 36-bin octave-independent [[Histogram|histogram]] depending on the desired resolution, representing the relative intensity of each 1, 1/2, or 1/3 of the 12 [[Semitone|semitones]] of the equal tempered scale.
<br/>
==General HPCP feature extraction procedure==
[[File:HPCP_block_diagram.jpg|thumb|Fig.1 General HPCP feature extraction block diagram]]
The block diagram of the procedure is shown in '''Fig.1'''<ref>Joan Serra, Emilia Gomez, Perfecto Herrera, and Xavier Serra ''Chroma Binary Similarity and Local Alignment Applied to Cover Song Identification'' August, 2008</ref>.
The General HPCP feature extraction procedure is summarized as follows:<ref>Gomez, E. ''Tonal description of polyphonic audio for music content processing''.  INFORMS Journal on Computing. Special Cluster on Music Computing. Chew, E., Guest Editor, 2004.</ref>
#Input musical signal.
#Do '''spectral analysis''' to know the frequency components of the music signal.
#Use '''[[Constant Q transform|constant Q transform]]''' to convert the signal into a spectrogram. (The constant-Q transform is a type of '''[[Time-frequency analysis|time-frequency analysis]]'''.)
#Do '''[[Filter (signal processing)|frequency filtering]]'''. Only a frequency band between 100 and 5000 Hz is used. 
#Do '''peak detection'''. Only the local maximum values of spectrum are considered.
#Do '''reference frequency computation''' procedure. Estimate the '''deviation''' with respect to 440Hz. 
#'''Normalize''' the feature frame by frame dividing through the maximum value in order to eliminate dependency on global loudness. And then we can get a result HPCP sequence like Fig.2.
:'''Pitch class mapping''' is a procedure for determining the pitch class value from frequency values. A weighting scheme with cosine function is used. It considers the presence of harmonic frequency, taking account a total of 8 harmonics for each frequency. In order to map the value on a one-third of a [[Semitone|semitone]], the size of the pitch class distribution vectors has to be equal to '''36'''. 
[[File:HPCP_output.jpg|thumb|Fig.2 Example of a high-resolution HPCP sequence]]
==System of measuring similarity between two songs==
[[File:Compare_songs.jpg|thumb|Fig.3 System of measuring similarity between two songs]]
After getting the '''HPCP feature''', the pitch of the signal in a time section is known. The HPCP feature has been used to compute similarity between two songs in many research. A system of measuring similarity between two songs is shown in '''Fig.3'''. First, '''time-frequency analysis''' is needed to extract the HPCP feature. And then set two songs’ HPCP feature to a global HPCP, so there is a standard of comparing. The next step is to use the two features to construct a '''binary similarity matrix'''. '''[[Smith–Waterman algorithm]]''' is used to construct a local alignment matrix H in the '''Dynamic Programming Local Alignment'''. Finally, after doing post processing, the distance between two songs can be computed.
<br/>
==See also==
*[[Time-frequency analysis]]
*[[Time-frequency analysis for music signal]]
*[[Pitch]]
*[[Musical theory]]
==References==
{{Reflist}}
{{DEFAULTSORT:Time–Frequency Analysis}}
[[Category:Time–frequency analysis| ]]</body> </html>