<html> <head> <title>Regression toward the mean</title></head><body>{{Expert-subject|statistics|date=July 2010}}

In [[statistics]], '''regression toward the mean''' is the phenomenon that if a variable is extreme on its first measurement, it will tend to be closer to the average on a second measurement, and&mdash;a fact that may superficially seem paradoxical&mdash;if it is extreme on a second measurement, will tend to have been closer to the average on the first measurement.<ref>Everitt, B.S. (2002) ''The Cambridge Dictionary of Statistics'', CUP. ISBN 0-521-81099-x</ref><ref>Upton, G., Cook, I. (2006) ''Oxford Dictionary of Statistics'', OUP. ISBN 978-0-19-954145-4</ref><ref>{{cite journal | doi=10.1191/096228097676361431 | last=Stigler | first=Stephen M | title=Regression toward the mean, historically considered | journal=Statistical Methods in Medical Research | volume=6 | issue=2 | pages=103–114 | url=http://smm.sagepub.com/content/6/2/103.abstract | year=1997 | pmid=9261910}}</ref>  To avoid making wrong inferences, the possibility of regression toward the mean must be considered when designing experiments and interpreting experimental, survey, and other empirical data in the physical, life, behavioral and social sciences.

The conditions under which regression toward the mean occurs depend on the way the term is mathematically defined.  [[Sir Francis Galton]] first observed the phenomenon in the context of [[simple linear regression]] of data points. However, a less restrictive approach is possible. Regression towards the mean can be defined for any [[joint probability distribution|bivariate distribution]] with identical [[marginal distribution]]s. Two such definitions exist.<ref name=Samuels>Samuels (1991).</ref>  One definition accords closely with the common usage of the term “regression towards the mean”. Not all such bivariate distributions show regression towards the mean under this definition. However, all such bivariate distributions show regression towards the mean under the other definition.

Historically, what is now called regression toward the mean has also been called '''reversion to the mean''' and '''reversion to mediocrity'''.

==Conceptual background==

Consider a simple example: a class of students takes a 100-item true/false test on a subject. Suppose that all students choose randomly on all questions. Then, each student’s score would be a realization of one of a set of [[i.i.d. random variables]], with a [[mean]] of 50. Naturally, some students will score substantially above 50 and some substantially below 50 just by chance. If one takes only the top scoring 10% of the students and gives them a second test on which they again choose randomly on all items, the mean score would again be expected to be close to 50. Thus the mean of these students would “regress” all the way back to the mean of all students who took the original test. No matter what a student scores on the original test, the best prediction of his score on the second test is 50.

If there were no luck or random guessing involved in the answers supplied by students to the test questions then all students would score the same on the second test as they scored on the original test, and there would be no regression toward the mean.

Most realistic situations fall between these two extremes: for example, one might consider exam scores as a combination of [[skill]] and [[luck]]. In this case, the subset of students scoring above average would be composed of those who were skilled and had not especially bad luck, together with those who were unskilled, but were extremely lucky. On a retest of this subset, the unskilled will be unlikely to repeat their lucky break, while the skilled will have a second chance to have bad luck. Hence, those who did well previously are unlikely to do quite as well in the second test.

The following is a second example of regression toward the mean. A class of students takes two editions of the same test on two successive days.  It has frequently been observed that the worst performers on the first day will tend to improve their scores on the second day, and the best performers on the first day will tend to do worse on the second day.  The phenomenon occurs because student scores are determined in part by underlying ability and in part by chance. For the first test, some will be lucky, and score more than their ability, and some will be unlucky and score less than their ability. Some of the lucky students on the first test will be lucky again on the second test, but more of them will have (for them) average or below average scores. Therefore a student who was lucky on the first test is more likely to have a worse score on the second test than a better score. Similarly, students who score less than the mean on the first test will tend to see their scores increase on the second test.

==History==

The concept of regression comes from genetics and was popularized by [[Sir Francis Galton]] during the late 19th century with the publication of ''Regression towards mediocrity in hereditary stature''.<ref name="galton1886">{{cite journal|doi=10.2307/2841583|journal=The Journal of the Anthropological Institute of Great Britain and Ireland|url=http://jstor.org/stable/2841583|author=Galton, F.|title=Regression towards mediocrity in hereditary stature|year=1886|volume=15|pages=246–263|id={{JSTOR|2841583}}}}</ref>  Galton observed that extreme characteristics (e.g., height) in parents are not passed on completely to their offspring.  Rather, the characteristics in the offspring ''regress'' towards a ''mediocre'' point (a point which has since been identified as the mean).  By measuring the heights of hundreds of people, he was able to quantify regression to the mean, and estimate the size of the effect. Galton wrote that, “the average regression of the offspring is a constant fraction of their respective mid-parental deviations”. This means that the difference between a child and its parents for some characteristic is proportional to its parents' deviation from typical people in the population. So if its parents are each two inches taller than the averages for men and women, on average it will be shorter than its parents by some factor (which, today, we would call one minus the [[Regression analysis|regression coefficient]]) times two inches.  For height, Galton estimated this coefficient to be about 2/3: the height of an individual will measure around a mid-point that is two thirds of the parents’ deviation from the population average. 

Although Galton popularized the concept of regression, he fundamentally misunderstood the phenomenon; thus, his understanding of regression differs from that of modern statisticians as follows:  Galton observed correctly that the characteristics of an individual are not determined completely by their parents; there must be another source.  However, his explanation is that “A child inherits partly from his parents, partly from his ancestors.  Speaking generally, the further his genealogy goes back, the more numerous and varied will his ancestry become, until they cease to differ from any equally numerous sample taken at haphazard from the race at large.”<ref name="galton1886"/> In other words, Galton saw regression toward the mean as simply an inheritance of characteristics from ancestors that are not expressed in the parents; he did not understand regression to the mean as a general statistical phenomenon. In contrast to this view, it is now known that regression towards the mean occurs in all [[bivariate normal distribution]]s. If there is any random variation between the height of an individual and parents — if the correlation is not exactly equal to one — then the predictions must regress to the mean regardless of the underlying mechanisms of inheritance, race or culture. Thus, Galton was attributing random variation in height to the ancestry of the individual.

== Importance ==

Regression toward the mean is a significant consideration in the  [[design of experiments]].

Take a hypothetical example of 1,000 individuals of a similar age who were examined and scored on the risk of experiencing a heart attack. Statistics could be used to measure the success of an intervention on the 50 who were rated at the greatest risk. The intervention could be a change in diet, exercise, or a drug treatment. Even if the interventions are worthless, the test group would be expected to show an improvement on their next physical exam, because of regression toward the mean. The best way to combat this effect is to divide the group randomly into a treatment group that receives the treatment, and a [[Scientific control|control]] group that does not.
The treatment would then be judged effective only if the treatment group improves more than the control group.

Alternately, a group of [[disadvantaged]] children could be tested to identify the ones with most college potential.
The top 1% could be identified and supplied with special enrichment courses, tutoring, counseling and computers. Even if the program is effective, their average scores may well be less when the test is repeated a year later. However, in these circumstances it may be considered unfair to have a control group of disadvantaged children whose special needs are ignored. A mathematical calculation for [[Shrinkage (statistics)|shrinkage]] can adjust for this effect, although it will not be as reliable as the control group method (see also [[Stein's example]].)

The effect can also be exploited for general inference and estimation. The hottest place in the country today is more likely to be cooler tomorrow than hotter. The best performing mutual fund over the last three years is more likely to see relative performance decline than improve over the next three years. The most successful Hollywood actor of this year is likely to have less gross than more gross for his or her next movie. The baseball player with the greatest batting average by the All-Star break is more likely to have a lower average than a higher average over the second half of the season.

=== Misunderstandings===

The concept of regression toward the mean can be misused very easily.

In the student test example above, it was assumed implicitly that what was being measured did not change between the two measurements.
But suppose it was a pass/fail course and you had to score above 70 on both tests to pass. Then the students who scored under 70 the first time would have no incentive to do well, and might score worse on average the second time. The students just over 70, on the other hand, would have a strong incentive to study overnight and concentrate while taking the test. In that case you might see movement ''away'' from 70, scores below it getting lower and scores above it getting higher. It is possible for changes between the measurement times to augment, offset or reverse the statistical tendency to regress toward the mean.

Statistical regression toward the mean is not a [[causality|causal]] phenomenon. A student with the worst score on the test on the first day will not necessarily increase her score substantially on the second day due to the effect. On average the worst scorers improve, but that's only true because the worst scorers are more likely to have been unlucky than lucky. To the extent that a score is determined randomly, or that a score has random variation or error, as opposed to being determined by the student's academic ability or being a "true value", the phenomenon will have an effect. A classic mistake in this regard was in education. The students that received praise for good work were noticed to do more poorly on the next measure, and the students who were punished for poor work were noticed to do better on the next measure. The educators decided to stop praising and keep punishing on this basis. Such a decision was a mistake, because regression toward the mean is not based on cause and effect, but rather on random error in a natural distribution around a mean.{{Citation needed|date=January 2011}}

Although individual measurements regress toward the mean, the second [[Sampling (statistics)|sample]] of measurements will be no closer to the mean than the first. Consider the students again. Suppose their tendency is to regress 10% of the way toward the [[mean]] of 80, so a student who scored 100 the first day is [[Expected value|expected]] to score 98 the second day, and a student who scored 70 the first day is [[Expected value|expected]] to score 71 the second day. Those expectations are closer to the mean, on average, than the first day scores. But the second day scores will vary around their expectations, some will be higher and some will be lower. This will make the second set of measurements farther from the mean, on average, than their expectations. The effect is the exact reverse of regression toward the mean, and exactly offsets it. So for every individual, we expect the second score to be closer to the mean than the first score, but for ''all'' individuals, we expect the average distance from the mean to be the same on both sets of measurements.

Related to the point above, regression toward the mean works equally well in both directions. We expect the student with the highest test score on the second day to have done worse on the first day. And if we compare the best student on the first day to the best student on the second day, regardless of whether it is the same individual or not, there is a tendency to regress toward the mean going in either direction. We [[Expected value|expect]] the best scores on both days to be equally far from the mean.

=== Regression fallacies ===
{{Main|regression fallacy}}
Many phenomena tend to be attributed to the wrong causes when regression to the mean is not taken into account.

An extreme example is Horace Secrist’s 1933 book ''The Triumph of Mediocrity in Business'', in which the statistics professor collected mountains of data to prove that the profit rates of competitive businesses tend toward the average over time. In fact, there is no such effect; the variability of profit rates is almost constant over time. Secrist had only described the common regression toward the mean. One exasperated reviewer, [[Harold Hotelling]], likened the book to “proving the multiplication table by arranging elephants in rows and columns, and then doing the same for numerous other kinds of animals”.<ref>Hotelling, H. (1933). Review of The triumph of mediocrity in business by Secrist, H., ''Journal of the American Statistical Association'', 28, 433–435.</ref>

The calculation and interpretation of “improvement scores” on standardized educational tests in Massachusetts probably provides another example of the regression fallacy. In 1999, schools were given improvement goals. For each school, the Department of Education tabulated the difference in the average score achieved by students in 1999 and in 2000. It was quickly noted that most of the worst-performing schools had met their goals, which the Department of Education took as confirmation of the soundness of their policies. However, it was also noted that many of the supposedly best schools in the Commonwealth, such as Brookline High School (with 18 National Merit Scholarship finalists) were declared to have failed. As in many cases involving statistics and public policy, the issue is debated, but “improvement scores” were not announced in subsequent years and the findings appear to be a case of regression to the mean.

The psychologist [[Daniel Kahneman]], winner of the 2002 [[Nobel prize in economics]],  pointed out that regression to the mean might explain why rebukes can seem to improve performance, while praise seems to backfire.<ref>[http://nobelprize.virtual.museum/nobel_prizes/economics/laureates/2002/kahneman-autobio.html Daniel Kahneman’s autobiography]</ref>

{{cquote|I had the most satisfying Eureka experience of my career while attempting to teach flight instructors that praise is more effective than punishment for promoting skill-learning. When I had finished my enthusiastic speech, one of the most seasoned instructors in the audience raised his hand and made his own short speech, which began by conceding that positive reinforcement might be good for the birds, but went on to deny that it was optimal for flight cadets. He said, “On many occasions I have praised flight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don’t tell us that reinforcement works and punishment does not, because the opposite is the case.” This was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them. I immediately arranged a demonstration in which each participant tossed two coins at a target behind his back, without any feedback. We measured the distances from the target and could see that those who had done best the first time had mostly deteriorated on their second try, and vice versa. But I knew that this demonstration would not undo the effects of lifelong exposure to a perverse contingency.}}

UK law enforcement policies have encouraged the visible sitting of static or mobile [[speed camera]]s at [[accident blackspot]]s. This policy was justified by a perception that there is a corresponding reduction in serious [[road traffic accidents]] after a camera is set up. However, statisticians have pointed out that, although there is a net benefit in lives saved, failure to take into account the effects of regression to the mean results in the beneficial effects being overstated. It is thus claimed that some of the money currently spent on traffic cameras could be more productively directed elsewhere.<ref>[http://www.timesonline.co.uk/tol/news/uk/article766659.ece The Times, 16 December 2005 Speed camera benefits overrated]</ref>

Statistical analysts have long recognized the effect of regression to the mean in sports; they even have a special name for it: the “[[Sophomore slump|Sophomore Slump]]”. For example, [[Carmelo Anthony]] of the [[National Basketball Association|NBA]]’s [[Denver Nuggets]] had an outstanding rookie season in 2004. It was so outstanding, in fact, that he couldn’t possibly be expected to repeat it: in 2005, Anthony’s numbers had dropped from his rookie season. The reasons for the “sophomore slump” abound, as sports are all about adjustment and counter-adjustment, but luck-based excellence as a rookie is as good a reason as any.

Regression to the mean in sports performance may be the reason for the “[[Sports Illustrated Cover Jinx]]” and the “[[Madden Curse]]”. [[John Hollinger]] has an alternate name for the phenomenon of regression to the mean: the “fluke rule”, while [[Bill James]] calls it the “Plexiglas Principle”.

Because popular lore has focused on “regression toward the mean” as an account of declining performance of athletes from one season to the next, it has usually overlooked the fact that such regression can also account for improved performance.  For example, if one looks at the [[batting average]] of [[Major League Baseball]] players in one season, those whose batting average was above the league mean tend to regress downward toward the mean the following year, while those whose batting average was below the mean tend to progress upward toward the mean the following year.<ref>For an illustration see [[Nate Silver]], “Randomness: Catch the Fever!”, [http://www.baseballprospectus.com/article.php?articleid=1897''[[Baseball Prospectus]]'', May 14, 2003].</ref>

==Definition for simple linear regression of data points==

This is the definition of regression toward the mean that closely follows [[Sir Francis Galton]]'s original usage.<ref name="galton1886" />

Suppose there are ''n'' data points {''y''<sub>''i''</sub>, ''x''<sub>''i''</sub>}, where ''i'' = 1, 2, …, ''n''. We want to find the equation of the '''regression line''', ''i.e.'' the straight line

: <math> y = \alpha + \beta x, \,</math>
which would provide a <!-- maybe indefinite article here will aggravate some of the grammar purists, but it attempts to convey the idea that there could be many different ways to define "best" fit --> “best” fit for the data points. (Note that a straight line may not be the appropriate regression curve for the given data points.) Here the “best” will be understood as in the [[Ordinary least squares|least-squares]] approach: such a line that minimizes the sum of squared residuals of the linear regression model. In other words, numbers ''α'' and ''β'' solve the following minimization problem:

: Find <math>\min_{\alpha,\,\beta}Q(\alpha,\beta)</math>, where <math>Q(\alpha,\beta) = \sum_{i=1}^n\hat{\varepsilon}_i^{\,2} 

                        = \sum_{i=1}^n (y_i - \alpha - \beta x_i)^2\ </math>

Using simple [[calculus]] it can be shown that the values of ''α'' and ''β'' that minimize the objective function ''Q'' are

: <math>\begin{align}
  & \hat\beta = \frac{ \sum_{i=1}^{n} (x_{i}-\bar{x})(y_{i}-\bar{y}) }{ \sum_{i=1}^{n} (x_{i}-\bar{x})^2 }
              = \frac{ \overline{xy} - \bar{x}\bar{y} }{ \overline{x^2} - \bar{x}^2 }
              = \frac{ \operatorname{Cov}[x,y] }{ \operatorname{Var}[x] }
              = r_{xy} \frac{s_y}{s_x}, \\
  & \hat\alpha = \bar{y} - \hat\beta\,\bar{x},
  \end{align}</math>

where ''r<sub>xy</sub>'' is the [[Correlation#Sample_correlation|sample correlation coefficient]] between ''x'' and ''y'', ''s<sub>x</sub>'' is the [[standard deviation]] of ''x'', and ''s<sub>y</sub>'' is correspondingly the standard deviation of ''y''. Horizontal bar over a variable means the sample average of that variable. For example: <math style="height:1.5em">\overline{xy} = \tfrac{1}{n}\textstyle\sum_{i=1}^n x_iy_i\ .</math>

Substituting the above expressions for <math>\hat\alpha</math> and <math>\hat\beta</math> into  <math> y = \alpha + \beta x, \,</math> yields fitted values

: <math> \hat y = \hat\alpha + \hat\beta x, \,</math>

which yields

: <math>\frac{ \hat y-\bar{y}}{s_y} = r_{xy} \frac{ x-\bar{x}}{s_x}  </math>

This shows the role ''r''<sub>''xy''</sub> plays in the regression line of standardized data points.

If  -1 < ''r''<sub>''xy''</sub> < 1 , then we say that the data points exhibit regression toward the mean. In other words, if linear regression is the appropriate model for a set of data points  whose [[Correlation#Sample_correlation|sample correlation coefficient]]  is not perfect, then there is regression toward the mean.  The predicted (or fitted) standardized value of y is closer to its mean than the standardized value of x is to its mean.

==Definitions for bivariate distribution with identical marginal distributions==
===Restrictive definition===
Let ''X''<sub>1</sub>, ''X''<sub>2</sub> be [[random variable]]s with identical [[marginal distribution]]s with [[expected value|mean]] ''μ''. In this formalization, the [[joint distribution|bivariate distribution]] of ''X''<sub>1</sub> and ''X''<sub>2</sub> is said to exhibit '''regression toward the mean''' if, for every number ''c'' > ''μ'', we have
:''&mu;'' ≤ E[''X''<sub>2</sub> | ''X''<sub>1</sub> = ''c''] < ''c'',
with the reverse inequalities holding for ''c'' < ''μ''.<ref name=Samuels>Samuels (1991)</ref><ref name=Schmittlein>Schmittlein (1989)</ref>

The following is an informal description of the above definition. Consider a population of [[Widget (economics)|widgets]]. Each widget has two numbers, ''X''<sub>1</sub> and ''X''<sub>2</sub> (say, its left span (''X''<sub>1</sub> ) and right span (''X''<sub>2</sub>)).  Suppose that the probability distributions of ''X''<sub>1</sub> and ''X''<sub>2</sub> in the population are identical, and that the means of ''X''<sub>1</sub> and ''X''<sub>2</sub> are both  ''μ''. We now take a random widget from the population, and denote its ''X''<sub>1</sub> value by ''c''. (Note that ''c'' may be greater than, equal to, or smaller than ''μ''.) We have no access to the value of this widget's ''X''<sub>2</sub> yet. Let ''d'' denote the expected value of ''X''<sub>2</sub> of this particular widget. (''i.e.'' Let ''d'' denote the average value of ''X''<sub>2</sub> of all widgets in the population with ''X''<sub>1</sub>=''c''.) If the following condition is true:

:Whatever the value ''c'' is, ''d'' lies between ''&mu;'' and ''c'' (''i.e.'' ''d'' is closer to ''&mu;'' than ''c '' is),

then we say that ''X''<sub>1</sub> and ''X''<sub>2</sub> show '''regression toward the mean'''.

This definition accords closely with the current common usage, evolved from Galton's original usage, of the term "regression toward the mean." It is "restrictive" in the sense that not every bivariate distribution with identical marginal distributions exhibits regression toward the mean (under this definition).<ref name=Schmittlein>Schmittlein (1989)</ref>

====Theorem====

If a pair (''X'', ''Y'') of random variables follows a [[bivariate normal distribution]], then the conditional mean E(''Y''|''X'') is a linear function of ''X''.  The [[correlation coefficient]] ''r'' between ''X'' and ''Y'', along with the marginal means and variances of ''X'' and ''Y'', determines this linear relationship:

:<math>
\frac{E(Y|X)-EY}{\sigma_y} = r\frac{X-EX}{\sigma_x},
</math>

where ''EX'' and ''EY'' are the expected values of ''X'' and ''Y'', respectively, and σ<sub>''x''</sub> and σ<sub>''y''</sub> are the standard deviations of ''X'' and ''Y'', respectively.

Hence the conditional expected value of ''Y'', given that ''X'' is ''t'' [[standard deviation]]s above its mean (and that includes the case where it's below its mean, when ''t'' < 0), is ''rt'' standard deviations above the mean of ''Y''. Since |''r''| ≤ 1,  ''Y'' is no farther from the mean than ''X'' is, as measured in the number of standard deviations.<ref name=Chernick>[http://books.google.com/books?id=QRwuz6yA97oC&pg=PA272&dq=%22bivariate+normal+distribution%22+%22regression+toward+the+mean%22&ei=TkxvSqiiI47okATC-bHwDg Chernick & Friis (2003)]</ref>.

Hence, if  0 ≤  r < 1, then (''X'',''Y'') shows regression toward the mean (by this definition).

===General definition===
The following definition of ''reversion toward the mean'' has been proposed by Samuels as an alternative to the more restrictive definition of ''regression toward the mean'' above.<ref name=Samuels>Samuels (1991)</ref>

Let ''X''<sub>1</sub>, ''X''<sub>2</sub> be [[random variable]]s with identical [[marginal distribution]]s with [[expected value|mean]] ''μ''. In this formalization, the [[joint distribution|bivariate distribution]] of ''X''<sub>1</sub> and ''X''<sub>2</sub> is said to exhibit '''reversion toward the mean''' if, for every number ''c'', we have
:''&mu;'' ≤ E[''X''<sub>2</sub> | ''X''<sub>1</sub> > ''c''] < E[''X''<sub>1</sub> | ''X''<sub>1</sub> > ''c''], and

:''&mu;'' ≥ E[''X''<sub>2</sub> | ''X''<sub>1</sub> < ''c''] > E[''X''<sub>1</sub> | ''X''<sub>1</sub> < ''c'']

This definition is "general" in the sense that every bivariate distribution with identical marginal distributions exhibits ''reversion toward the mean''.

==See also==
{{Portal|Statistics}}
*[[Internal validity]]

==Notes==
{{Reflist|2}}

==References==
* {{Cite journal
 | author = J.M. Bland and D.G. Altman
 | title = Statistic Notes: Regression towards the mean
 | journal = [[British Medical Journal]]
 | volume = 308
 | pages = 1499
 | year = 1994
 | month = June
 | url = http://bmj.bmjjournals.com/cgi/content/full/308/6942/1499
 | pmid = 8019287
 | issue = 6942
 | pmc = 2540330
}} Article, including a diagram of Galton's original data.

* {{Cite book
 | author = Michael R. Chernick & Robert H. Friis
 | title = Introductory Biostatistics for the Health Sciences
 | publisher = [[Wiley-Interscience]]
 | year = 2003
 | url=http://books.google.com/?id=QRwuz6yA97oC&pg=PA272&dq=%22bivariate+normal+distribution%22+%22regression+toward+the+mean%22&q=%22bivariate%20normal%20distribution%22%20%22regression%20toward%20the%20mean%22
 | isbn = 9780471411376
}} Page 272

* {{Cite book
 | author = Edward J. Dudewicz & Satya N. Mishra
 | title = Modern Mathematical Statistics
 | publisher = [[John Wiley & Sons]]
 | year = 1988
 | isbn = 978-0471814726
}} Section 14.1: ''Estimation of regression parameters; Linear models''

* {{Cite journal
 | author = [[Francis Galton]]
 | title = Regression towards mediocrity in hereditary stature
 | journal = The Journal of the Anthropological Institute of Great Britain and Ireland
 | volume = 15
 | pages = 246&ndash;263
 | year = 1886
 | url = http://galton.org/essays/1880-1889/galton-1886-jaigi-regression-stature.pdf
 | doi = 10.2307/2841583
 | jstor = 2841583
 | publisher = The Journal of the Anthropological Institute of Great Britain and Ireland, Vol. 15
}}

* {{Cite book
 | author = Donald F. Morrison
 | title = Multivariate Statistical Methods
 | publisher = [[McGraw-Hill]]
 | year = 1967
 | isbn = 978-0534387785
}} Chapter 3: ''Samples from the Multivariate Normal Population''

* {{Cite book
 | author = [[Stephen M. Stigler]]
 | title = Statistics on the Table
 | publisher = [[Harvard University Press]]
 | year = 1999
}} See Chapter 9.

*{{cite journal |author=Myra L. Samuels |year=1991 |month=November |title=Statistical Reversion Toward the Mean: More Universal than Regression Toward the Mean |journal=[[The American Statistician]] |volume=45 |issue=4 |pages=pp 344–346 |url=http://www.jstor.org/pss/2684474 |doi=10.2307/2684474 |publisher=The American Statistician, Vol. 45, No. 4}}

* Stephen Senn (1990) [http://www.jstor.org/pss/2684164 Regression: A New Mode for an Old Meaning], ''[[The American Statistician]]'', Vol 44, No 2 (May 1990), pp 181–183.

* David C. Schmittlein (1989) [http://www.jstor.org/pss/2685070 Surprising Inferences from unsurprising Observations: Do Conditional Expectations really regress to the Mean?] ''[[The American Statistician]]'', Vol 43, No 3 (August 1989), pp 176–183.

==External links==
* [http://ije.oxfordjournals.org/cgi/reprint/34/1/215 Regression to the mean: what it is and how to deal with it], ''International Journal of Epidemiology''.
* [http://isites.harvard.edu/fs/docs/icb.topic469678.files/regress_to_mean1.pdf Regression Toward the Mean and the Study of Change], ''Psychological Bulletin''

*  [http://davidmlane.com/hyperstat/B153351.html A non-mathematical explanation of regression toward the mean.]
*  [http://onlinestatbook.com/stat_sim/reg_to_mean/index.html A simulation of regression toward the mean.]
* Amanda Wachsmuth, Leland Wilkinson, Gerard E. Dallal. [http://www.spss.com/research/wilkinson/Publications/galton.pdf Galton's Bend: An Undiscovered Nonlinearity in Galton's Family Stature Regression Data and a Likely Explanation Based on Pearson and Lee's Stature Data] ''(A modern look at Galton's analysis.)''

* Massachusetts standardized test scores, interpreted by a statistician as an example of regression: see [http://groups.google.com/groups?q=g:thl3845480903d&dq=&hl=en&lr=&ie=UTF-8&oe=UTF-8&safe=off&selm=93ikdr%24i20%241%40nnrp1.deja.com discussion in sci.stat.edu] and [http://groups.google.com/group/sci.stat.edu/tree/browse_frm/thread/c1086922ef405246/60bb528144835a38?rnum=21&hl=en&_done=%2Fgroup%2Fsci.sta its continuation].

{{Statistics}}

{{DEFAULTSORT:Regression Toward The Mean}}
[[Category:Statistical terminology]]
[[Category:Regression analysis]]
[[Category:Statistical laws]]

[[de:Regression zur Mitte]]
[[es:Regresión (estadística)]]
[[ja:平均への回帰]]</body> </html>