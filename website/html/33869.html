<html> <head> <title>Semidefinite embedding</title></head><body>{{Cleanup-jargon|date=June 2007}}
'''Semidefinite embedding''' (SDE) or '''maximum variance unfolding (MVU)''' is an [[algorithm]] in [[computer science]], that uses [[semidefinite programming]] to perform [[non-linear dimensionality reduction]] of high-dimensional [[coordinate vector|vector]]ial input data. 

Non-linear dimensionality reduction algorithms attempt to map high-dimensional data onto a low-dimensional [[Euclidean]] [[vector space]]. Maximum variance Unfolding is a member of the [[manifold learning]] family, which also include algorithms such as [[isomap]] and [[locally linear embedding]]. In manifold learning, the input data is assumed to be sampled from a low dimensional [[manifold]] that is embedded inside of a higher dimensional vector space. The main intuition behind MVU is to exploit the local linearity of manifolds and create a mapping that preserves local neighborhoods at every point of the underlying manifold. 

MVU creates a mapping from the high dimensional input vectors to some low dimensional Euclidean vector space in the following three steps:

# A [[neighborhood (graph theory)|neighborhood]] graph is created. Each input is connected with its k-nearest input vectors (according to Euclidean distance metric) and all k-nearest neighbors are connected with each other. If the data is sampled well enough, the resulting graph is a discrete approximation of the underlying manifold. 
# The neighborhood graph is "unfolded" with the help of semidefinite programming. Instead of learning the output vectors directly, the semidefinite programming aims to find an inner product matrix that maximizes the pairwise distances between any two inputs that are not connected in the neighborhood graph.
# The low-dimensional embedding is finally obtained by application of [[multidimensional scaling]] on the learned inner product matrix.

The steps of applying semidefinite programming followed by a linear dimensionality reduction step to recover a low-dimensional embedding into a Euclidean space were first proposed by Linial, London, and Rabinovich in a now classical article (see below). 
<!--
[[Image:Swissroll.jpg|500ex|caption]]-->

==See also==
* [[Locally linear embedding]]

==External links==
*[http://repository.upenn.edu/cgi/viewcontent.cgi%3farticle=1000%26context=cis_papers Unsupervised learning of image manifolds by semidefinite programming] K. Q. Weinberger and L. K. Saul (2004). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-04), Washington D.C. 
*[http://www.springerlink.com/content/t21q747q278qx4x1/ Unsupervised learning of image manifolds by semidefinite programming] K. Q. Weinberger and L. K. Saul (2005), International Journal of Computer Vision - In Special Issue: Computer Vision and Pattern Recognition-CVPR 2005 Guest Editor(s): [[Aaron Bobick]], [[Rama Chellappa]], [[Larry Davis]], pages 77-90, Volume 70, Number 1, [[Springer Netherlands]]
*[http://www.weinbergerweb.net/Downloads/MVU.html MVU Matlab code online]
*[http://citeseer.ist.psu.edu/170127.html The geometry of graphs and some of its algorithmic applications], [[Nathan Linial]], [[Eran London]], [[Yuri Rabinovich]], [[IEEE]] Symposium on Foundations of Computer Science. 


[[Category:Computational statistics]]
[[Category:Dimension reduction]]</body> </html>