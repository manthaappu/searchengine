<html> <head> <title>Entropy estimation</title></head><body>Estimating the [[differential entropy]] of a system or process, given some observations, is useful in various science/engineering applications, such as [[Independent Component Analysis]],<ref>Dinh-Tuan Pham (2004) Fast algorithms for mutual information based independent component analysis. In ''Signal Processing''. Volume 52,  Issue 10, 2690 - 2700, {{doi|10.1109/TSP.2004.834398}}</ref> [[image analysis]],<ref>Chang, C.-I.; Du, Y.; Wang, J.; Guo, S.-M.; Thouin, P.D. (2006) Survey and comparative analysis of entropy and relative entropy thresholding techniques. In ''Vision, Image and Signal Processing'', Volume 153,  Issue 6, 837 - 850, {{doi|10.1049/ip-vis:20050032}}</ref> [[genetic analysis]],<ref>Martins, D. C. ''et al.'' (2008) Intrinsically Multivariate Predictive Genes. In ''Selected Topics in Signal Processing''. Volume 2,  Issue 3, 424 - 439, {{doi|10.1109/JSTSP.2008.923841}}</ref> [[speech recognition]],<ref>Gue Jun Jung; Yung-Hwan Oh (2008) Information Distance-Based Subvector Clustering for ASR Parameter Quantization. In ''Signal Processing Letters'', Volume 15, 209 - 212, {{doi|10.1109/LSP.2007.913132 }}</ref> [[manifold learning]],<ref>Costa, J.A.; Hero, A.O. (2004), Geodesic entropic graphs for dimension and entropy estimation in manifold learning. In ''Signal Processing'', Volume 52,  Issue 8,  2210 - 2221, {{doi|10.1109/TSP.2004.831130}}</ref> and time delay estimation.<ref>Benesty, J.; Yiteng Huang; Jingdong Chen (2007) Time Delay Estimation via Minimum Entropy. In ''Signal Processing Letters'', Volume 14,  Issue 3,  March 2007 157 - 160 {{doi|10.1109/LSP.2006.884038 }}</ref> The simplest and most common approach uses histogram-based estimation, but other approaches have been developed and used, each with their own benefits and drawbacks.<ref name="beirlant">J. Beirlant, E. J. Dudewicz, L. Gyorfi, and E. C. van der Meulen (1997) [http://www.its.caltech.edu/~jimbeck/summerlectures/references/Entropy%20estimation.pdf Nonparametric entropy estimation: An overview]. In ''International Journal of Mathematical and Statistical Sciences'', Volume 6, pp. 17– 
39.</ref> The main factor in choosing a method is often  a trade-off between the bias and the variance of the estimate<ref name="schurmann">T. Schürmann, Bias analysis in entropy estimation. In ''J. Phys. A: Math. Gen'', 37 (2004), pp. L295–L301. {{doi|10.1088/0305-4470/37/27/L02}}</ref> although the nature of the (suspected) distribution of the data may also be a factor.<ref name="beirlant"/>

==Histogram estimator==
The histogram approach uses the idea that the differential entropy, 

:<math>H(X) = -\int_\mathbb{X} f(x)\log f(x)\,dx</math>

can be approximated by producing a [[histogram]] of the observations, and then finding the discrete entropy

:<math>
\begin{matrix}
H(X)  =  - \displaystyle{\sum_{i=1}^nf(x_i)\log f(x_i)} \qquad
\end{matrix}
</math>

of that histogram (which is itself a [[Maximum likelihood|maximum-likelihood estimate]] of the discretized frequency distribution). Histograms can be quick to calculate, and simple, so this approach has some attractions. However, the estimate produced is [[bias]]ed, and although corrections can be made to the estimate, they may not always be satisfactory.<ref name="miller55">G. Miller (1955) Note on the bias of information estimates. In ''Information Theory in Psychology: Problems and Methods'', pp. 95–100.</ref>

A method better suited for multidimensional pdf's is to first make a pdf estimate with some method, and then, from the pdf estimate, compute the entropy. A useful pdf estimate method is e.g. Gaussian Mixture Modeling (GMM), where the [[Expectation Maximization]] (EM) algorithm is used to find an ML estimate of a weighted sum of Gaussian pdf's approximating the data pdf.

==Estimates based on sample-spacings==
If the data is one-dimensional, we can imagine taking all the observations and putting them in order of their value. The spacing between one value and the next then gives us a rough idea of (the [[Multiplicative inverse|reciprocal]] of) the probability density in that region: the closer together the values are, the higher the probability density. This is a very rough estimate with high [[variance]], but can be improved, for example by thinking about the space between a given value and the one ''m'' away from it, where ''m'' is some fixed number.<ref name="beirlant"/>

The probability density estimated in this way can then be used to calculate the entropy estimate, in a similar way to that given above for the histogram, but with some slight tweaks.

One of the main drawbacks with this approach is going beyond one dimension: the idea of lining the data points up in order falls apart in more than one dimension. However, using analogous methods, some multidimensional entropy estimators have been developed.<ref name="lm2003">E. G. Learned-Miller (2003) A new class of entropy estimators for multi-dimensional densities, in ''Proceedings of the [[International Conference on Acoustics, Speech, and Signal Processing]] (ICASSP’03)'', vol. 3, April 2003, pp. 297–300.</ref><ref name="il2010">I. Lee (2010) Sample-spacings based density and entropy estimators for spherically invariant multidimensional data, In ''Neural Computation'', vol. 22, issue 8, April 2010, pp. 2208–2227.</ref>

==Estimates based on nearest-neighbours==
For each point in our dataset, we can find the distance to its [[nearest neighbour]]. We can in fact estimate the entropy from the distribution of the nearest-neighbour-distance of our datapoints.<ref name="beirlant"/> (In a uniform distribution these distances all tend to be fairly similar, whereas in a strongly nonuniform distribution they may vary a lot more.)

==References==
{{reflist}}

{{DEFAULTSORT:Entropy Estimation}}
[[Category:Entropy and information]]
[[Category:Information theory]]
[[Category:Statistical randomness]]
[[Category:Randomness]]

[[de:Entropieschätzung]]</body> </html>