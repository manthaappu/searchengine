<html> <head> <title>Conditional entropy</title></head><body>[[Image:Conditional_entropy.png|thumb|256px|right|Individual (H(X),H(Y)), joint (H(X,Y)), and conditional entropies for a pair of correlated subsystems X,Y with mutual information I(X; Y).]]
In [[information theory]], the '''conditional entropy''' (or '''equivocation''') quantifies the remaining [[information entropy|entropy]] (i.e. uncertainty) of a [[random variable]] <math>Y</math> given that the value of another random variable <math>X</math> is known.  It is referred to as ''the entropy of <math>Y</math> conditional on <math>X</math>'', and is written <math>H(Y|X)</math>.  Like other entropies, the conditional entropy is measured in [[bit]]s, [[nat (information)|nat]]s, or [[ban (information)|ban]]s.

== Definition ==
More precisely, if <math>H(Y|X=x)</math> is the entropy of the variable <math>Y</math> conditional on the variable <math>X</math> taking a certain value <math>x</math>, then <math>H(Y|X)</math> is the result of averaging <math>H(Y|X=x)</math> over all possible values <math>x</math> that <math>X</math> may take.

Given discrete random variable <math>X</math> with support <math>\mathcal X</math> and <math>Y</math> with support <math>\mathcal Y</math>, the conditional entropy of <math>Y</math> given <math>X</math> is defined as:

::<math>\begin{align}
H(Y|X)\ &\stackrel{\mathrm{def}}{=}\sum_{x\in\mathcal X}\,p(x)\,H(Y|X=x)\\
&{=}-\sum_{x\in\mathcal X}p(x)\sum_{y\in\mathcal Y}\,p(y|x)\,\log\,p(y|x)\\
&=-\sum_{x\in\mathcal X}\sum_{y\in\mathcal Y}\,p(x,y)\,\log\,p(y|x)\\
&=-\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(y|x)\\
&=-\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x,y)} {p(x)} \\
&= \sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x)} {p(x,y)}.
\end{align}</math>

==Chain rule==

From this definition and the definition of conditional probability, the chain rule for conditional entropy is 

<math>H(Y|X)\,=\,H(Y,X)-H(X) \, .</math>

This is true because 

<math>\begin{align}
H(Y|X)=&\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x)} {p(x,y)}\\
 =&-\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(x,y) + \sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(x) \\
=& H(X,Y) + \sum_{x \in \mathcal X} p(x)\log\,p(x) \\
=& H(X,Y) - H(X).
\end{align}</math>

==Intuition==

Intuitively, the combined system contains <math>H(X,Y)</math> bits of information: we need <math>H(X,Y)</math> bits of information to reconstruct its exact state. If we learn the value of <math>X</math>, we have gained <math>H(X)</math> bits of information, and the system has <math>H(Y|X)</math> bits of uncertainty remaining. 

<math>H(Y|X)=0</math> if and only if the value of <math>Y</math> is completely determined by the value of <math>X</math>.  Conversely, <math>H(Y|X) = H(Y)</math> if and only if <math>Y</math> and <math>X</math> are [[independent random variables]].

==Generalization to quantum theory==

In [[quantum information theory]], the conditional entropy is generalized to the [[conditional quantum entropy]].

==Other properties==

For any <math>X</math> and <math>Y</math>:

<math>H(X|Y) \le H(X)</math> 

<math>H(X,Y) = H(X|Y) + H(Y|X) + I(X;Y)</math>, where <math>I(X;Y)</math> is the mutual information between <math>X</math> and <math>Y</math>.

<math>I(X;Y) \le H(X)</math>, where <math>I(X;Y)</math> is the mutual information between <math>X</math> and <math>Y</math>.

For independent <math>X</math> and <math>Y</math>:

<math>H(Y|X) = H(X)</math> and <math>H(X|Y) = H(X)</math>

==References==
# {{cite book |author=Theresa M. Korn; Korn, Granino Arthur |title=Mathematical Handbook for Scientists and Engineers: Definitions, Theorems, and Formulas for Reference and Review |publisher=Dover Publications |location=New York |year= |pages=613–614 |isbn=0-486-41147-8 |oclc= |doi=}}
# {{cite book |author=C. Arndt |title=Information Measures: Information and its description in Science and Engineering)  |publisher=Springer |location=Berlin |year=2001 |pages=370–373 |isbn=3-540-41633-1 |oclc= |doi=}}

== See also ==
* [[Entropy (information theory)]]
* [[Mutual information]]
* [[Conditional quantum entropy]]
* [[Variation of information]]

* [[Likelihood function]]

[[Category:Entropy and information]]
[[Category:Information theory]]

[[bar:Bedingte Entropie]]
[[de:Bedingte Entropie]]
[[fr:Entropie conditionnelle]]
[[pl:Entropia warunkowa]]
[[zh:条件熵]]</body> </html>