<html> <head> <title>Operant conditioning</title></head><body>{{Redirect|Operant|the definition of the word "operant"|Wiktionary:operant}}
{{Merge from|Mutual operant conditioning|date=May 2008}}
'''Operant conditioning''' is the use of a behavior's antecedent and/or its consequence to influence the occurrence and form of behavior. Operant conditioning is distinguished from [[classical conditioning]] (also called respondent conditioning) in that operant conditioning deals with the [[behavior modification|modification of "voluntary behavior"]] or operant behavior. Operant behavior "operates" on the environment and is maintained by its consequences, while classical conditioning deals with the conditioning of reflexive ([[reflex]]) behaviors which are elicited by [[antecedent]] conditions. Behaviors conditioned via a classical conditioning procedure are not maintained by consequences.<ref>Domjan, Michael, Ed., The Principles of Learning and Behavior, Fifth Edition, Belmont, CA: Thomson/Wadsworth, 2003</ref>
{{TOC limit|limit=2}}
==Reinforcement, punishment, and extinction==
[[Reinforcement]] and [[punishment]], the core tools of operant conditioning, are either positive (delivered following a response), or negative (withdrawn following a response). This creates a total of four basic consequences, with the addition of a fifth procedure known as [[extinction (psychology)|extinction]] (i.e. no change in consequences following a response).

It's important to note that actors are not spoken of as being reinforced, punished, or extinguished; it is the actions that are reinforced, punished, or extinguished. Additionally, reinforcement, punishment, and extinction are not terms whose use is restricted to the laboratory. Naturally occurring consequences can also be said to reinforce, punish, or extinguish behavior and are not always delivered by people.
* '''[[Reinforcement]]''' is a consequence that causes a behavior to occur with greater frequency.
* '''[[Punishment (psychology)|Punishment]]''' is a consequence that causes a behavior to occur with less frequency.
* '''[[extinction (psychology)|Extinction]]''' is the lack of any consequence following a behavior. When a behavior is inconsequential, producing neither favorable nor unfavorable consequences, it will occur with less frequency. When a previously reinforced behavior is no longer reinforced with either positive or negative reinforcement, it leads to a decline in the response.

===Four contexts of operant conditioning===
Here the terms ''positive'' and ''negative'' are not used in their [[popular culture|popular sense]], but rather: ''positive'' refers to addition, and ''negative'' refers to subtraction.

What is added or subtracted may be either reinforcement or punishment. Hence ''positive punishment'' is sometimes a confusing term, as it denotes the "addition" of a stimulus or increase in the intensity of a stimulus that is aversive (such as spanking or an electric shock). The four procedures are:
# '''Positive reinforcement''' (Reinforcement): occurs when a behavior (response) is followed by a stimulus that is appetitive or [[reward system|rewarding]], increasing the frequency of that behavior. In the [[Skinner box]] experiment, a stimulus such as food or sugar solution can be delivered when the rat engages in a target behavior, such as pressing a lever.
# '''Negative reinforcement''' (Escape): occurs when a behavior (response) is followed by the removal of an [[aversive]] stimulus, thereby increasing that behavior's frequency. In the Skinner box experiment, negative reinforcement can be a loud noise continuously sounding inside the rat's cage until it engages in the target behavior, such as pressing a lever, upon which the loud noise is removed.
# '''Positive punishment''' (Punishment) (also called "Punishment by contingent stimulation"): occurs when a behavior (response) is followed by a stimulus, such as introducing a shock or loud noise, resulting in a decrease in that behavior.
# '''Negative punishment''' (Penalty) (also called "Punishment by contingent withdrawal"): occurs when a behavior (response) is followed by the removal of a stimulus, such as taking away a child's toy following an undesired behavior, resulting in a decrease in that behavior.

''Also:''
* '''Avoidance learning''' is a type of learning in which a certain behavior results in the cessation of an aversive stimulus. For example, performing the behavior of shielding one's eyes when in the sunlight (or going indoors) will help avoid the aversive stimulation of having light in one's eyes.
* '''Extinction''' occurs when a behavior (response) that had previously been reinforced is no longer effective. In the Skinner box experiment, this is the rat pushing the lever and being rewarded with a food pellet several times, and then pushing the lever again and never receiving a food pellet again. Eventually the rat would cease pushing the lever.
* '''Noncontingent reinforcement''' refers to delivery of reinforcing stimuli regardless of the organism's (aberrant) behavior. The idea is that the target behavior decreases because it is no longer necessary to receive the reinforcement. This typically entails time-based delivery of stimuli identified as maintaining aberrant behavior, which serves to decrease the rate of the target behavior.<ref>Tucker, M., Sigafoos, J., & Bushell, H. (1998). Use of noncontingent reinforcement in the treatment of challenging behavior. Behavior Modification, 22, 529–547.</ref> As no measured behavior is identified as being strengthened, there is controversy surrounding the use of the term noncontingent "reinforcement".<ref>Poling, A., & Normand, M. (1999). Noncontingent reinforcement: an inappropriate description of time-based schedules that reduce behavior. Journal of Applied Behavior Analysis, 32, 237–238.</ref>
* '''[[Shaping (psychology)|Shaping]]''' is a form of operant conditioning in which the increasingly accurate approximations of a desired response are reinforced.<ref name="bbbautism">http://www.bbbautism.com/aba_shaping_and_chaining.htm</ref>
* '''[[Chaining]]''' is an instructional procedure which involves reinforcing individual responses occurring in a sequence to form a complex behavior.<ref name="bbbautism"/>

==Thorndike's law of effect==
{{Main|Law of effect}}
Operant conditioning, sometimes called ''instrumental conditioning'' or ''instrumental learning'', was first extensively studied by [[Edward L. Thorndike]] (1874–1949), who observed the behavior of cats trying to escape from home-made puzzle boxes.<ref>Thorndike, E.L. (1901). Animal intelligence: An experimental study of the associative processes in animals. Psychological Review Monograph Supplement, 2, 1–109.</ref> When first constrained in the boxes, the cats took a long time to escape. With experience, ineffective responses occurred less frequently and successful responses occurred more frequently, enabling the cats to escape in less time over successive trials. In his [[law of effect]], Thorndike theorized that successful responses, those producing ''satisfying'' consequences, were "stamped in" by the experience and thus occurred more frequently. Unsuccessful responses, those producing ''annoying'' consequences, were ''stamped out'' and subsequently occurred less frequently. In short, some consequences ''strengthened'' behavior and some consequences ''weakened'' behavior. Thorndike produced the first known learning curves through this procedure. [[B.F. Skinner]] (1904–1990) formulated a more detailed analysis of operant conditioning based on reinforcement, punishment, and extinction. Following the ideas of [[Ernst Mach]], Skinner rejected Thorndike's mediating structures required by "satisfaction" and constructed a new conceptualization of behavior without any such references. So, while experimenting with some homemade feeding mechanisms, Skinner invented the [[operant conditioning chamber]] which allowed him to measure rate of response as a key dependent variable using a cumulative record of lever presses or key pecks.<ref>Mecca Chiesa (2004) Radical Behaviorism: the philosophy and the science</ref>

==Biological correlates of operant conditioning==
The first scientific studies identifying [[neuron]]s that responded in ways that suggested they encode for conditioned stimuli came from work by [[Mahlon deLong]]<ref>[http://jn.physiology.org/cgi/content/citation/34/3/414 "Activity of pallidal neurons during movement"], M.R. DeLong, ''J. Neurophysiol.'', 34:414–27, 1971</ref><ref name=RTR&MRD>Richardson RT, DeLong MR (1991): Electrophysiological studies of the function of the nucleus basalis in primates. In Napier TC, Kalivas P, Hamin I (eds), ''The Basal Forebrain: Anatomy to Function'' (''Advances in Experimental Medicine and Biology'', vol. 295. New York, Plenum, pp. 232–252</ref> and by R.T. "Rusty" Richardson.<ref name=RTR&MRD/> They showed that [[nucleus basalis]] neurons, which release [[Acetylcholine#Neuromodulatory Effects|acetylcholine]] broadly throughout the [[cerebral cortex]], are activated shortly after a conditioned stimulus, or after a primary reward if no conditioned stimulus exists. These neurons are equally active for positive and negative reinforcers, and have been demonstrated to cause [[neuroplasticity|plasticity]] in many [[cerebral cortex|cortical]] regions.<ref>PNAS 93:11219-24 1996, Science 279:1714–8 1998</ref> Evidence also exists that [[Dopamine#Functions in the brain|dopamine]] is activated at similar times. There is considerable evidence that dopamine participates in both reinforcement and aversive learning.<ref>Neuron 63:244–253, 2009, Frontiers in Behavioral Neuroscience, 3: Article 13, 2009</ref> Dopamine pathways project much more densely onto [[frontal cortex]] regions. [[Cholinergic]] projections, in contrast, are dense even in the posterior cortical regions like the [[primary visual cortex]]. A study of patients with [[Parkinson's disease]], a condition attributed to the insufficient action of dopamine, further illustrates the role of dopamine in positive reinforcement.<ref>Michael J. Frank, Lauren C. Seeberger, and Randall C. O'Reilly (2004) "By Carrot or by Stick: Cognitive Reinforcement Learning in Parkinsonism," Science 4, November 2004</ref> It showed that while off their medication, patients learned more readily with aversive consequences than with positive reinforcement. Patients who were on their medication showed the opposite to be the case, positive reinforcement proving to be the more effective form of learning when the action of dopamine is high.

==Factors that alter the effectiveness of consequences==
When using consequences to modify a response, the effectiveness of a consequence can be increased or decreased by various factors. These factors can apply to either reinforcing or punishing consequences.
# '''Satiation/Deprivation:''' The effectiveness of a consequence will be reduced if the individual's "appetite" for that source of stimulation has been satisfied. Inversely, the effectiveness of a consequence will increase as the individual becomes deprived of that stimulus. If someone is not hungry, food will not be an effective reinforcer for behavior. Satiation is generally only a potential problem with primary reinforcers, those that do not need to be learned such as food and water.
# '''Immediacy:''' After a response, how immediately a consequence is then felt determines the effectiveness of the consequence. More immediate feedback will be more effective than less immediate feedback. If someone's license plate is caught by a traffic camera for speeding and they receive a speeding ticket in the mail a week later, this consequence will not be very effective against speeding. But if someone is speeding and is caught in the act by an officer who pulls them over, then their speeding behavior is more likely to be affected.
# '''Contingency:''' If a consequence does not contingently (reliably, or consistently) follow the target response, its effectiveness upon the response is reduced. But if a consequence follows the response consistently after successive instances, its ability to modify the response is increased. The schedule of reinforcement, when consistent, leads to faster learning. When the schedule is variable the learning is slower. Extinction is more difficult when learning occurs during intermittent reinforcement and more easily extinguished when learning occurs during a highly consistent schedule.
# '''Size:''' This is a "cost-benefit" determinant of whether a consequence will be effective. If the size, or amount, of the consequence is large enough to be worth the effort, the consequence will be more effective upon the behavior. An unusually large lottery jackpot, for example, might be enough to get someone to buy a one-dollar lottery ticket (or even buying multiple tickets). But if a lottery jackpot is small, the same person might not feel it to be worth the effort of driving out and finding a place to buy a ticket. In this example, it's also useful to note that "effort" is a punishing consequence. How these opposing expected consequences (reinforcing and punishing) balance out will determine whether the behavior is performed or not.

Most of these factors exist for biological reasons. The biological purpose of the Principle of Satiation is to maintain the organism's [[homeostasis]]. When an organism has been deprived of sugar, for example, the effectiveness of the taste of sugar as a reinforcer is high. However, as the organism reaches or exceeds their optimum blood-sugar levels, the taste of sugar becomes less effective, perhaps even aversive.

The Principles of Immediacy and Contingency exist for neurochemical reasons. When an organism experiences a reinforcing stimulus, [[dopamine]] pathways in the brain are activated. This network of pathways "releases a short pulse of dopamine onto many [[dendrites]], thus broadcasting a rather global reinforcement signal to postsynaptic neurons."<ref>Schultz, Wolfram (1998). Predictive Reward Signal of Dopamine Neurons. ''The Journal of Neurophysiology'', 80(1), 1–27.</ref> This results in the plasticity of these synapses allowing recently activated synapses to increase their sensitivity to efferent signals, hence increasing the probability of occurrence for the recent responses preceding the reinforcement. These responses are, statistically, the most likely to have been the behavior responsible for successfully achieving reinforcement. But when the application of reinforcement is either less immediate or less contingent (less consistent), the ability of dopamine to act upon the appropriate synapses is reduced.

==Operant variability==
Operant variability is what allows a response to adapt to new situations. Operant behavior is distinguished from reflexes in that its '''response topography''' (the form of the response) is subject to slight variations from one performance to another. These slight variations can include small differences in the specific motions involved, differences in the amount of force applied, and small changes in the timing of the response. If a subject's history of reinforcement is consistent, such variations will remain stable because the same successful variations are more likely to be reinforced than less successful variations. However, behavioral variability can also be altered when subjected to certain controlling variables.<ref>Neuringer, A. (2002). Operant variability: Evidence, functions, and theory. Psychonometric Bulletin & Review, 9(4), 672–705.</ref>

==Avoidance learning==
Avoidance learning belongs to negative reinforcement schedules. The subject learns that a certain response will result in the termination or prevention of an aversive stimulus. There are two kinds of commonly used experimental settings: discriminated and free-operant avoidance learning.

===Discriminated avoidance learning===
In discriminated avoidance learning, a novel stimulus such as a light or a tone is followed by an aversive stimulus such as a shock (CS-US, similar to classical conditioning). During the first trials (called escape-trials) the animal usually experiences both the CS (Conditioned Stimulus) and the US (Unconditioned Stimulus), showing the operant response to terminate the aversive US. During later trials, the animal will learn to perform the response already during the presentation of the CS thus preventing the aversive US from occurring. Such trials are called "avoidance trials."

===Free-operant avoidance learning===
In this experimental session, no discrete stimulus is used to signal the occurrence of the aversive stimulus. Rather, the aversive stimulus (mostly shocks) are presented without explicit warning stimuli. There are two crucial time intervals determining the rate of avoidance learning. This first one is called the S-S-interval (shock-shock-interval). This is the amount of time which passes during successive presentations of the shock (unless the operant response is performed). The other one is called the R-S-interval (response-shock-interval) which specifies the length of the time interval following an operant response during which no shocks will be delivered. Note that each time the organism performs the operant response, the R-S-interval without shocks begins anew.

==Two-process theory of avoidance==
This theory was originally established to explain learning in discriminated avoidance learning. It assumes two processes to take place:
; ''a) Classical conditioning of fear.''
: During the first trials of the training, the organism experiences both CS and aversive US (escape-trials). The theory assumed that during those trials classical conditioning takes place by pairing the CS with the US. Because of the aversive nature of the US the CS is supposed to elicit a conditioned emotional reaction (CER) – fear. In classical conditioning, presenting a CS conditioned with an aversive US disrupts the organism's ongoing behavior.
; ''b) Reinforcement of the operant response by fear-reduction.''
: Because during the first process, the CS signaling the aversive US has itself become aversive by eliciting fear in the organism, reducing this unpleasant emotional reaction serves to motivate the operant response. The organism learns to make the response during the US, thus terminating the aversive internal reaction elicited by the CS. An important aspect of this theory is that the term "avoidance" does not really describe what the organism is doing. It does not "avoid" the aversive US in the sense of anticipating it. Rather the organism escapes an aversive internal state, caused by the CS.

==''Verbal Behavior''==
{{Main|Verbal Behavior (book)}}
In 1957, [[B. F. Skinner|Skinner]] published ''[[Verbal Behavior (book)|Verbal Behavior]]'', a theoretical extension of the work he had pioneered since 1938. This work extended the theory of operant conditioning to human behavior previously assigned to the areas of language, linguistics and other areas. ''Verbal Behavior'' is the logical extension of Skinner's ideas, in which he introduced new functional relationship categories such as intraverbals, [[Autoclitics (psychology)|autoclitics]], mands, tacts and the controlling relationship of the audience. All of these relationships were based on operant conditioning and relied on no new mechanisms despite the introduction of new functional categories.

==Four term contingency==
Applied behavior analysis, which is the name of the discipline directly descended from Skinner's work, holds that behavior is explained in four terms: conditional stimulus (S<sup>C</sup>), a discriminative stimulus (S<sup>d</sup>), a response (R), and a reinforcing stimulus (S<sup>rein</sup> or S<sup>r</sup> for reinforcers, sometimes S<sup>ave</sup> for aversive stimuli).<ref>Pierce & Cheney (2004) Behavior Analysis and Learning</ref>

==Operant hoarding==
'''Operant hoarding''' is a referring to the choice made by a rat, on a [[compound schedule]] called a [[multiple schedule]], that maximizes its rate of [[reinforcement]] in an operant conditioning context.  More specifically, rats were shown to have allowed food pellets to accumulate in a food tray by continuing to press a lever on a [[continuous reinforcement]] schedule instead of retrieving those pellets. Retrieval of the pellets always instituted a one-minute period of [[extinction]] during which no additional food pellets were available but those that had been accumulated earlier could be consumed. This finding appears to contradict the usual finding that rats behave impulsively in situations in which there is a choice between a smaller food object right away and a larger food object after some delay. See [[schedules of reinforcement]].<ref>Cole, M.R. (1990). Operant hoarding: A new paradigm for the study of self-control. ''Journal of the Experimental Analysis of Behavior, 53'', 247–262.</ref>

==An alternative to the law of effect==
However, an alternative perspective has been proposed by R. Allen and Beatrix Gardner.<ref>Gardner, R.A., & Gardner, B.T. (1988). Feedforward vs feedbackward: An ethological alternative to the law of effect. Behavioral and Brain Sciences. 11:429–447.</ref><ref>Gardner, R.A. & Gardner, B.T. (1998). The structure of learning from sign stimuli to sign language. Mahwah NJ: Lawrence Erlbaum Associates.</ref> Under this idea, which they called "feedforward," animals learn during operant conditioning by simple pairing of stimuli, rather than by the consequences of their actions. Skinner asserted that a rat or pigeon would only manipulate a lever if rewarded for the action, a process he called "shaping" (reward for approaching then manipulating a lever).<ref>Skinner, B.F. (1953). Science and human behavior. Oxford, England: Macmillan.</ref> However, in order to prove the necessity of reward (reinforcement) in lever pressing, a control condition where food is delivered without regard to behavior must also be conducted. Skinner never published this control group. Only much later was it found that rats and pigeons do indeed learn to manipulate a lever when food comes irrespective of behavior. This phenomenon is known as autoshaping.<ref> Brown, P., & Jenkins, H.M. (1968). Autoshaping of the pigeon's key-peck. J. Exp. Anal. Behav. 11:1–8.</ref> Autoshaping demonstrates that consequence of action is not necessary in an operant conditioning chamber, and it contradicts the law of effect. Further experimentation has shown that rats naturally handle small objects, such as a lever, when food is present.<ref>Timberlake, W. (1983). Rats' responses to a moving object related to food or water: A behavior-systems analysis. Animal Learning & Behavior. 11(3):309–320.</ref> Rats seem to insist on handling the lever when free food is available (contra-freeloading)<ref>Jensen, G.D. (1963). Preference for bar pressing over 'freeloading' as a function of number of rewarded presses. Journal of Experimental Psychology. 65:451–454.</ref><ref>Neuringer, A.J. (1969). Animals respond for food in the presence of free food. Science. 166:399-401.</ref> and even when pressing the lever leads to less food (omission training).<ref>Williams, D.R. and Williams, H. (1969). Auto-maintenance in the pigeon: sustained pecking despite contingent non-reinforcement. J. Exper. Analys. of Behav. 12:511–520.</ref><ref>Peden, B.F., Brown, M.P., & Hearst, E. (1977). Persistent approaches to a signal for food despite food omission for approaching. Journal of Experimental Psychology: Animal Behavior Processes. 3(4):377–399.</ref> Whenever food is presented, rats handle the lever, regardless if lever pressing leads to more food. Therefore, handling a lever is a natural behavior that rats do as preparatory feeding activity, and in turn, lever pressing cannot logically be used as evidence for reward or reinforcement to occur. In the absence of evidence for reinforcement during operant conditioning, learning which occurs during operant experiments is actually only Pavlovian (classical) conditioning. The dichotomy between Pavlovian and operant conditioning is therefore an inappropriate separation.

==See also==
{{Portal|Psychology}}
{{multicol}}
* [[Animal testing]]
* [[Applied behavior analysis]], the application of operant behaviorism
* [[Behaviorism]], the family of philosophies behind operant conditioning
* [[Cognitivism (psychology)]], a competing theory that invokes internal mechanisms without reference to behavior
* [[Educational psychology]]
* [[Educational technology]]
* [[Experimental analysis of behavior]]
* [[Exposure therapy]]
* [[Habituation]]
{{multicol-break}}
* [[Matching law]]
* [[Negative (positive) contrast effect]]
* [[Premack principle]]
* [[Reinforcement learning]]
* [[Reward system]]
* [[Sensitization]]
* [[Social conditioning]]
* [[Spontaneous recovery]]
{{multicol-end}}

==References==
{{Reflist|2}}

==External links==
* [http://www.sciencedirect.com/science?_ob=PublicationURL&_issn=03766357&_pubType=J&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=f2cb4d6abf599fdb991a75e175d8189b&jchunk=50#50 Behavioural Processes]
* [http://seab.envmed.rochester.edu/jaba/ Journal of Applied Behavior Analysis]
* [http://seab.envmed.rochester.edu/jeab/ Journal of the Experimental Analysis of Behavior]
* [http://www.mcli.dist.maricopa.edu/proj/nru/nr.html Negative reinforcement]
* [http://www.scholarpedia.org/article/Operant_Conditioning Scholarpedia Operant conditioning]
* [http://www.scienceofbehavior.com/lms/mod/glossary/view.php?id=408 scienceofbehavior.com]
* [[Society for Quantitative Analysis of Behavior]][http://sqab.psychology.org/]
* [http://foxylearning.com/vb An Introduction to Verbal Behavior Online Tutorial]
* [http://foxylearning.com/rft An Introduction to Relational Frame Theory Online Tutorial]

{{Learning}}

{{DEFAULTSORT:Operant Conditioning}}
[[Category:Dog training and behavior]]
[[Category:Educational technology]]
[[Category:Behaviorism]]
[[Category:Learning]]

[[bg:Оперантно кондициониране]]
[[ca:Condicionament operant]]
[[cs:Operantní podmiňování]]
[[de:Instrumentelle und operante Konditionierung]]
[[es:Condicionamiento operante]]
[[fr:Conditionnement opérant]]
[[ko:조작적 조건화]]
[[is:Virk skilyrðing]]
[[it:Condizionamento operante]]
[[he:התניה אופרנטית]]
[[nl:Operante conditionering]]
[[ja:オペラント条件づけ]]
[[no:Operant betinging]]
[[pl:Warunkowanie instrumentalne]]
[[pt:Condicionamento operante]]
[[sr:Оперантно условљавање]]
[[sv:Operant betingning]]
[[zh:操作制約]]</body> </html>