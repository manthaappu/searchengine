<html> <head> <title>Integrated Information Theory (IIT)</title></head><body>{{orphan|date=August 2010}}

{{New unreviewed article|source=ArticleWizard|date=May 2010}}

The '''Integrated Information Theory''' is a recently formulated theory which attempts to study and explain [[consciousness]]. It was developed by psychiatrist and neuroscientist [[Giulio Tononi]] of the [[University of Wisconsin–Madison]].

<big>Overview</big><br />
The theory is based on two key observations. The first is that every observable [[conscious state]] contains an uncountable amount of information. A common example of this is every frame in a movie. Upon seeing a single frame of a movie you have watched you instantly associate it with a "specific conscious precept."<ref>http://www.scientificamerican.com/article.cfm?id=a-theory-of-consciousness</ref>  This brings use to the second key observation of the theory.

All of the information you have gleaned from conscious states is highly, and innately, integrated into your mind. It is impossible for you to see the world apart from all of the information that you are conscious of.

== Definition of Consciousness ==
In this theory, consciousness arises as a property of a physical system, its 'integrated information'. Integrated information is an exact quantity that can be measured using the following equations:

<big> Information </big> </br>

Given: a system (including current probability distribution) and Mechanism (which specifies the possible next state probability distribution, if the current state is perturbed with all possible inputs).
You can determine: Actual Distribution - Possible system states at time t -= 1
Thus: System and Mechanism constitute information (about the system's previous state), in the classic sense of 'reduction of uncertainty.'

<big> Relative Entropy/Effective Information </big> </br>

Effective Information = relative entropy H between the actual and potential repertoires = KullBack-Leibler divergence
Implicitly specified by mechanism and state, so it is an 'intrinsic' property of the system
Can calculate actual repertoire by perturbing the system in all possible ways to obtain forward repertoire of output states, and then using Bayes' Rule.

<big> Example: </big> </br>
System of two Binary elements - Four possible states (00, 01, 10, 11)
The first binary element operates randomly. The second binary element will be whatever the first element was in the previous state. Initially: (0, 0).
maximum entropy: p = (1/4, 1/4, 1/4, 1/4)
Given, at time t, state is 11
Previous state must have been 11 or 10, p = (0, 0, 1/4, 1/4)
Generated one bit of information since ei(X(mech,x1)) = H[p(X0(mech,x1)) || p(X0(maxH))]
where X is our system, mech is that system's mechanism, x1 is a state of the system, and p(X0(maxH)) is the uniform or potential distribution. 

<big> Integration (phi) </big> </br>

phi(X(mech,x1)) = H[p(X0(mech,x1)) || PRODUCT(p(kM0(mech,mu1)))] for kM0 in MIP

where X is our system, mech is that system's mechanism, x1 is a state of the system, PRODUCT(p(kM0(mech,mu1))) is the product of all the probability distributions of each part of the system in the minimal information partition. 

It's clear then that phi will be high when there is a lot of information generated among the parts of a system as opposed to within them.
 

<big> Complexes </big> </br>
A complex a set of elements that generate integrated information that is not fully contained in a larger set of higher phi.

This then leads naturally to the notion of a main complex, which is the complex in a system that generates the largest amount of phi. Note that a main complex can partially contain complexes of lower phi within it.

== Interpretations of different aspects of consciousness ==

<big> Quality of consciousness </big> </br>

We begin by defining a multi dimensional space called qualia space, or Q-space.
This space has an axis for every state of the system. A point in this space, then, has a component for every state; if we restrict the components to be numbers from 0 to 1, then we can view the components as probabilities that the system is in that state.
Thus a point in Q-space represents a probability distribution.
Now again using relative entropy we can measure the amount of information generated by a single connection c within the system with the following equation:

phi_c = H[p(X(mech,x)) || p(Y(mech,y))]

where Y is the system with that connection removed.
Thus there is are points Y and X in Q-space that correspond to the probability distributions of the system with and without the connection c, respectively. We can then draw a vector from Y to X that has length phi_c. This vector is associated with the connection c and is called a q-arrow. So a q-arrow is a representation of the informational relationship specified by a connection.

<big> Properties of q-arrows </big>

<big> Context dependency </big>

<big> Q-folds </big>

<big> Entanglement </big>

==References==
<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->
{{Reflist}}

==External links==
* [http://www.scientificamerican.com/article.cfm?id=a-theory-of-consciousness Scientific American Article]
* [http://www.biolbull.org/cgi/content/full/215/3/216 Integrated Information Theory: A Provisional Manifesto]

{{DEFAULTSORT:Integrated Information Theory (Iit)}}
[[Category:Consciousness studies]]

[[zh:資訊統整理論]]</body> </html>